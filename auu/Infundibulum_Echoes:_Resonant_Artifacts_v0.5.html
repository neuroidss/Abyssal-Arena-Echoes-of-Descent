<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, viewport-fit=cover">
    <!-- Removed deprecated meta tag, keeping mobile-web-app-capable -->
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes"> <!-- Keep for older iOS compatibility -->
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Infundibulum Echoes: Resonant Artifacts v0.5</title>
    <style>
        html, body { margin: 0; padding: 0; overflow: hidden; height: 100%; width: 100%; background-color: #000; color: #fff; font-family: sans-serif; cursor: none; -webkit-tap-highlight-color: transparent; touch-action: none; }
        canvas { display: block; width: 100%; height: 100%; }
        #debugInfo { position: absolute; top: 5px; left: 5px; color: rgba(200,200,200,0.7); font-family: monospace; font-size: 8px; line-height: 1.2; background-color: rgba(0,0,0,0.5); padding: 3px 5px; border-radius: 3px; display: none; z-index: 10; pointer-events: none; max-width: calc(100% - 10px); }
        #warningInfo { position: absolute; bottom: 10px; left: 10px; color: yellow; font-family: sans-serif; font-size: 12px; background-color: rgba(0,0,0,0.6); padding: 8px; border-radius: 5px; display: block; z-index: 10; pointer-events: none; }
        #loadingInfo { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: #eee; font-size: 14px; background-color: rgba(0,0,0,0.7); padding: 15px; border-radius: 8px; display: none; z-index: 10; text-align: center; pointer-events: none; }
        #resetButton { position: absolute; bottom: -999px; left: -999px; width: 1px; height: 1px; opacity: 0; pointer-events: none; } /* Keep hidden button for programmatic click */
        #speechStatus { position: absolute; top: 10px; right: 10px; color: rgba(180, 180, 255, 0.6); font-family: monospace; font-size: 9px; background-color: rgba(0,0,50,0.4); padding: 2px 4px; border-radius: 3px; display: none; z-index: 10; pointer-events: none; }
    </style>
</head>
<body>
    <canvas id="renderCanvas"></canvas>
    <div id="debugInfo">Debug Info Placeholder</div>
    <div id="warningInfo">Interact to initialize. Say "Reset Echoes" for voice command reset.</div>
    <div id="loadingInfo">Loading Assets...<br><span id="loadingProgress"></span></div>
    <div id="speechStatus">Speech: Idle</div>
    <button id="resetButton">Reset</button>

    <!-- External Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js" type="module"></script>
    <!-- Transformers.js imported dynamically -->

    <!-- Main Game Logic -->
    <script type="module">

        // Import Three.js
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js';

        // --- Constants & Configuration ---
        const VERSION = "0.5";
        const USE_DEBUG = true;
        const TARGET_FPS = 55;
        const HIGH_PERFORMANCE_MODE = hasRTX3060LevelGPU();
        const STATE_VECTOR_SIZE = 128;
        const INPUT_VECTOR_SIZE = 128;
        const EMBEDDING_DIM = 384;
        const MAX_ARTIFACTS = 18;
        const MAX_ACTIVE_ARTIFACTS = 5;
        const ARTIFACT_SIMILARITY_THRESHOLD = 0.47;
        const ARTIFACT_CREATION_INTERVAL_MS = 9000;
        const ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MIN = 0.28;
        const ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MAX = 0.75;
        const EMBEDDING_MODEL_NAME = 'Xenova/all-MiniLM-L6-v2';
        const MIC_FFT_SIZE = 256;
        const ACCEL_FFT_SIZE = 64;
        const LOCAL_STORAGE_KEY = `infundibulumEchoesState_v${VERSION}`; // Version bump
        const SPEECH_COMMANDS = { // Keep defined for matching
            CREATE: ["create artifact", "make echo", "capture this"],
            FORGET_OLDEST: ["forget oldest", "remove last echo", "clear history"],
            RESET: ["reset echoes", "start over", "clear all"], // Still available via voice
        };
        const SYNC_THRESHOLD = 0.3;
        const SYNC_DECAY = 0.98;
        const ACCEL_ANALYSIS_INTERVAL_S = ACCEL_FFT_SIZE / (TARGET_FPS * 0.9); // Analyse slightly more often
        const LONG_PRESS_DURATION_MS = 2000; // Duration for long press reset part 1
        const RESET_SECOND_TAP_WINDOW_MS = 400; // Time window for second tap after long press release

        // --- Global State ---
        let renderer, scene, camera, audioContext, masterGain, analyserNode;
        let inputProcessorModel, coreLogicModel;
        let graphicsController, audioController, artifactManager, embeddingProvider, featureExtractor, speechController;
        let interactionOccurred = false; let embeddingsReady = false;
        let lastArtifactCreationTime = 0;
        let complexityLevel = HIGH_PERFORMANCE_MODE ? 0.7 : 0.4;
        let lastFpsTime = 0; let frameCount = 0; let currentFPS = TARGET_FPS;
        let currentInputState = {
            touch: { x: 0.5, y: 0.5, active: false, pressure: 0, dx: 0, dy: 0, lastX: 0.5, lastY: 0.5 },
            motion: { alpha: 0, beta: 0, gamma: 0, available: false },
            mic: { level: 0, fft: new Float32Array(MIC_FFT_SIZE / 2).fill(-140), available: false, rhythmPeak: 0, rhythmTempo: 0 },
            accelerometer: { x: 0, y: 0, z: 0, magnitude: 0, available: false, history: new Array(ACCEL_FFT_SIZE).fill(0), rhythmPeak: 0, rhythmTempo: 0 },
            syncFactor: 0.0
        };
        let unifiedIntentVector = tf.zeros([1, INPUT_VECTOR_SIZE]);
        let currentResonantState = tf.fill([1, STATE_VECTOR_SIZE], 0.5);
        let activeArtifactInfo = { ids: [], stateArrays: [], similarities: [], keptVectors: [] };
        let resetGestureState = {
            pointerDownTime: 0,
            longPressDetected: false,
            longPressReleaseTime: 0,
            resetTimeout: null
        };
        let lastAccelTime = 0;
        let visualFeedback = { active: false, intensity: 0, startTime: 0, duration: 0.1 };

        // --- Rhythm Analysis Helper ---
        function analyzeRhythm(fftData, sampleRate, fftSize) {
            // ... (implementation unchanged from v0.4) ...
            let peakFreq = 0; let peakMag = -Infinity; let energy = 0;
            const nyquist = sampleRate / 2;
            const freqResolution = nyquist / fftData.length;
            for (let i = 0; i < fftData.length; i++) { const freq = i * freqResolution; energy += Math.pow(10, fftData[i] / 10); if (fftData[i] > peakMag && freq > 1 && freq < 15) { peakMag = fftData[i]; peakFreq = freq; } }
            const normalizedPeak = clamp((peakMag + 100.0) / 100.0, 0, 1);
            const estimatedTempo = clamp(peakFreq * 60, 60, 240); // Simple BPM estimate
            return { peak: normalizedPeak, tempo: estimatedTempo };
        }

        // --- Feature Extractor ---
        class FeatureExtractor {
            // ... (implementation unchanged from v0.4) ...
             constructor(stateVectorSize) { this.stateVectorSize = stateVectorSize; this.indices = { kick: 0, bassCut: 2, bassRes: 3, bassPat: 4, arpRate: 5, noiseLvl: 9, hue: 10, flow: 11, warp: 12, complexity: 17, tempo: 19, reverb: 25, leadDecay: 26, masterVol: 35 }; } _getCategory(v, th, l) { for (let i=0;i<th.length;i++) { if (v<th[i]) return l[i]; } return l[l.length-1]; }
             extractTags(arr) { if (!arr || arr.length !== this.stateVectorSize) return ""; const tags = new Set(); const i = this.indices; const tempo=arr[i.tempo]??0.5; tags.add(this._getCategory(tempo, [0.3,0.7], ["slow","mid","fast"])); if(arr[i.kick]>0.75) tags.add("drive"); const bc=arr[i.bassCut]??0.5; const br=arr[i.bassRes]??0.5; if(bc<0.3 && br>0.6) tags.add("dark_bass"); else if(bc>0.5 && br>0.7) tags.add("acid"); else if(bc>0.4) tags.add("bright_bass"); tags.add(`bass_${Math.floor((arr[i.bassPat]??0)*4)}`); if((arr[i.arpRate]??0.5)>0.8) tags.add("fast_arp"); if((arr[i.noiseLvl]??0)>0.6) tags.add("noisy"); if((arr[i.leadDecay]??0.1)<0.1) tags.add("short_lead"); else tags.add("long_lead"); const comp=arr[i.complexity]??0.5; const hue=arr[i.hue]??0.5; tags.add(this._getCategory(comp, [0.4,0.8], ["simple","mid","complex"])); tags.add(this._getCategory(hue, [0.15,0.3,0.45,0.6,0.75,0.9], ["red","orange","yellow","green","blue","purple","red"])); if((arr[i.flow]??0)>0.7) tags.add("flow"); if((arr[i.warp]??0)>0.6) tags.add("warp"); if((arr[i.reverb]??0)>0.5) tags.add("reverb"); if((arr[i.masterVol]??0.6)<0.3) tags.add("quiet"); else if((arr[i.masterVol]??0.6)>0.8) tags.add("loud"); return Array.from(tags).join(' ');
             }
         }

        // --- Embedding Provider ---
        class EmbeddingProvider {
             // ... (implementation unchanged from v0.4) ...
            constructor(modelName) { this.modelName = modelName; this.pipeline = null; this.isInitializing = false; this.loadingDiv = document.getElementById('loadingInfo'); this.loadingProgress = document.getElementById('loadingProgress'); }
            async init() { if (this.pipeline || this.isInitializing) return; this.isInitializing = true; showLoading(true, `Loading Embedding: ${this.modelName}...`); console.log(`Loading model: ${this.modelName}...`); try { const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1'); env.allowLocalModels = false; env.allowRemoteModels = true; env.backends.onnx.wasm.numThreads = 1; env.backends.onnx.wasm.simd = true; console.log("Transformer env configured."); this.pipeline = await pipeline('feature-extraction', this.modelName, { quantized: true, progress_callback: (p) => { if (this.loadingProgress) { let s=p.status; if(p.file) s+=`: ${p.file}`; if(p.loaded&&p.total){s+=` (${((p.loaded/p.total)*100).toFixed(1)}%)`;} this.loadingProgress.textContent=s; } } }); console.log("Embedding pipeline loaded."); embeddingsReady = true; showLoading(false); showWarning("System Ready. Interact or Speak. Say 'Reset Echoes' to reset.", 6000); } catch (e) { console.error("FATAL: Embedding model failed:", e); showError("Embedding Model Failed! Artifacts disabled."); embeddingsReady = false; showLoading(false); artifactManager = null; } finally { this.isInitializing = false; } }
            async embed(text) { if (!this.pipeline || !embeddingsReady) return null; if (!text || typeof text !== 'string' || text.trim().length === 0) { console.warn("Embed skip: Invalid text."); return null; } try { const r = await this.pipeline(text, { pooling: 'mean', normalize: true }); if (r && r.data && r.data.length === EMBEDDING_DIM) return r.data; else { console.warn("Embed failed/bad shape:", text); return null; } } catch (e) { console.error("Embedding error:", e); return null; } }
         }

        // --- Artifact Manager ---
        class ArtifactManager {
             // ... (implementation unchanged from v0.4) ...
             constructor(maxArtifacts, stateVectorSize, embeddingDim, featureExtractor, embeddingProvider) { this.maxArtifacts=maxArtifacts; this.stateVectorSize=stateVectorSize; this.embeddingDim=embeddingDim; this.featureExtractor=featureExtractor; this.embeddingProvider=embeddingProvider; this.artifacts=[]; this.nextId=0; }
             async createArtifact(stateVectorTensor) { if (!embeddingsReady || !this.featureExtractor || !this.embeddingProvider) return false; if (!stateVectorTensor || stateVectorTensor.isDisposed) { console.warn("Artifact create skip: Invalid tensor."); return false; } const stateArr = await stateVectorTensor.data(); const tags = this.featureExtractor.extractTags(stateArr); if (!tags) return false; const emb = await this.embeddingProvider.embed(tags); if (!emb || emb.length !== this.embeddingDim) return false; const newArt = { id: this.nextId++, stateVector: Array.from(stateArr), featureTags: tags, embedding: emb, timestamp: Date.now() }; this.artifacts.push(newArt); console.log(`Artifact ${newArt.id} created: "${tags.substring(0,50)}..."`); triggerVisualFeedback(0.6); if (this.artifacts.length > this.maxArtifacts) { this.artifacts.sort((a,b) => a.timestamp - b.timestamp); const removed = this.artifacts.shift(); console.log(`Pruned oldest artifact ${removed.id}. Count: ${this.artifacts.length}`); } return true; }
             _cosineSimilarity(a,b) { if (!a||!b||a.length !== b.length||a.length===0) return 0; let dot=0, nA=0, nB=0; for (let i=0; i<a.length; i++) { dot+=a[i]*b[i]; nA+=a[i]*a[i]; nB+=b[i]*b[i]; } if (nA===0||nB===0) return 0; return dot / ((Math.sqrt(nA)*Math.sqrt(nB))+1e-9); }
             async findRelevantArtifacts(stateTensor, thresh, maxCnt) { const result = { ids:[], stateArrays:[], similarities:[], keptVectors:[] }; if (!embeddingsReady || this.artifacts.length===0 || !stateTensor || stateTensor.isDisposed) return result; const stateArr = await stateTensor.data(); const tags = this.featureExtractor.extractTags(stateArr); if (!tags) return result; const queryEmb = await this.embeddingProvider.embed(tags); if (!queryEmb) return result; const candidates = this.artifacts.map(art => ({ art: art, similarity: this._cosineSimilarity(queryEmb, art.embedding) })); const relevant = candidates.filter(c => c.similarity >= thresh).sort((a,b) => b.similarity - a.similarity); const selected = relevant.slice(0, maxCnt); tf.tidy(() => { selected.forEach(item => { result.ids.push(item.art.id); result.stateArrays.push(item.art.stateVector); result.similarities.push(item.similarity); const vecTensor = tf.tensor1d(item.art.stateVector).expandDims(0); result.keptVectors.push(tf.keep(vecTensor)); }); }); return result; }
             getArtifactCount() { return this.artifacts.length; }
             setArtifacts(loaded) { this.artifacts = loaded; this.nextId = loaded.reduce((maxId, art) => Math.max(maxId, art.id), -1) + 1; console.log(`Loaded ${this.artifacts.length} artifacts. Next ID: ${this.nextId}`); }
             forgetOldestArtifact() { if (this.artifacts.length > 0) { this.artifacts.sort((a, b) => a.timestamp - b.timestamp); const removed = this.artifacts.shift(); console.log(`Forgot oldest artifact ${removed.id} via command.`); triggerVisualFeedback(0.3, 0.2); return true; } console.log("No artifacts to forget."); return false; }
         }

        // --- Speech Recognition Controller (Improved Stability) ---
        class SpeechRecognitionController {
            constructor() {
                this.recognition = null;
                this.isSupported = ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window);
                this.isListening = false; // Actively supposed to be listening
                this.isActive = false;    // Recognition engine is actually running
                this.isStarting = false;  // In the process of starting
                this.isStopping = false;  // In the process of stopping
                this.permissionGranted = false;
                this.statusDiv = document.getElementById('speechStatus');
                this.commandCallback = null;
                this.consecutiveErrorCount = 0;
                this.MAX_CONSECUTIVE_ERRORS = 8; // Stop trying after this many errors in a row
                this.restartTimeoutId = null;

                if (!this.isSupported) {
                    console.warn("Speech Recognition API not supported.");
                    this.updateStatus("Unsupported");
                }
            }

            updateStatus(status) {
                if (this.statusDiv) {
                    this.statusDiv.textContent = `Speech: ${status}`;
                    this.statusDiv.style.display = 'block';
                }
                 // console.log(`Speech Status Update: ${status}`); // Less verbose logging
            }

            async requestPermissionAndInit() {
                if (!this.isSupported || this.permissionGranted || this.recognition) return; // Prevent re-init if already done
                try {
                    // Request mic permission subtly if not already granted
                    await navigator.mediaDevices.getUserMedia({ audio: true });
                    this.permissionGranted = true;
                    console.log("Audio permission likely granted for Speech Rec.");
                    this.initializeRecognition();
                } catch (err) {
                    console.error("Mic permission denied for Speech Rec:", err);
                    this.updateStatus("Permission Denied");
                    showWarning("Voice commands disabled: Mic permission needed.", 5000);
                    this.permissionGranted = false; // Ensure flag is false
                }
            }

            initializeRecognition() {
                 if (!this.isSupported || !this.permissionGranted || this.recognition) return;

                 const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                 this.recognition = new SpeechRecognition();
                 this.recognition.continuous = true; // Keep listening after speech
                 this.recognition.interimResults = false; // Only final results
                 this.recognition.lang = 'en-US';

                 this.recognition.onstart = () => {
                     console.log("Speech recognition actually started.");
                     this.isActive = true;
                     this.isStarting = false;
                     this.updateStatus("Listening");
                 };

                 this.recognition.onend = () => {
                     console.log("Speech recognition ended.");
                     this.isActive = false;
                     this.isStarting = false;
                     this.isStopping = false;
                     if (this.isListening) { // If we *intended* to be listening...
                        console.log("...restarting listening after short delay.");
                        this.scheduleRestart(150 + Math.random() * 100); // Schedule restart
                     } else {
                        this.updateStatus("Idle");
                     }
                 };

                 this.recognition.onresult = (event) => {
                     this.consecutiveErrorCount = 0; // Reset error count on successful result
                     const last = event.results.length - 1;
                     const transcript = event.results[last][0].transcript.trim().toLowerCase();
                     console.log(`Speech Result: "${transcript}"`);
                     this.handleCommand(transcript);
                     // NOTE: recognition might automatically stop after a result even with continuous=true
                     // The onend handler should manage restarting if isListening is still true.
                 };

                 this.recognition.onerror = (event) => {
                     console.error('Speech Recognition Error:', event.error);
                     const error = event.error;
                     this.isActive = false; // Assume it stopped on error
                     this.isStarting = false;
                     this.isStopping = false;

                     if (error === 'no-speech') {
                         this.updateStatus("Quiet");
                         // No-speech is common, restart if we should be listening.
                         if (this.isListening) this.scheduleRestart(500 + Math.random() * 250);
                     } else if (error === 'audio-capture') {
                          this.updateStatus("Mic Problem");
                          this.consecutiveErrorCount++;
                          if (this.isListening) this.scheduleRestart(1000 + Math.random() * 500);
                     } else if (error === 'network') {
                          this.updateStatus("Network Issue");
                          this.consecutiveErrorCount++;
                          if (this.isListening) this.scheduleRestart(1500 + Math.random() * 500);
                     } else if (error === 'not-allowed' || error === 'service-not-allowed') {
                         this.updateStatus("Blocked/Disabled");
                         this.permissionGranted = false; // Permission likely revoked or service disabled
                         this.isListening = false; // Stop trying
                         showError(`Voice commands ${error}! Check permissions.`);
                         // Do NOT restart
                     } else {
                          this.updateStatus(`Error (${error})`);
                          this.consecutiveErrorCount++;
                          if (this.isListening) this.scheduleRestart(750 + Math.random() * 250); // Restart for other errors
                     }

                     if (this.consecutiveErrorCount > this.MAX_CONSECUTIVE_ERRORS) {
                         console.error("Too many consecutive speech errors, stopping attempts.");
                         this.updateStatus("Stopped (Errors)");
                         this.isListening = false; // Give up
                         showWarning("Speech recognition stopped due to repeated errors.", 6000);
                     }
                 };
                 console.log("Speech Recognition initialized.");
                 this.updateStatus("Initialized");
            }

            setCommandCallback(callback) {
                this.commandCallback = callback;
            }

            scheduleRestart(delay) {
                if (this.restartTimeoutId) clearTimeout(this.restartTimeoutId); // Clear previous schedule
                this.restartTimeoutId = setTimeout(() => {
                    console.log("Executing scheduled restart.");
                    this.startListening();
                }, delay);
            }

            startListening() {
                 // Prevent starting if not supported, no permission, already active, or in transition states
                if (!this.isSupported || !this.permissionGranted || this.isActive || this.isStarting || this.isStopping) {
                    // console.log(`Speech start skipped: Sup:${this.isSupported}, Perm:${this.permissionGranted}, Act:${this.isActive}, Start:${this.isStarting}, Stop:${this.isStopping}`);
                    if (!this.isListening && this.recognition && this.permissionGranted) {
                       // If we intended to stop, but something tried to start again, ensure status is Idle
                       this.updateStatus("Idle");
                    }
                    return;
                }
                if (!this.recognition) {
                    console.warn("Speech start skipped: Recognition not initialized yet. Retrying init...");
                    // Attempt to re-initialize if called too early
                     this.requestPermissionAndInit().then(() => {
                         if(this.recognition && this.permissionGranted) {
                              console.log("Re-init successful, now attempting start.");
                              this.startListening(); // Try again after init
                         }
                     });
                    return;
                }

                console.log("Attempting to start speech recognition...");
                this.isListening = true; // Set intention to listen
                this.isStarting = true; // Mark as starting
                this.updateStatus("Starting...");
                try {
                    // Clear any pending restarts before explicitly starting
                    if (this.restartTimeoutId) clearTimeout(this.restartTimeoutId);
                    this.restartTimeoutId = null;

                    this.recognition.start();
                    // onstart event will set isActive=true and update status to Listening
                } catch (e) {
                    console.error("Error executing recognition.start():", e);
                    this.isListening = false; // Failed, clear intention
                    this.isStarting = false;
                    this.isActive = false; // Ensure inactive state
                    this.updateStatus("Start Failed");
                    // Don't immediately retry here, let onerror/onend handle it if applicable
                    if (e.name === 'InvalidStateError') {
                        console.warn("InvalidStateError caught, likely rapid start/stop. Waiting for onend.");
                         // Schedule a delayed start to recover
                         this.scheduleRestart(300 + Math.random() * 100);
                    }
                }
            }

            stopListening() {
                console.log("Attempting to stop speech recognition...");
                this.isListening = false; // Clear intention
                // Clear any pending restarts
                if (this.restartTimeoutId) clearTimeout(this.restartTimeoutId);
                this.restartTimeoutId = null;

                if (!this.recognition || !this.isActive || this.isStopping) {
                     console.log(`Speech stop skipped: Rec:${!!this.recognition}, Act:${this.isActive}, Stop:${this.isStopping}`);
                     if (!this.isActive && !this.isStarting) this.updateStatus("Idle"); // Ensure correct idle status
                     return; // Only stop if actually active and not already stopping
                }

                this.isStopping = true; // Mark as stopping
                this.updateStatus("Stopping");
                try {
                    this.recognition.stop();
                     // onend event will set isActive=false, isStopping=false
                } catch(e) {
                    console.error("Error executing recognition.stop():", e);
                    this.isStopping = false; // Reset flag on error
                    this.isActive = false; // Assume it stopped somehow
                    this.updateStatus("Stop Failed");
                }
            }

            handleCommand(transcript) {
                let matchedCommand = null;
                // Trim again just in case
                const cleanTranscript = transcript.trim();
                if (cleanTranscript.length === 0) return; // Ignore empty results

                for (const commandType in SPEECH_COMMANDS) {
                    if (SPEECH_COMMANDS[commandType].some(phrase => cleanTranscript.includes(phrase))) {
                        matchedCommand = commandType;
                        console.log(`Matched Command: ${matchedCommand} from "${cleanTranscript}"`);
                        break;
                    }
                }

                if (matchedCommand && this.commandCallback) {
                    // Execute silently (no app-generated sound)
                    this.commandCallback(matchedCommand);
                    // Potentially add a brief visual feedback cue here if desired
                    triggerVisualFeedback(0.2, 0.1); // Subtle flash on command recognized
                } else {
                    console.log(`Unrecognized command: "${cleanTranscript}"`);
                }
            }
        }


        // --- NN Model Placeholders ---
        class PlaceholderInputProcessor {
            // ... (implementation unchanged from v0.4) ...
             constructor(inputDim, outputDim) { this.inputDim = inputDim; this.outputDim = outputDim; }
             async process(inputData) {
                 await tf.ready();
                 return tf.tidy(() => {
                     const vec = new Array(INPUT_VECTOR_SIZE).fill(0);
                     const touchFactor = inputData.touch.active ? (inputData.touch.pressure || 1.0) : 0;
                     const micLevel = inputData.mic.available ? inputData.mic.level : 0;
                     const motionAvailable = inputData.motion.available;
                     const alphaNorm = motionAvailable ? (inputData.motion.alpha / 360.0) % 1.0 : 0.5;
                     const betaNorm = motionAvailable ? (inputData.motion.beta + 180.0) / 360.0 : 0.5;
                     const gammaNorm = motionAvailable ? (inputData.motion.gamma + 90.0) / 180.0 : 0.5;
                     const touchVelMag = clamp(Math.sqrt(inputData.touch.dx**2 + inputData.touch.dy**2) * 25, 0, 1);
                     const accelAvailable = inputData.accelerometer.available;
                     const accelNormX = accelAvailable ? clamp((inputData.accelerometer.x + 20) / 40, 0, 1) : 0.5;
                     const accelNormY = accelAvailable ? clamp((inputData.accelerometer.y + 20) / 40, 0, 1) : 0.5;
                     const accelNormZ = accelAvailable ? clamp((inputData.accelerometer.z + 20) / 40, 0, 1) : 0.5;
                     const accelMagNorm = accelAvailable ? clamp(inputData.accelerometer.magnitude / 25, 0, 1) : 0;

                     for (let i = 0; i < INPUT_VECTOR_SIZE; i++) {
                         switch (i % 14) {
                             case 0: vec[i] = inputData.touch.x; break; case 1: vec[i] = inputData.touch.y; break; case 2: vec[i] = touchFactor; break;
                             case 3: vec[i] = alphaNorm; break; case 4: vec[i] = betaNorm; break; case 5: vec[i] = gammaNorm; break;
                             case 6: vec[i] = micLevel; break; case 7: vec[i] = touchVelMag; break; case 8: vec[i] = accelNormX; break;
                             case 9: vec[i] = accelNormY; break; case 10: vec[i] = accelNormZ; break; case 11: vec[i] = accelMagNorm; break;
                             case 12: vec[i] = fract(accelMagNorm*0.5 + micLevel*0.3 + touchVelMag*0.2 + betaNorm*0.1); break;
                             case 13: vec[i] = fract(Math.sin(vec[i-1]*15.1 + vec[i-3]*11.3 + accelNormY*19.5 + gammaNorm*7.7) * 437.545); break;
                         }
                         vec[i] = 1.0 / (1.0 + Math.exp(-(vec[i] * 2.0 - 1.0) * 1.8)); vec[i] = clamp(vec[i] || 0, 0, 1);
                     }
                     if (inputData.mic.available && inputData.mic.fft) {
                        const fftData = inputData.mic.fft; const fftLen = fftData.length; const segments = 16; const binsPerSegment = Math.max(1, Math.floor(fftLen/segments));
                        for(let seg=0; seg<segments; seg++){ let sumDb=0; let peakDb=-140; let cnt=0; const start=seg*binsPerSegment; const end=Math.min(start+binsPerSegment, fftLen); for(let k=start;k<end;k++){ if(isFinite(fftData[k]) && fftData[k]>-100){ sumDb+=fftData[k]; peakDb=Math.max(peakDb,fftData[k]); cnt++; } } let avgDb=cnt>0?sumDb/cnt:-140; const normAvg=clamp((avgDb+100)/100,0,1); const normPeak=clamp((peakDb+100)/100,0,1); const tIdx1=(INPUT_VECTOR_SIZE-1-seg*2); const tIdx2=(INPUT_VECTOR_SIZE-2-seg*2); if(tIdx1>=0) vec[tIdx1]=(vec[tIdx1]*0.5+normAvg*0.5); if(tIdx2>=0) vec[tIdx2]=(vec[tIdx2]*0.4+normPeak*0.6); }
                     }
                     return tf.tensor1d(vec).expandDims(0);
                 });
             }
         }

        class PlaceholderCoreLogic {
             // ... (implementation unchanged from v0.4) ...
            constructor(stateSize) { this.stateSize = stateSize; }
            async predict(intentVectorTensor, currentStateTensor, activeArtifactVectors, activeArtifactSimilarities) {
                if (!intentVectorTensor || intentVectorTensor.isDisposed || !currentStateTensor || currentStateTensor.isDisposed || !activeArtifactVectors || !activeArtifactSimilarities) { console.warn("CoreLogic predict skip: Invalid tensors."); return tf.keep(currentStateTensor.clone()); }
                return tf.tidy(() => {
                    const decayFactor = 0.993; const intentInfluence = 0.24; const artifactInfluenceFactor = 0.16;
                    const complexityFactor = 0.06 * complexityLevel;

                    let processedIntent = intentVectorTensor;
                    const intentInfluenceVec = processedIntent.sub(tf.scalar(0.5)).mul(tf.scalar(intentInfluence));

                    let combinedArtifactInfluence = tf.zerosLike(currentStateTensor);
                    if (activeArtifactVectors.length > 0) {
                        for(let i=0; i<activeArtifactVectors.length; i++) { const artVec = activeArtifactVectors[i]; const sim = activeArtifactSimilarities[i]; if (!artVec || artVec.isDisposed) continue; const influence = artVec.sub(tf.scalar(0.5)).mul(tf.scalar(sim * artifactInfluenceFactor)); combinedArtifactInfluence = combinedArtifactInfluence.add(influence); }
                    }

                    const shiftedRight = currentStateTensor.slice([0, 1], [1, STATE_VECTOR_SIZE - 1]).pad([[0, 0], [1, 0]], 0.5);
                    const shiftedLeft = currentStateTensor.slice([0, 0], [1, STATE_VECTOR_SIZE - 1]).pad([[0, 0], [0, 1]], 0.5);
                    const crossTalk = shiftedLeft.sub(shiftedRight).mul(tf.scalar(0.035));
                    const dynamicNoise = tf.randomUniform(currentStateTensor.shape, -complexityFactor, complexityFactor);

                    let nextState = currentStateTensor.sub(tf.scalar(0.5)).mul(tf.scalar(decayFactor)).add(tf.scalar(0.5))
                        .add(intentInfluenceVec).add(combinedArtifactInfluence).add(crossTalk).add(dynamicNoise);
                    nextState = nextState.clipByValue(0.01, 0.99);
                    return nextState;
                });
            }
        }

        // --- Graphics Controller ---
        class GraphicsController {
             // ... (implementation mostly unchanged from v0.4, fragment shader uses complexityLevel) ...
            constructor(canvas) {
                this.canvas = canvas; scene = new THREE.Scene(); camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, 0.1, 100); camera.position.z = 4;
                renderer = new THREE.WebGLRenderer({ canvas: this.canvas, antialias: HIGH_PERFORMANCE_MODE, powerPreference: "high-performance" });
                renderer.setSize(window.innerWidth, window.innerHeight); renderer.setPixelRatio(Math.min(window.devicePixelRatio, HIGH_PERFORMANCE_MODE ? 2 : 1.2));
                const geometry = new THREE.PlaneGeometry(2, 2);
                this.material = new THREE.ShaderMaterial({
                    uniforms: {
                        time: { value: 0.0 }, resolution: { value: new THREE.Vector2(window.innerWidth * renderer.getPixelRatio(), window.innerHeight * renderer.getPixelRatio()) },
                        mainState: { value: new Float32Array(STATE_VECTOR_SIZE).fill(0.5) },
                        numActiveArtifacts: { value: 0 }, artifactStates: { value: this._createArtifactUniformArray() }, artifactSimilarities: { value: new Float32Array(MAX_ACTIVE_ARTIFACTS).fill(0.0) },
                        complexity: { value: complexityLevel },
                        syncFactor: { value: 0.0 },
                        feedbackIntensity: { value: 0.0 }
                    },
                    vertexShader: `varying vec2 vUv; void main() { vUv = uv; gl_Position = vec4(position.xy, 0.0, 1.0); }`,
                    fragmentShader: `
                        precision highp float;
                        uniform float time; uniform vec2 resolution; uniform float mainState[${STATE_VECTOR_SIZE}];
                        uniform int numActiveArtifacts; uniform float artifactStates[${MAX_ACTIVE_ARTIFACTS * STATE_VECTOR_SIZE}]; uniform float artifactSimilarities[${MAX_ACTIVE_ARTIFACTS}];
                        uniform float complexity; uniform float syncFactor; uniform float feedbackIntensity;
                        varying vec2 vUv;

                        #define PI 3.14159265359
                        #define STATE_VEC_SIZE ${STATE_VECTOR_SIZE}
                        #define MAX_ARTIFACTS_SHADER ${MAX_ACTIVE_ARTIFACTS}

                        float hash1(float n){ return fract(sin(n)*43758.5453); }
                        vec2 hash2(vec2 p){ p=vec2(dot(p,vec2(127.1,311.7)),dot(p,vec2(269.5,183.3))); return -1.+2.*fract(sin(p)*43758.5453); }
                        float noise(vec2 x){ vec2 p=floor(x); vec2 f=fract(x); f=f*f*(3.-2.*f); float n=p.x+p.y*57.; return mix(mix(hash1(n),hash1(n+1.),f.x),mix(hash1(n+57.),hash1(n+58.),f.x),f.y); }
                        float fbm(vec2 p, float H, int octaves){ float G=exp2(-H); float f=1., a=1., t=0.; for(int i=0; i<10; i++){ if(i>=octaves) break; t+=a*noise(f*p); f*=2.; a*=G; } return t; }
                        vec3 hsv2rgb(vec3 c){ vec4 K=vec4(1.,2./3.,1./3.,3.); vec3 p=abs(fract(c.xxx+K.xyz)*6.-K.www); return c.z*mix(K.xxx,clamp(p-K.xxx,0.,1.),c.y); }
                        float pulse(float t, float freq){ return 0.5+0.5*cos(t*freq*2.*PI); }

                        float getArtifactState(int artIdx, int stateIdx){
                            if(artIdx<0 || artIdx>=MAX_ARTIFACTS_SHADER || stateIdx<0 || stateIdx>=STATE_VEC_SIZE) return 0.5;
                            int flatIdx = artIdx * STATE_VEC_SIZE + stateIdx;
                            if(flatIdx < 0 || flatIdx >= MAX_ARTIFACTS_SHADER * STATE_VEC_SIZE) return 0.5; // Safety check
                            return artifactStates[flatIdx];
                        }

                        void main() {
                            vec2 uv = vUv; vec2 centerUv = uv-0.5; float distCenter=length(centerUv);

                            // Extract state vars, influenced by complexity
                            float kick=mainState[0]; float bassCut=mainState[2]; float sat=0.4+mainState[4]*0.6; float arpSpeed=0.1+mainState[5]*2.; float bright=0.1+mainState[8]*0.6;
                            float noiseInt=mainState[9]*0.8 * (0.7 + complexity * 0.5);
                            float hueBase=mainState[10]; float flowSpeed=0.02+mainState[11]*0.25; float warpAmt=mainState[12]*0.35 * (0.8 + complexity * 0.4);
                            float compH=0.4+mainState[17]*0.5*(0.5+complexity*0.7);
                            int compOct=2+int(mainState[17]*4.*(0.6+complexity*0.8)); compOct=clamp(compOct,1,7);
                            float pulseInt=mainState[18]*0.8; float tempo=100.+mainState[19]*100.; float vignette=0.2+mainState[20]*0.7; float grain=mainState[21]*0.08 * (0.6 + complexity * 0.7);

                            // Core visual calculations
                            vec2 flowVec=vec2(cos(time*flowSpeed*.7),sin(time*flowSpeed))*.5; vec2 warpDir=hash2(uv*3.+time*.05); vec2 warpOffset=warpDir*warpAmt*(.5+noise(uv*1.5+time*.02)*.5)*(1.-distCenter);
                            vec2 warpedUv=(uv-.5)*(1.-bassCut*.1)+.5+warpOffset; float baseFreq=1.5+arpSpeed*3.; float n=fbm(warpedUv*baseFreq+flowVec,compH,compOct);
                            float hue=fract(hueBase+time*.01+n*.1-bassCut*.2); float beatPulse=pulse(time,tempo/60.); float kickPulse=beatPulse*kick*pulseInt;
                            float finalBright=bright*(1.+kickPulse*.4-pulseInt*.2); finalBright*=pow(max(0.,1.-distCenter*distCenter*vignette*2.),1.5);
                            vec3 finalColor=hsv2rgb(vec3(hue,sat,finalBright));

                            // Artifact Influence (Scales with complexity)
                            float totalArtInfluence=0.;
                            for(int i=0; i<MAX_ARTIFACTS_SHADER; ++i) { if(i>=numActiveArtifacts)break; float sim=artifactSimilarities[i]; if(sim<=0.05)continue; float artHueBase=getArtifactState(i,10); float artSat=0.4+getArtifactState(i,4)*0.6; float artCompH=0.4+getArtifactState(i,17)*0.5*(.5+complexity*.7); int artOct=2+int(getArtifactState(i,17)*4.*(.6+complexity*.6)); artOct=clamp(artOct,1,6); float artTempo=100.+getArtifactState(i,19)*100.; float artKick=getArtifactState(i,0); float artBright=0.1+getArtifactState(i,8)*0.6; float artSeed=hash1(float(i)+getArtifactState(i,30)); float maskFreq=2.+float(i)*1.5; float maskTime=time*.03*(float(i+1)*.5); float artMask=noise(uv*maskFreq+maskTime+artSeed*5.); artMask=smoothstep(.45,.55,artMask); artMask*=sim*(.3+complexity*.7); float artHue=fract(artHueBase+time*.005+n*.05); vec3 artColor=hsv2rgb(vec3(artHue,artSat,artBright)); finalColor=mix(finalColor,artColor,artMask*clamp(.4+complexity*.6,.1,.8)); float artBeatPulse=pulse(time,artTempo/60.); float echoInt=artBeatPulse*artKick*sim*clamp(.1+complexity*.2,.05,.25); finalColor+=vec3(echoInt*artMask)*artColor; totalArtInfluence+=artMask; }

                            // Grain / Noise
                            finalColor+=(hash1(dot(uv,vec2(12.9898,78.233))+time)-.5)*grain*noiseInt;

                            // Sync Factor Visual Cue (Subtle cool overlay)
                            finalColor = mix(finalColor, vec3(0.6, 0.7, 0.9), syncFactor * 0.08);

                            // Feedback Flash
                            finalColor += feedbackIntensity * vec3(1.0, 1.0, 0.9);

                            gl_FragColor=vec4(clamp(finalColor,0.,1.),1.0);
                        }
                    `
                });
                const mesh = new THREE.Mesh(geometry, this.material); scene.add(mesh);
            }
            _createArtifactUniformArray() { return new Float32Array(MAX_ACTIVE_ARTIFACTS * STATE_VECTOR_SIZE).fill(0.5); }
            update(mainStateArray, time, currentActiveArtifactInfo, currentComplexity, currentSyncFactor, currentFeedbackIntensity) {
                 this.material.uniforms.time.value = time;
                 this.material.uniforms.complexity.value = currentComplexity;
                 this.material.uniforms.syncFactor.value = currentSyncFactor;
                 this.material.uniforms.feedbackIntensity.value = currentFeedbackIntensity;

                 if (mainStateArray?.length === STATE_VECTOR_SIZE) this.material.uniforms.mainState.value = Float32Array.from(mainStateArray);

                 const artifactStatesFlat = this.material.uniforms.artifactStates.value;
                 const artifactSimilarities = this.material.uniforms.artifactSimilarities.value;
                 artifactStatesFlat.fill(0.5); artifactSimilarities.fill(0.0);
                 const numActive = Math.min(currentActiveArtifactInfo.ids.length, MAX_ACTIVE_ARTIFACTS);
                 this.material.uniforms.numActiveArtifacts.value = numActive;
                 for (let i=0; i<numActive; ++i) { const stateArr = currentActiveArtifactInfo.stateArrays[i]; const sim = currentActiveArtifactInfo.similarities[i]; if (stateArr?.length === STATE_VECTOR_SIZE) { const offset = i*STATE_VECTOR_SIZE; for(let j=0; j<STATE_VECTOR_SIZE; ++j) artifactStatesFlat[offset+j] = stateArr[j]??0.5; artifactSimilarities[i] = sim; } else { artifactSimilarities[i]=0.; } }
                 this.material.uniforms.artifactStates.needsUpdate = true; this.material.uniforms.artifactSimilarities.needsUpdate = true;

                 renderer.render(scene, camera);
            }
            resize() { const w=window.innerWidth; const h=window.innerHeight; camera.aspect=w/h; camera.updateProjectionMatrix(); const pr=Math.min(window.devicePixelRatio,HIGH_PERFORMANCE_MODE?2:1.2); renderer.setSize(w,h); renderer.setPixelRatio(pr); this.material.uniforms.resolution.value.set(w*pr,h*pr); }
        }

        // --- Audio Controller (Increased Output Gain) ---
        class AudioController {
             constructor() { /* ... (Initialization logic unchanged from v0.4) ... */ this.audioWorkletNode = null; this.isInitialized = false; this.isInitializing = false; this.micStreamSource = null; this.audioWarningDisplayed = false; this.warningTimeout = null; document.body.addEventListener('pointerdown', () => this.tryInitializeAudio(), { once: true, passive: true }); document.body.addEventListener('touchstart', () => this.tryInitializeAudio(), { once: true, passive: true }); document.addEventListener('visibilitychange', async () => { if (!audioContext) return; if (document.hidden) { if (audioContext.state === 'running') { console.log("Suspending AC"); await audioContext.suspend().catch(e=>console.warn("AC suspend err:", e)); } } else { await this.tryInitializeAudio(); } }); }
             showWarning(msg, dur=5000) { const w=document.getElementById('warningInfo'); if(w){ if(this.warningTimeout) clearTimeout(this.warningTimeout); w.textContent=msg; w.style.display='block'; w.style.color='yellow'; this.audioWarningDisplayed=true; if(dur>0) this.warningTimeout=setTimeout(()=>this.hideWarning(), dur); } }
             hideWarning() { const w=document.getElementById('warningInfo'); if(w&&this.audioWarningDisplayed){ w.style.display='none'; this.audioWarningDisplayed=false; if(this.warningTimeout){ clearTimeout(this.warningTimeout); this.warningTimeout=null; } } }

             async tryInitializeAudio() {
                 if (this.isInitializing || (this.isInitialized && audioContext?.state === 'running')) return;
                 this.isInitializing = true; interactionOccurred = true; console.log("Trying AC init/resume...");
                 try {
                     if (audioContext && audioContext.state !== 'running') { console.log(`Closing prev AC (state: ${audioContext.state})`); await audioContext.close().catch(e => console.error("AC close err:", e)); audioContext=null; this.audioWorkletNode=null; this.isInitialized=false; this.micStreamSource=null; analyserNode=null; masterGain=null; currentInputState.mic.available=false; }
                     if (!audioContext) {
                         audioContext = new (window.AudioContext || window.webkitAudioContext)({ latencyHint: 'interactive', sampleRate: 44100 }); console.log(`AC created. State: ${audioContext.state}, SR: ${audioContext.sampleRate}`);
                         masterGain = audioContext.createGain(); masterGain.gain.setValueAtTime(1.0, audioContext.currentTime);
                         masterGain.connect(audioContext.destination);
                         analyserNode = audioContext.createAnalyser(); analyserNode.fftSize = MIC_FFT_SIZE; analyserNode.smoothingTimeConstant = 0.5;
                         currentInputState.mic.fft = new Float32Array(analyserNode.frequencyBinCount);
                     }
                     if (audioContext.state === 'suspended') { console.log("AC suspended, resuming..."); await audioContext.resume(); console.log(`AC resumed. State: ${audioContext.state}`); }
                 } catch (e) { console.error("AC create/resume failed:", e); showError("Audio Error: Context init failed."); this.isInitializing = false; return; }

                 if (audioContext.state === 'running' && !this.isInitialized) {
                     console.log("Adding AudioWorklet...");
                     try {
                          // AudioWorklet processor code (Increased final gain stage)
                          const processorCode = `
                              const WORKLET_STATE_SIZE = ${STATE_VECTOR_SIZE}; const BASE_BPM = 90.0;
                              function fract(n){return n-Math.floor(n);} function lerp(a,b,t){return a+(b-a)*t;} function clamp(v,min,max){return Math.min(max,Math.max(min,v));} function hash(n){return fract(Math.sin(n*12.9898)*43758.5);} function hashSigned(n){return Math.sin(n*78.233)*.5+.5;}
                              function sineLFO(p){return(Math.sin(p*2*Math.PI)+1)*.5;} function sawLFO(p){return p;} function triLFO(p){return 1-Math.abs(fract(p+.5)*2-1);} function sqrLFO(p,d=.5){return p<d?1:0;}
                              class SVF{constructor(){this.z1=0;this.z2=0;this.g=0;this.k=0;this.inv_den=0;}setParams(c,r,sr){const f=20*Math.pow(1000,clamp(c,.01,.99));const q=.5+clamp(r,0,.98)*19.5;this.g=Math.tan(Math.PI*f/sr);this.k=1/q;const g2=this.g*this.g;this.inv_den=1/(1+this.k*this.g+g2);}process(i){const v0=i;const v1=(this.z1+this.g*(v0-this.z2))*this.inv_den;const v2=(this.z2+this.g*v1)*this.inv_den;this.z1=v1+(v1-this.z1);this.z2=v2+(v2-this.z2);return{lowpass:v2,bandpass:v1,highpass:(v0-this.k*v1-v2),notch:v2+(v0-this.k*v1-v2)};}}

                              class GenerativeProcessor extends AudioWorkletProcessor {
                                  constructor(options) { super(options); this.sr=sampleRate; this.phase=0; this.state=new Array(WORKLET_STATE_SIZE).fill(.5); this.lastBeatPhase=0; this.lastSixteenthPhase=0; this.bc=0; this.sc=0; this.filters={bass:[new SVF(),new SVF()],noise:[new SVF(),new SVF()],lead:[new SVF(),new SVF()],hat:[new SVF(),new SVF()]}; const maxDS=1.2; this.dB=[new Float32Array(Math.ceil(this.sr*maxDS)),new Float32Array(Math.ceil(this.sr*maxDS))]; this.dWP=[0,0]; this.dTS=[this.sr*.375,this.sr*.375]; this.dF=[.5,.5]; this.dM=.3; const cB=s=>new Float32Array(Math.ceil(this.sr*s)); this.cL=[.0311,.0383,.0427,.0459]; this.cB=this.cL.map(l=>[cB(l),cB(l)]); this.cWP=this.cL.map(()=>[0,0]); this.cF=.8; this.cD=.3; this.cLS=this.cL.map(()=>[0,0]); this.aL=[.0053,.0121]; this.aB=this.aL.map(l=>[cB(l),cB(l)]); this.aWP=this.aL.map(()=>[0,0]); this.aF=.5; this.rM=.2; this.lfoP={f:0,p:0,n:0,x:0}; this.lfoR={f:.1,p:3,n:.2,x:.05};
                                      this.complexity = 0.5; // Default complexity
                                      this.port.onmessage=(e)=>{ if(e.data.state?.length===WORKLET_STATE_SIZE) this.state=e.data.state; if(typeof e.data.complexity === 'number') this.complexity=clamp(e.data.complexity,0,1); }; console.log("Worklet Processor Initialized. SR:", this.sr); }
                                  static get parameterDescriptors(){return[{name:'masterLevel',defaultValue:1.0,minValue:0,maxValue:2.0}];} // Keep masterLevel param

                                  process(inputs, outputs, params) {
                                      const out=outputs[0]; const bufSize=out[0].length; const mLvlP=params.masterLevel; const masterLevel=mLvlP?.length>0&&isFinite(mLvlP[0])?mLvlP[0]:1.0; // Get masterLevel from parameter
                                      const s=this.state; const ki=.8+(s[0]??.5)*.4; const kt=s[1]??.5; const bcb=.02+(s[2]??.5)*.4; const br=(s[3]??.5)*.85; const bp=Math.floor((s[4]??0)*5); const bo=Math.floor((s[20]??.5)*3)-1; const bd=(s[22]??.1)*.8; const arm=.2+(s[5]??.5)*5; const ap=Math.floor((s[6]??0)*6); const anl=.05+(s[23]??.5)*.5; const nl=.001+(s[9]??.1)*.08*(0.6+this.complexity*0.6); const ncb=.05+(s[7]??.5)*.85; const nr=(s[8]??.5)*.9; const nsr=.05+(s[24]??.5)*.5; const lp=s[10]??.5; const lcb=.08+(s[11]??.5)*.75; const lr=(s[12]??.5)*.88; const lpmd=(s[13]??.2)*.6; const lpat=Math.floor((s[14]??0)*5); const ld=.03+(s[26]??.5)*.6; const llr=.1+(s[27]??.5)*6; const h1l=(s[15]??.5)*.5; const h2l=(s[16]??.5)*.4; const hpat=Math.floor((s[17]??0)*5); const hd1=.008+(s[28]??.5)*.05; const hd2=.04+(s[29]??.5)*.18; const hhp=.25+(s[30]??.5)*.65;
                                      this.lfoR.x = 0.03 + (s[36]??.5) * .15; const fxLfo=sineLFO(this.lfoP.x); const dtb=.05+(s[18]??.5)*.8; this.dF[0]=this.dF[1]=clamp(((s[19]??.5)+fxLfo*.1)*.9,0,.92); this.dM=clamp((s[21]??.3)*.7,0,1); this.rM=clamp((s[25]??.2)*.6,0,1); this.cF=.7+clamp((s[31]??.5),0,1)*.28;
                                      this.cD=.05+clamp((s[32]??.5),0,1)*.7*(.5+this.complexity*.5); this.aF=.4+(s[33]??.5)*.3; const bpm=BASE_BPM+(s[19]??.5)*120; const spb=60/clamp(bpm,40,240); const smpb=spb*this.sr; const smps=smpb/4;
                                      const srInv=1/this.sr; this.lfoP.f=fract(this.lfoP.f+bufSize*nsr*srInv); this.lfoP.p=fract(this.lfoP.p+bufSize*llr*srInv); this.lfoP.n=fract(this.lfoP.n+bufSize*this.lfoR.n*srInv); this.lfoP.x=fract(this.lfoP.x+bufSize*this.lfoR.x*srInv);
                                      const filtLfo=triLFO(this.lfoP.f); const fBassCut=clamp(bcb+filtLfo*.15,.01,.95); const fNoiseCut=clamp(ncb+filtLfo*.5,.01,.95); const leadLfo=sineLFO(this.lfoP.p); const fLeadCut=clamp(lcb+leadLfo*.4,.01,.95); this.filters.bass.forEach(f=>f.setParams(fBassCut,br,this.sr)); this.filters.noise.forEach(f=>f.setParams(fNoiseCut,nr,this.sr)); this.filters.lead.forEach(f=>f.setParams(fLeadCut,lr,this.sr)); this.filters.hat.forEach(f=>f.setParams(hhp,.1,this.sr));

                                      for(let ch=0;ch<out.length;++ch){ const outCh=out[ch]; const delayB=this.dB[ch]; let dwp=this.dWP[ch]; const combBs=this.cB.map(b=>b[ch]); let cwp=this.cWP.map(p=>p[ch]); let cls=this.cLS.map(s=>s[ch]); const apBs=this.aB.map(b=>b[ch]); let awp=this.aWP.map(p=>p[ch]); const bassF=this.filters.bass[ch]; const noiseF=this.filters.noise[ch]; const leadF=this.filters.lead[ch]; const hatF=this.filters.hat[ch]; const panLfo=ch===0?triLFO(this.lfoP.n):triLFO(fract(this.lfoP.n+.5)); const currDTS=Math.floor(clamp((dtb*(1+panLfo*.1))*this.sr,1,delayB.length-1)); this.dTS[ch]=currDTS;
                                          for(let i=0;i<bufSize;++i){ const ph=this.phase+i; const ct=ph*srInv; const cBeat=ct/spb; const bPhase=fract(cBeat); const sPhase=fract(cBeat*4); const cSix=Math.floor(cBeat*4); const sInM=cSix%16; const bTrig=bPhase<this.lastBeatPhase; const sTrig=sPhase<this.lastSixteenthPhase; if(bTrig)this.bc=(this.bc+1)%16; if(sTrig)this.sc=(this.sc+1)%16; this.lastBeatPhase=bPhase; this.lastSixteenthPhase=sPhase;
                                              let kick=0,bass=0,noise=0,lead=0,hat1=0,hat2=0;
                                              if(bTrig&&(this.bc%4===0||(this.bc%4===2&&hash(this.bc)>.6))){const dec=.015+kt*.12;const penv=80+kt*900;const sp=50+kt*20;const pit=sp+penv*Math.exp(-bPhase/(dec*.04+.001));const amp=Math.exp(-bPhase/dec);const clk=(hash(ct*9123.4)-.5)*Math.exp(-bPhase/.002)*.5;kick=Math.sin(pit*bPhase*2*Math.PI)*amp+clk;kick*=ki;}
                                              let playB=false;const bFreq=41.2*Math.pow(2,bo); switch(bp){case 0:playB=sTrig&&(sInM%4!==0);break; case 1:playB=sTrig&&[1,2,5,6,9,10,13,14].includes(sInM);break; case 2:playB=sTrig&&(sInM%2===1);break; case 3:playB=sTrig&&(hash(cSix)>.3);break; case 4:playB=sTrig&&[0,4,8,12,14].includes(sInM);break;} if(playB){const bEnv=Math.exp(-sPhase/(.04+bd*.15));const sawP=fract(ct*bFreq);let rB=(sawP*2-1)*bEnv;rB=Math.tanh(rB*(1+bd*4));bass=bassF.process(rB*.8).lowpass;}
                                              let playH1=false,playH2=false; switch(hpat){case 0:playH1=sTrig;playH2=sTrig&&(sInM%4===2);break; case 1:playH1=sTrig&&(sInM%2===1);playH2=sTrig&&[6,14].includes(sInM);break; case 2:playH1=sTrig&&sInM%3!==0;playH2=sTrig&&sInM===10;break; case 3:playH1=sTrig&&hash(cSix*1.9)>.4;playH2=sTrig&&hash(cSix*2.1+.5)>.7;break; case 4:playH1=sTrig&&[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15].filter(x=>x%2==0||x%3==0).includes(sInM);playH2=sTrig&&sInM%8===6;break;} if(playH1){hat1=hatF.process((hash(ct*5432.1)*2-1)*Math.exp(-sPhase/hd1)).highpass*h1l;} if(playH2){hat2=hatF.process((hash(ct*9876.5)*2-1)*Math.exp(-sPhase/hd2)).highpass*h2l;}
                                              noise=noiseF.process((hash(ph*.5)-.5)*2*nl).bandpass*2;
                                              let playL=false;const lbf=87.3; switch(lpat){case 0:playL=sTrig&&[0,3,7,10].includes(sInM);break; case 1:playL=sTrig&&(hash(Math.floor(cBeat*2))>.65);break; case 2:playL=sTrig&&(sInM===0||sInM===8||sInM===12);break; case 3:playL=sTrig&&(hash(cSix*1.1)>.85);break; case 4:playL=sTrig&&Math.sin(cBeat*Math.PI*2+Math.sin(cBeat*Math.PI*.5)*2)>.5;break;} if(playL&&lp>.05){const lEnv=Math.exp(-sPhase/ld);const pMod=sineLFO(fract(ct*llr*.6))*lpmd;const nC=hash(Math.floor(cSix/2));const freq=lbf*Math.pow(2,pMod+Math.floor(nC*5)/12);const sawP=fract(ct*freq);const rL=(sawP*2-1)*lEnv;lead=leadF.process(rL*.6*lp).bandpass;}
                                              let dry=kick+bass+hat1+hat2+noise+lead; dry=Math.tanh(dry*1.1);
                                              const drp=(dwp-currDTS+delayB.length)%delayB.length; const delayedS=delayB[Math.floor(drp)]; delayB[dwp]=clamp(dry+delayedS*this.dF[ch],-1,1);
                                              let combSum=0; for(let c=0;c<this.cL.length;c++){const cLSamps=Math.floor(this.cL[c]*this.sr);const crp=(cwp[c]-cLSamps+combBs[c].length)%combBs[c].length;let cOut=combBs[c][crp];cls[c]=cOut*(1-this.cD)+cls[c]*this.cD;combBs[c][cwp[c]]=clamp(dry+cls[c]*this.cF,-1,1);combSum+=cOut;cwp[c]=(cwp[c]+1)%combBs[c].length;} combSum*=.25;
                                              let apIn=combSum; let apOut=0; for(let a=0;a<this.aL.length;a++){const aLSamps=Math.floor(this.aL[a]*this.sr);const arp=(awp[a]-aLSamps+apBs[a].length)%apBs[a].length;apOut=apBs[a][arp];const apProc=clamp(apIn+apOut*this.aF,-1,1);apBs[a][awp[a]]=apProc;apIn=apOut-apProc*this.aF;awp[a]=(awp[a]+1)%apBs[a].length;} const wet=apIn;

                                              // ** FINAL GAIN STAGE - Significantly Increased + Soft Clipping **
                                              let finalSig = dry*(1-this.dM-this.rM) + delayedS*this.dM + wet*this.rM;
                                              // Apply masterLevel parameter (controls internal signal level before final boost)
                                              finalSig *= masterLevel;
                                              // Apply large boost and tanh clipping for loudness
                                              outCh[i] = Math.tanh(finalSig * 35.0); // << INCREASED GAIN HERE (was 8.0)

                                              dwp=(dwp+1)%delayB.length;
                                          } // End sample loop
                                          this.dWP[ch]=dwp; this.cWP.forEach((_,c)=>this.cWP[c][ch]=cwp[c]); this.aWP.forEach((_,a)=>this.aWP[a][ch]=awp[a]); this.cLS.forEach((_,c)=>this.cLS[c][ch]=cls[c]);
                                      } // End channel loop
                                      this.phase+=bufSize; return true;
                                  }
                              }
                              registerProcessor('generative-processor', GenerativeProcessor);
                          `; // End processorCode string

                          const blob = new Blob([processorCode], { type: 'application/javascript' }); const workletURL = URL.createObjectURL(blob);
                          await audioContext.audioWorklet.addModule(workletURL); console.log("AudioWorklet added.");
                          this.audioWorkletNode = new AudioWorkletNode(audioContext, 'generative-processor', { outputChannelCount:[2], parameterData:{masterLevel:masterGain.gain.value} });
                          this.audioWorkletNode.connect(masterGain); console.log("AudioWorklet Node connected."); URL.revokeObjectURL(workletURL); this.isInitialized = true; this.hideWarning();
                          await this.setupMicrophone();
                     } catch (err) { console.error("!!! Worklet Init Failed:", err); if(err.message.includes("Syntax")) showError("Audio Error: Engine syntax error."); else showError("Audio Error: Worklet setup failed."); this.isInitialized=false; this.audioWorkletNode?.disconnect(); this.audioWorkletNode=null; this.micStreamSource?.disconnect(); this.micStreamSource=null; currentInputState.mic.available=false; }
                 } else if (audioContext.state === 'running' && this.isInitialized) { await this.setupMicrophone(); }
                 this.isInitializing = false;
            }

             update(stateVectorTensor, complexityValue) {
                 if (!this.isInitialized || !this.audioWorkletNode || !stateVectorTensor || stateVectorTensor.isDisposed) return;
                 // Update worklet state and complexity
                 if (this.audioWorkletNode.port) {
                     stateVectorTensor.data().then(stateVectorArray => {
                          if (this.audioWorkletNode?.port) { // Re-check port exists
                              this.audioWorkletNode.port.postMessage({ state: stateVectorArray, complexity: complexityValue });
                          }
                     }).catch(e => console.error("Error getting state data for audio:", e));
                 }
                 // Update masterLevel audio parameter based on state[35]
                 const levelParam = this.audioWorkletNode.parameters?.get('masterLevel');
                 if(levelParam && audioContext?.currentTime !== undefined) { // Check AC exists before accessing currentTime
                     stateVectorTensor.slice([0, 35], [1, 1]).data().then(data => {
                          const targetLevel = clamp((data[0] ?? 0.8) * 1.5, 0.0, 2.0); // Map state[35] to 0-2 range
                          levelParam.linearRampToValueAtTime(targetLevel, audioContext.currentTime + 0.07);
                      }).catch(e => console.error("Error reading gain state:", e));
                 }
             }

             getMicrophoneInput() {
                // ... (implementation unchanged from v0.4) ...
                if(!this.isInitialized||!analyserNode||!currentInputState.mic.fft||audioContext?.state!=='running'||!currentInputState.mic.available){currentInputState.mic.fft.fill(-140);return{level:0,fft:currentInputState.mic.fft,available:false,rhythmPeak:0,rhythmTempo:0};} analyserNode.getFloatFrequencyData(currentInputState.mic.fft); let sum=0; let cnt=0; const len=analyserNode.frequencyBinCount; for(let i=0;i<len;i++){const dB=currentInputState.mic.fft[i]; if(isFinite(dB)&&dB>-100){sum+=Math.pow(10,dB/10);cnt++;}else{currentInputState.mic.fft[i]=-140;}} let rms=cnt>0?Math.sqrt(sum/cnt):0; currentInputState.mic.level=clamp(rms*7.0,0,1);
                const micRhythm = analyzeRhythm(currentInputState.mic.fft, audioContext.sampleRate, MIC_FFT_SIZE);
                currentInputState.mic.rhythmPeak = micRhythm.peak;
                currentInputState.mic.rhythmTempo = micRhythm.tempo;
                return { level: currentInputState.mic.level, fft: currentInputState.mic.fft, available: true, rhythmPeak: micRhythm.peak, rhythmTempo: micRhythm.tempo };
             }
             async setupMicrophone() {
                 // ... (implementation unchanged from v0.4) ...
                 if(this.micStreamSource||!this.isInitialized||!audioContext||audioContext.state!=='running'||!analyserNode){return;} console.log("Requesting mic access..."); try{const stream=await navigator.mediaDevices.getUserMedia({audio:{echoCancellation:false,noiseSuppression:false,autoGainControl:false},video:false}); this.micStreamSource=audioContext.createMediaStreamSource(stream); this.micStreamSource.connect(analyserNode); console.log("Mic connected."); currentInputState.mic.available=true; if(currentInputState.mic.fft.length!==analyserNode.frequencyBinCount){currentInputState.mic.fft=new Float32Array(analyserNode.frequencyBinCount); console.log("Mic FFT resized:", analyserNode.frequencyBinCount);} } catch(err){console.error("Mic access denied/failed:", err); showWarning("Mic Disabled/Denied.", 5000); currentInputState.mic.available=false; currentInputState.mic.fft.fill(-140);}
             }
         } // End AudioController


        // --- Input Handling (Updated Reset Gesture) ---
        function setupInputListeners() {
            const canvas = document.getElementById('renderCanvas');
            const resetButton = document.getElementById('resetButton'); // Still used programmatically

            // Pointer Events
            const handlePointerMove = (e) => { /* ... (unchanged from v0.4) ... */ const x=clamp(e.clientX/window.innerWidth,0,1); const y=1-clamp(e.clientY/window.innerHeight,0,1); currentInputState.touch.dx=x-currentInputState.touch.lastX; currentInputState.touch.dy=y-currentInputState.touch.lastY; currentInputState.touch.x=x; currentInputState.touch.y=y; currentInputState.touch.lastX=x; currentInputState.touch.lastY=y; currentInputState.touch.pressure=(e.pressure!==undefined&&e.pointerType==='touch')?e.pressure:(currentInputState.touch.active?1:0); if(currentInputState.touch.active)e.preventDefault(); };
            const handlePointerDown = (e) => {
                e.preventDefault(); // Prevent default actions like text selection or double-tap zoom
                const now = performance.now();
                // Reset gesture part 2: Check for quick tap after long press release
                if (resetGestureState.longPressDetected && (now - resetGestureState.longPressReleaseTime < RESET_SECOND_TAP_WINDOW_MS)) {
                    console.log("Reset Gesture Confirmed: Long press + Quick Tap!");
                    resetButton.click(); // Trigger the reset
                    // Reset gesture state fully
                    clearTimeout(resetGestureState.resetTimeout);
                    resetGestureState.longPressDetected = false;
                    resetGestureState.pointerDownTime = 0;
                    resetGestureState.longPressReleaseTime = 0;
                    return; // Don't process as normal interaction
                }

                // If not completing reset gesture, handle as normal interaction
                currentInputState.touch.active = true;
                currentInputState.touch.dx = 0;
                currentInputState.touch.dy = 0;
                currentInputState.touch.lastX = clamp(e.clientX / window.innerWidth, 0, 1);
                currentInputState.touch.lastY = 1 - clamp(e.clientY / window.innerHeight, 0, 1);
                currentInputState.touch.x = currentInputState.touch.lastX;
                currentInputState.touch.y = currentInputState.touch.lastY;
                handlePointerMove(e); // Update pressure etc.

                audioController?.tryInitializeAudio();
                requestMotionPermission(); // Request sensor permissions on interaction
                requestAccelerometerPermission();
                // Start speech after interaction if not already running/starting
                speechController?.requestPermissionAndInit().then(() => {
                    if (speechController && !speechController.isActive && !speechController.isStarting) {
                        speechController.startListening();
                    }
                });


                // Reset gesture part 1: Record start time
                resetGestureState.pointerDownTime = now;
                resetGestureState.longPressDetected = false; // Reset detection flag
                resetGestureState.longPressReleaseTime = 0;
                // Clear any lingering timeout from previous attempts
                if (resetGestureState.resetTimeout) clearTimeout(resetGestureState.resetTimeout);
            };
            const handlePointerUp = (e) => {
                e.preventDefault();
                const now = performance.now();
                if (currentInputState.touch.active) { // Only process if it was an active touch
                    const pressDuration = now - resetGestureState.pointerDownTime;

                    // Reset gesture part 1 check: Was it a long press?
                    if (pressDuration > LONG_PRESS_DURATION_MS && resetGestureState.pointerDownTime > 0) {
                        console.log("Long press detected (> " + LONG_PRESS_DURATION_MS + "ms). Waiting for second tap...");
                        resetGestureState.longPressDetected = true;
                        resetGestureState.longPressReleaseTime = now;
                        // Set a timeout to cancel the reset state if no second tap occurs
                        resetGestureState.resetTimeout = setTimeout(() => {
                            if (resetGestureState.longPressDetected) {
                                console.log("Reset gesture timed out.");
                                resetGestureState.longPressDetected = false;
                                resetGestureState.longPressReleaseTime = 0;
                            }
                        }, RESET_SECOND_TAP_WINDOW_MS);
                    } else {
                        // If it wasn't a long press, ensure long press state is cleared
                        resetGestureState.longPressDetected = false;
                        resetGestureState.longPressReleaseTime = 0;
                         if (resetGestureState.resetTimeout) clearTimeout(resetGestureState.resetTimeout);
                    }

                    // Normal interaction end
                    currentInputState.touch.active = false;
                    currentInputState.touch.pressure = 0;
                    currentInputState.touch.dx = 0;
                    currentInputState.touch.dy = 0;
                    resetGestureState.pointerDownTime = 0; // Clear down time
                }
            };
            canvas.addEventListener('pointerdown', handlePointerDown, { passive: false });
            canvas.addEventListener('pointerup', handlePointerUp, { passive: false });
            canvas.addEventListener('pointerleave', handlePointerUp, { passive: false }); // Treat leave as up
            canvas.addEventListener('pointermove', handlePointerMove, { passive: false });

            // Reset Button (Programmatic trigger)
            resetButton.addEventListener('click', () => { console.log("Resetting state..."); showWarning("Resetting...", 2000); speechController?.stopListening(); try { localStorage.removeItem(LOCAL_STORAGE_KEY); console.log("Cleared localStorage."); setTimeout(() => { window.location.reload(); }, 500); } catch (e) { console.error("Error clearing storage:", e); showError("Failed to clear state."); } });

            // Device Orientation & Motion Listeners
            let motionListenerAdded = false; const requestMotionPermission = () => { /* ... (unchanged from v0.4) ... */ if(motionListenerAdded)return; if(typeof DeviceOrientationEvent!=='undefined'&&typeof DeviceOrientationEvent.requestPermission==='function'){ console.log("Requesting Orientation perm..."); DeviceOrientationEvent.requestPermission().then(p=>{if(p==='granted'){console.log("Orientation granted.");window.addEventListener('deviceorientation',handleOrientation,true);motionListenerAdded=true;}else{console.warn("Orientation denied.");showWarning("Motion Sensor Disabled.",5000);currentInputState.motion.available=false;}}).catch(e=>{console.error("Orientation perm err:",e);showWarning("Motion Sensor Error.",5000);currentInputState.motion.available=false;}); }else{console.log("Adding orientation listener directly.");window.addEventListener('deviceorientation',handleOrientation,true);motionListenerAdded=true;} };
            const handleOrientation = (e) => { /* ... (unchanged from v0.4) ... */ if(e.alpha!==null||e.beta!==null||e.gamma!==null){currentInputState.motion.alpha=e.alpha||0;currentInputState.motion.beta=e.beta||0;currentInputState.motion.gamma=e.gamma||0;if(!currentInputState.motion.available){console.log("Orientation data received.");currentInputState.motion.available=true;}} };
            let accelListenerAdded = false; const requestAccelerometerPermission = () => { /* ... (unchanged from v0.4) ... */ if (accelListenerAdded) return; if (typeof DeviceMotionEvent !== 'undefined' && typeof DeviceMotionEvent.requestPermission === 'function') { console.log("Requesting Motion (Accel) perm..."); DeviceMotionEvent.requestPermission().then(p => { if (p === 'granted') { console.log("Motion (Accel) granted."); window.addEventListener('devicemotion', handleMotion, true); accelListenerAdded = true; } else { console.warn("Motion (Accel) denied."); showWarning("Accelerometer Disabled.", 5000); currentInputState.accelerometer.available = false; } }).catch(err => { console.error("Accel perm err:", err); showWarning("Accelerometer Error.", 5000); currentInputState.accelerometer.available = false; }); } else { console.log("Adding devicemotion listener directly."); window.addEventListener('devicemotion', handleMotion, true); accelListenerAdded = true; } };
            const handleMotion = (event) => { /* ... (unchanged from v0.4) ... */ if (event.accelerationIncludingGravity && (event.accelerationIncludingGravity.x != null || event.accelerationIncludingGravity.y != null || event.accelerationIncludingGravity.z != null)) { const acc = event.accelerationIncludingGravity; currentInputState.accelerometer.x = acc.x || 0; currentInputState.accelerometer.y = acc.y || 0; currentInputState.accelerometer.z = acc.z || 0; currentInputState.accelerometer.magnitude = Math.sqrt(acc.x**2 + acc.y**2 + acc.z**2); if (!currentInputState.accelerometer.available) { console.log("Accelerometer data received."); currentInputState.accelerometer.available = true; } const accelHistory = currentInputState.accelerometer.history; accelHistory.shift(); accelHistory.push(currentInputState.accelerometer.magnitude); } };
        }

        // --- Speech Command Handling ---
        function handleSpeechCommand(command) {
             console.log("Executing Speech Command:", command);
             switch (command) {
                 case 'CREATE': artifactManager?.createArtifact(currentResonantState); break;
                 case 'FORGET_OLDEST': artifactManager?.forgetOldestArtifact(); break;
                 case 'RESET': document.getElementById('resetButton').click(); break; // Still allow voice reset
                 default: console.warn("Unknown speech command received:", command);
             }
         }
        // --- Visual Feedback Trigger ---
        function triggerVisualFeedback(intensity = 0.5, duration = 0.1) {
            visualFeedback.intensity = Math.max(visualFeedback.intensity, intensity); // Use max intensity if triggered rapidly
            visualFeedback.startTime = performance.now() / 1000.0;
            visualFeedback.duration = duration;
            visualFeedback.active = true;
        }


        // --- Main Game Loop ---
        let lastTimestamp = 0;
        async function gameLoop(timestamp) {
             requestAnimationFrame(gameLoop);

             const currentTime = timestamp / 1000.0 || (lastTimestamp / 1000.0 + 0.016);
             if (!interactionOccurred && !loadStateFromLocalStorage()) { return; } // Wait for interaction or loaded state
             if (!tf.ready() || !audioController?.isInitialized || graphicsController?.material === undefined) { return; } // Wait for essentials

             await tf.ready();
             const deltaTime = Math.min(0.1, (timestamp - lastTimestamp) / 1000.0 || 0.016);
             lastTimestamp = timestamp;

             // FPS & Dynamic Complexity Adjustment (Slightly more reactive)
             frameCount++; const fpsElapsed = (timestamp - lastFpsTime)/1000.0;
             if (fpsElapsed >= 1.0) {
                 currentFPS = frameCount / fpsElapsed;
                 lastFpsTime = timestamp;
                 frameCount = 0;
                 const error = TARGET_FPS - currentFPS;
                 // Adjust complexity more strongly based on the error magnitude
                 const baseAdj = 0.018;
                 const errorFactor = 1.0 + Math.abs(error) / TARGET_FPS; // Increase adjustment if error is large
                 const adj = clamp(error * baseAdj * errorFactor, -0.10, 0.10); // Wider adjustment range
                 complexityLevel = clamp(complexityLevel + adj, 0.05, 1.0);
             }

             // Accelerometer Rhythm Analysis
             if (currentInputState.accelerometer.available && currentTime - lastAccelTime > ACCEL_ANALYSIS_INTERVAL_S) {
                  // ... (implementation unchanged from v0.4) ...
                  lastAccelTime = currentTime;
                   let peak = 0; let sum = 0;
                   const history = currentInputState.accelerometer.history;
                   for(let k=0; k<history.length; k++) { sum += history[k]; peak = Math.max(peak, history[k]); }
                   const avg = history.length > 0 ? sum / history.length : 0;
                   const dynamicRange = peak - avg;
                   currentInputState.accelerometer.rhythmPeak = clamp(dynamicRange / 10, 0, 1);
                   currentInputState.accelerometer.rhythmTempo = 60 + currentInputState.accelerometer.rhythmPeak * 120;
              }


             let frameMemory = { numBytes: 0, numTensors: 0 }; let newStateTensor = null;
             try {
                 tf.engine().startScope(); // Start TF Scope

                 // Get Inputs
                 currentInputState.mic = audioController?.getMicrophoneInput() ?? currentInputState.mic;
                 const newIntentTensor = await inputProcessorModel.process(currentInputState);
                 tf.dispose(unifiedIntentVector); unifiedIntentVector = tf.keep(newIntentTensor);

                 // Ambient Sync Factor (Using simple peak correlation for now)
                 let potentialSync = 0;
                 if (currentInputState.mic.available && currentInputState.accelerometer.available) {
                     const micPeak = currentInputState.mic.rhythmPeak;
                     const accelPeak = currentInputState.accelerometer.rhythmPeak;
                     // Basic correlation: high if both are active
                     potentialSync = micPeak * accelPeak * 1.5;
                     // Future: Compare micTempo and accelTempo for better sync detection
                 }
                 currentInputState.syncFactor = clamp(currentInputState.syncFactor * SYNC_DECAY + potentialSync * (1.0 - SYNC_DECAY), 0.0, 1.0);

                 // Artifact Creation Check (Influenced by Sync)
                 const now = Date.now();
                 if (embeddingsReady && artifactManager && now - lastArtifactCreationTime > ARTIFACT_CREATION_INTERVAL_MS) {
                      lastArtifactCreationTime = now;
                      const stateMean = await currentResonantState.mean().data();
                      const activityLevel = Math.abs(stateMean[0]-.5)*2 + currentInputState.mic.level*.5 + currentInputState.accelerometer.rhythmPeak*.3;
                      // Lower threshold if highly synced (encourage capturing synced moments)
                      // Slightly raise threshold if not synced (avoid capturing dissonant moments unless very active)
                      const syncModifier = 1.0 - (currentInputState.syncFactor * 0.6 - (1.0-currentInputState.syncFactor)*0.1);
                      const creationThreshold = ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MIN * syncModifier;

                      if (activityLevel > creationThreshold && activityLevel < ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MAX && artifactManager.getArtifactCount() < MAX_ARTIFACTS) {
                           const created = await artifactManager.createArtifact(currentResonantState);
                           if(created && currentInputState.syncFactor > SYNC_THRESHOLD) { console.log("Artifact created during high ambient sync!"); }
                      }
                 }

                 // RAG Search
                 activeArtifactInfo.keptVectors.forEach(v => tf.dispose(v)); activeArtifactInfo.keptVectors = [];
                 activeArtifactInfo = await artifactManager.findRelevantArtifacts(currentResonantState, ARTIFACT_SIMILARITY_THRESHOLD, MAX_ACTIVE_ARTIFACTS);

                 // Core Logic Prediction
                 newStateTensor = await coreLogicModel.predict(unifiedIntentVector, currentResonantState, activeArtifactInfo.keptVectors, activeArtifactInfo.similarities);
                 tf.dispose(currentResonantState); currentResonantState = tf.keep(newStateTensor);

                 frameMemory = tf.memory(); // Record memory usage within scope
             } catch(e) { console.error("Loop error:", e); frameMemory = tf.memory(); }
             finally { tf.engine().endScope(); } // End TF Scope

             // Post-Scope Updates
             const currentStateArray = await currentResonantState.data(); // Get data after scope disposal

             // Update Visual Feedback Intensity Decay
             let currentFeedbackIntensity = 0;
             if (visualFeedback.active) {
                 const elapsed = currentTime - visualFeedback.startTime;
                 if (elapsed < visualFeedback.duration) { currentFeedbackIntensity = visualFeedback.intensity * (1.0 - (elapsed / visualFeedback.duration)**0.5); } // Slightly slower decay
                 else { visualFeedback.active = false; visualFeedback.intensity = 0; } // Reset intensity when done
             }

             // Update Components
             graphicsController?.update(currentStateArray, currentTime, activeArtifactInfo, complexityLevel, currentInputState.syncFactor, currentFeedbackIntensity);
             audioController?.update(currentResonantState, complexityLevel); // Pass current state and complexity

             // Update Debug Info
             if (USE_DEBUG && document.getElementById('debugInfo')) {
                 // ... (debug info generation unchanged from v0.4) ...
                 const d = document.getElementById('debugInfo'); const { numBytes, numTensors } = frameMemory; const artCnt = artifactManager?.getArtifactCount() ?? 0; const actArt = activeArtifactInfo.ids.join(',')||'n'; const mtn = currentInputState.motion.available?`${currentInputState.motion.beta.toFixed(0)},${currentInputState.motion.gamma.toFixed(0)}`:'N'; const mic = currentInputState.mic.available?`${currentInputState.mic.level.toFixed(2)}[${currentInputState.mic.rhythmPeak.toFixed(1)}]`:'N'; const acc = currentInputState.accelerometer.available?`${currentInputState.accelerometer.magnitude.toFixed(1)}[${currentInputState.accelerometer.rhythmPeak.toFixed(1)}]`:'N'; const be = tf.getBackend()||'N'; const speech = speechController ? `${speechController.isListening?'Lstn':'Idle'}${speechController.isActive?'/Act':''}(${speechController.consecutiveErrorCount})` : 'N/A';
                 d.textContent = `FPS:${currentFPS.toFixed(1)}|Cmplx:${complexityLevel.toFixed(2)}|Sync:${currentInputState.syncFactor.toFixed(2)}|Ctx:${audioContext?.state??'N'}|Tch:${currentInputState.touch.active?'A':'I'}(${currentInputState.touch.x.toFixed(1)},${currentInputState.touch.y.toFixed(1)})|Mtn:${mtn}|Acc:${acc}|Mic:${mic}|Speech:${speech}|Art:${artCnt}(${actArt})|Emb:${embeddingsReady?'OK':'No'}|TF[${be}]:${numTensors}t/${(numBytes/1e6).toFixed(1)}MB`;
             }
        }

        // --- Persistence ---
        function saveStateToLocalStorage() {
             // ... (implementation unchanged from v0.4) ...
             if (!currentResonantState || currentResonantState.isDisposed || !artifactManager) { console.warn("Save skip: Not ready."); return; } console.log("Saving state...");
             try { currentResonantState.data().then(stateArray => { const serializableArts = artifactManager.artifacts.map(art => ({ ...art, stateVector: Array.from(art.stateVector), embedding: Array.from(art.embedding) })); const stateToSave = { resonantState: Array.from(stateArray), artifacts: serializableArts, timestamp: Date.now(), version: VERSION }; const stateJSON = JSON.stringify(stateToSave); localStorage.setItem(LOCAL_STORAGE_KEY, stateJSON); console.log(`State (${(stateJSON.length / 1024).toFixed(1)} KB) saved.`); }).catch(e => console.error("Error getting tensor data for save:", e)); } catch (e) { console.error("Save state error:", e); if (e.name === 'QuotaExceededError') { showError("Save Failed: Storage full."); } else { showWarning("Could not save state.", 3000); } }
         }
        function loadStateFromLocalStorage() {
            try { const stateJSON = localStorage.getItem(LOCAL_STORAGE_KEY); if (!stateJSON) { console.log("No saved state found."); return false; } console.log("Loading state..."); const savedState = JSON.parse(stateJSON); if (!savedState || savedState.version !== VERSION || !savedState.resonantState || !savedState.artifacts) { console.warn(`Invalid/old state found (Need v${VERSION}, got v${savedState?.version}). Clearing and starting fresh.`); localStorage.removeItem(LOCAL_STORAGE_KEY); return false; } tf.tidy(() => { const loadedTensor = tf.tensor1d(savedState.resonantState).expandDims(0); tf.dispose(currentResonantState); currentResonantState = tf.keep(loadedTensor); console.log("Restored state tensor."); }); if (artifactManager) { const loadedArtifacts = savedState.artifacts.map(art => ({ ...art, embedding: new Float32Array(art.embedding) })); artifactManager.setArtifacts(loadedArtifacts); } else { console.warn("Artifact manager not ready during load."); } interactionOccurred = true; showWarning("Loaded previous state.", 3000); console.log(`State loaded from: ${new Date(savedState.timestamp).toLocaleString()}`); return true; } catch (e) { console.error("Load state error:", e); showError("Failed to load state. Starting fresh."); localStorage.removeItem(LOCAL_STORAGE_KEY); return false; }
        }

        // --- Initialization ---
        async function initialize() {
            console.log(`Initializing Infundibulum Echoes v${VERSION}`);
            const warningDiv = document.getElementById('warningInfo');
            if (USE_DEBUG) document.getElementById('debugInfo').style.display = 'block';

            try { graphicsController = new GraphicsController(document.getElementById('renderCanvas')); } catch(e) { showError("Fatal: WebGL Init Failed."); console.error("Gfx Ctrl init fail:", e); return; }
            audioController = new AudioController(); // Instantiate early

            try {
                await tf.ready();
                await tf.setBackend(HIGH_PERFORMANCE_MODE ? 'webgl' : 'cpu');
                console.log(`TF Ready. Backend: ${tf.getBackend()}`);
                if (tf.getBackend()==='webgl'){
                    tf.env().set('WEBGL_CONV_IM2COL',false);
                    tf.env().set('WEBGL_PACK',false);
                    tf.env().set('WEBGL_FLUSH_THRESHOLD', -1); // Flush as needed, -1 might be default
                    tf.env().set('WEBGL_USE_SHAPES_UNIFORMS',true);
                }
                // tf.enableProdMode(); // Keep disabled for easier debugging for now
                console.log("TF configured.");
             } catch (err) { showError("Fatal: TF Backend Init Failed."); console.error("TF setup fail:", err); return; }

            inputProcessorModel = new PlaceholderInputProcessor(null, INPUT_VECTOR_SIZE);
            coreLogicModel = new PlaceholderCoreLogic(STATE_VECTOR_SIZE);
            featureExtractor = new FeatureExtractor(STATE_VECTOR_SIZE);
            embeddingProvider = new EmbeddingProvider(EMBEDDING_MODEL_NAME);
            artifactManager = new ArtifactManager(MAX_ARTIFACTS, STATE_VECTOR_SIZE, EMBEDDING_DIM, featureExtractor, embeddingProvider);
            speechController = new SpeechRecognitionController(); // Instantiate speech controller
            speechController.setCommandCallback(handleSpeechCommand); // Set callback
            console.log("Logic/RAG/Speech components instantiated.");

            const loaded = loadStateFromLocalStorage();
            if (!loaded) {
                 tf.keep(currentResonantState); // Ensure initial state is kept if not loading
                 console.log("Initialized with default state.");
            }

            // Start loading embeddings in the background
            embeddingProvider.init().catch(e => console.error("BG Embed Model init err:", e));

            setupInputListeners(); // Setup input listeners including new reset gesture
            window.addEventListener('resize', () => { graphicsController?.resize(); }, { passive: true });
            document.addEventListener('visibilitychange', () => { if (document.hidden) { saveStateToLocalStorage(); speechController?.stopListening(); } else { /* Don't auto-start speech, wait for interaction */ } });
            window.addEventListener('pagehide', saveStateToLocalStorage); // Use pagehide for more reliable save on mobile/unload

            if (!renderer) { showError("Init Error: Renderer invalid."); return; }
            console.log("Initialization complete.");
            if (!loaded) { warningDiv.textContent = "Ready. Interact or Speak. Say 'Reset Echoes' to restart."; }
            else { warningDiv.textContent = "State loaded. Exploring... Say 'Reset Echoes' to restart."; }

            // Initialize timestamps and start loop
            lastTimestamp = performance.now(); lastFpsTime = lastTimestamp; lastAccelTime = lastTimestamp / 1000.0;
            requestAnimationFrame(gameLoop);
        }

        // --- Utilities ---
        function hasRTX3060LevelGPU() { /* ... (unchanged from v0.4) ... */ try{const c=document.createElement('canvas');const g=c.getContext('webgl')||c.getContext('experimental-webgl');if(!g)return false;const d=g.getExtension('WEBGL_debug_renderer_info');if(d){const r=g.getParameter(d.UNMASKED_RENDERER_WEBGL);if(r.match(/NVIDIA/i)&&(r.match(/RTX/i)||r.match(/GTX.*(10[7-9]|16[6-9]|20|30|40)/i)))return true;if(r.match(/AMD/i)&&r.match(/Radeon.*(RX.*(5[7-9]00|6[6-9]00|7[6-9]00)|Vega|VII|PRO)/i))return true;}}catch(e){}return false; }
        function showError(msg) { const w=document.getElementById('warningInfo'); if(w){w.textContent=`FATAL: ${msg}`;w.style.color='red';w.style.display='block';if(audioController?.warningTimeout)clearTimeout(audioController.warningTimeout);} console.error(`FATAL: ${msg}`); }
        function showWarning(msg, dur=5000) { audioController?.showWarning(msg, dur); }
        function showLoading(show, msg="") { const l=document.getElementById('loadingInfo');const p=document.getElementById('loadingProgress'); if(l){l.style.display=show?'flex':'none';l.style.flexDirection='column';l.style.alignItems='center';l.style.justifyContent='center';if(show&&p){l.childNodes[0].nodeValue=msg+'\n';p.textContent='';}} }
        function clamp(v,min,max){return Math.max(min,Math.min(v,max));}
        function fract(n){return n-Math.floor(n);}

        // --- Entry Point ---
        if (typeof tf !== 'undefined' && typeof THREE !== 'undefined') {
            setTimeout(initialize, 50); // Short delay allows page elements to settle
        } else {
            showError("Fatal: Core libs (TF.js/Three.js) failed. Check network/CDN.");
            console.error("TF.js or Three.js script failed to load.");
        }

    </script>
</body>
</html>
