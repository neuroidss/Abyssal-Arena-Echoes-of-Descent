<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, viewport-fit=cover">
    <title>Infundibulum Echoes</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; cursor: none; }
        canvas { display: block; }
        #debugInfo { position: absolute; top: 10px; left: 10px; color: rgba(255,255,255,0.5); font-family: monospace; font-size: 10px; display: none; /* Enable for debugging */ }
        #warningInfo { position: absolute; bottom: 10px; left: 10px; color: yellow; font-family: sans-serif; font-size: 12px; display: none; background-color: rgba(0,0,0,0.5); padding: 5px; border-radius: 3px;}
    </style>
</head>
<body>
    <canvas id="renderCanvas"></canvas>
    <div id="debugInfo"></div>
    <div id="warningInfo"></div> <!-- Added for warnings like audio failure -->

    <!-- Libraries are now imported in the module script -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.min.js"></script> <-- REMOVED -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script> <!-- TF.js often works well globally -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1"></script> <-- REMOVED -->


    <!-- Main Game Script -->
    <script type="module"> // Use module for potential future AudioWorklet separation

        // Import necessary libraries as ES Modules
        // Use a CDN that serves ES Modules directly (like jsdelivr or skypack - unpkg might also work)
        // Note: Using unpkg requires ?module in URL sometimes, jsdelivr often works directly.
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js';
        // Import Transformers.js - we might not use it in the placeholder, but importing fixes loading errors
        // Just importing it might make its classes available globally or scoped depending on the bundle
        // Let's assume for now it exposes `Xenova` or similar, or we adapt the placeholder later.
        // For now, we just need to prevent the syntax error during load.
        // If direct import fails, we might need to dynamically import or check CDN docs.
        try {
             // Trying a dynamic import as static might interfere if library isn't pure ESM
             // const { pipeline } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1');
             // For now, let's comment out the direct usage, focus on other errors
             console.log("Transformers.js import placeholder - check if library loads without syntax error.");
        } catch (e) {
             console.error("Failed to load Transformers.js module:", e);
             // Handle inability to load transformers if needed
        }


        // --- Constants & Configuration ---
        const USE_DEBUG = true; // Enable debug overlay to see info
        const HIGH_PERFORMANCE_MODE = hasRTX3060LevelGPU(); // Simple check for PC quality
        const INPUT_VECTOR_SIZE = 128; // Size of the unified input/intent vector
        // *** FIX: Make state size match input size for placeholder NN ***
        const STATE_VECTOR_SIZE = 128; // Size of the resonant state vector (WAS 256)
        const MAX_PEERS = 64; // Max peers for local multiplayer simulation

        // --- Global State (within module scope) ---
        let renderer, scene, camera, audioContext, masterGain, analyserNode;
        let inputProcessorModel, coreLogicModel;
        let currentInputState = { touch: { x: 0.5, y: 0.5, active: false, pressure: 0 }, motion: { alpha: 0, beta: 0, gamma: 0 }, mic: { level: 0, fft: null } };
        let unifiedIntentVector = new Array(INPUT_VECTOR_SIZE).fill(0);
        let currentResonantState = new Array(STATE_VECTOR_SIZE).fill(0.5); // Initial state
        let graphicsController, audioController;
        let peers = {}; // Key: peerId, Value: { intentVector: tf.Tensor1D | null, timestamp: number }
        let localPeerCommunicator; // Placeholder for the complex local communication
        let audioWarningDisplayed = false;

        // --- NN Model Placeholders ---
        // In a real scenario, these would load external model files (TFJS format)
        // For now, they are functional placeholders.
        class PlaceholderInputProcessor {
            async process(inputData) {
                // Simple hash/mapping simulation for demonstration
                // Real model would use Transformers.js embeddings/processing
                 return tf.tidy(() => { // Wrap in tidy
                    const vec = new Array(INPUT_VECTOR_SIZE).fill(0);
                    vec[0] = inputData.touch.x;
                    vec[1] = inputData.touch.y;
                    vec[2] = inputData.touch.active ? 1 : 0;
                    vec[3] = inputData.motion.alpha / 360;
                    vec[4] = inputData.motion.beta / 180;
                    vec[5] = inputData.motion.gamma / 90;
                    vec[6] = inputData.mic.level;
                    // Add more complex mapping based on input combination...
                    // Ensure vector length matches INPUT_VECTOR_SIZE
                    return tf.tensor1d(vec.slice(0, INPUT_VECTOR_SIZE)); // Return a TF.js Tensor
                });
            }
        }

        class PlaceholderCoreLogic {
             constructor() {
                 // Internal state for simulation variation over time
                 this.simulationTime = 0;
             }

            async predict(intentVectorTensor, currentStateTensor, peerStateTensors) {
                 // Real model would be a stateful TF.js model (RNN/LSTM/etc.)
                 return tf.tidy(() => {
                    let combinedIntent = intentVectorTensor.clone(); // Start with player's intent
                    this.simulationTime += 1 / 60; // Rough time increment based on frame rate assumption

                    const numRealPeers = Object.keys(peerStateTensors).length;

                    if (numRealPeers > 0) {
                        // --- Real Peer Aggregation (as before) ---
                        let avgPeerIntent = tf.zerosLike(intentVectorTensor);
                        let peerCount = 0;
                        Object.values(peerStateTensors).forEach(peerTensor => {
                            if(peerTensor && !peerTensor.isDisposed) { // Check tensor validity
                                avgPeerIntent = avgPeerIntent.add(peerTensor);
                                peerCount++;
                            }
                        });

                        if (peerCount > 0) {
                            avgPeerIntent = avgPeerIntent.div(tf.scalar(peerCount));
                            // Blend player intent with average peer intent
                            combinedIntent = combinedIntent.add(avgPeerIntent).div(tf.scalar(2));
                        } else {
                            // Ensure avgPeerIntent is disposed if not used
                             avgPeerIntent.dispose();
                        }
                        // Dispose the temporary avgPeerIntent tensor used for calculation
                         if (peerCount > 0) avgPeerIntent.dispose();


                    } else {
                        // --- Simulated Peer Influence ---
                        // No real peers connected, generate ghost influence
                        const simIntensity = 0.3; // How strong is the simulated influence?
                        const simSpeed = 0.1; // How fast does the simulated influence change?

                        // Generate a time-varying noise vector as ghost intent
                        // Use sine waves based on simulationTime for smoother variation
                         const ghostIntentArray = new Array(INPUT_VECTOR_SIZE).fill(0).map((_, idx) => {
                             // Mix different sine frequencies for variety
                             const freq1 = Math.sin(this.simulationTime * simSpeed * (1.0 + idx * 0.05));
                             const freq2 = Math.sin(this.simulationTime * simSpeed * 0.5 * (1.0 + idx * 0.03));
                             return (freq1 + freq2) * 0.5 * simIntensity; // Scale intensity
                         });

                         const ghostIntentTensor = tf.tensor1d(ghostIntentArray);

                         // Combine player intent with ghost intent (e.g., simple average)
                         combinedIntent = combinedIntent.add(ghostIntentTensor).div(tf.scalar(2.0));

                         // Dispose the temporary ghost tensor
                         ghostIntentTensor.dispose();
                    }


                    // --- State Update Logic (as before) ---
                    const noise = tf.randomUniform(currentStateTensor.shape, -0.03, 0.03); // Slightly reduced noise maybe
                    const influence = combinedIntent; // Use the combined intent directly

                    let nextState = currentStateTensor
                        .mul(tf.scalar(0.97)) // Slightly faster decay/reaction?
                        .add(influence.mul(tf.scalar(0.15))) // Slightly stronger influence?
                        .add(noise)
                        .clipByValue(0, 1);

                    // combinedIntent is potentially a result of additions/divisions, managed by tidy
                    // currentStateTensor needs management outside
                    // noise is created/disposed within tidy

                    return nextState;
                 });
            }
        }

        // --- Graphics Controller (Three.js) ---
        class GraphicsController {
            constructor(canvas) {
                this.canvas = canvas;
                // Basic Three.js setup (using imported THREE)
                scene = new THREE.Scene();
                camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
                camera.position.z = 5;
                renderer = new THREE.WebGLRenderer({ canvas: this.canvas, antialias: !HIGH_PERFORMANCE_MODE }); // Adjust AA based on perf
                renderer.setSize(window.innerWidth, window.innerHeight);
                renderer.setPixelRatio(window.devicePixelRatio);

                // *** Unique Visuals Setup ***
                const geometry = new THREE.PlaneGeometry(window.innerWidth/100, window.innerHeight/100, 1, 1); // Scaled plane
                this.material = new THREE.ShaderMaterial({
                    uniforms: {
                        time: { value: 0.0 },
                        resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) },
                        // *** FIX: Pass correctly sized array ***
                        stateVector: { value: new Float32Array(STATE_VECTOR_SIZE).fill(0.5) } // Pass state to shader
                    },
                    vertexShader: `
                        varying vec2 vUv;
                        void main() {
                            vUv = uv;
                            gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
                        }
                    `,
                    // Fragment shader creates the unique visuals based on stateVector
                    // Inside GraphicsController class constructor:

                    fragmentShader: `
                        uniform float time;
                        uniform vec2 resolution;
                        uniform float stateVector[${STATE_VECTOR_SIZE}]; // Receive state
                        varying vec2 vUv;

                        // *** RESTORED FUNCTION BODIES ***
                        float hash(float n) { return fract(sin(n) * 43758.5453); }

                        float noise(vec2 x) {
                            vec2 p = floor(x);
                            vec2 f = fract(x);
                            f = f * f * (3.0 - 2.0 * f); // smoothstep calculation
                            float n = p.x + p.y * 57.0;
                            return mix(mix(hash(n + 0.0), hash(n + 1.0), f.x),
                                       mix(hash(n + 57.0), hash(n + 58.0), f.x), f.y);
                        }

                        float fbm(vec2 p, float H) {
                            float G = exp2(-H); // Hurst exponent factor
                            float f = 1.0;      // Frequency
                            float a = 1.0;      // Amplitude
                            float t = 0.0;      // Total
                            int iterations = 5; // Keep iterations reasonable for performance
                            for (int i = 0; i < iterations; i++) {
                                t += a * noise(f * p);
                                f *= 2.0; // Double frequency
                                a *= G;   // Reduce amplitude based on H
                            }
                            return t;
                        }

                        vec3 hsv2rgb(vec3 c) {
                            vec4 K = vec4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0);
                            vec3 p = abs(fract(c.xxx + K.xyz) * 6.0 - K.www);
                            return c.z * mix(K.xxx, clamp(p - K.xxx, 0.0, 1.0), c.y);
                        }
                        // *** END RESTORED FUNCTION BODIES ***


                        // Simple pulse function based on time and frequency
                        float pulse(float t, float freq) {
                            return 0.5 + 0.5 * cos(t * freq * 2.0 * 3.14159); // Cosine pulse (0 to 1)
                        }


                        void main() {
                            vec2 uv = (vUv - 0.5) * 2.0; // Center UVs (-1 to 1)
                            float dist = length(uv); // Distance from center


                            // --- State Vector Mapping (Examples) ---
                            float kickEffect = stateVector[0];
                            float bassFilterViz = stateVector[2];
                            float arpSpeedViz = stateVector[5];
                            float noiseIntensityViz = stateVector[9];

                            // --- Visual Effects ---

                            // 1. Base Fractal Noise
                            float baseFreq = 1.0 + arpSpeedViz * 4.0;
                            float complexity = 0.5 + stateVector[1] * 0.4; // Use another state element
                            float speed = 0.1 + arpSpeedViz * 0.5;
                            float n = fbm(uv * baseFreq + vec2(time * speed, time * speed * 0.5), complexity);

                            // 2. Color
                            float hue = fract(time * 0.05 + n * 0.1 + bassFilterViz * 0.5);
                            float saturation = mix(0.6, 1.0, stateVector[4]);
                            float value = mix(0.3, 0.9, stateVector[8]);

                            // 3. Rhythmic Pulse
                            float bpm = 145.0;
                            float beatsPerSecond = bpm / 60.0;
                            float beatPulse = pulse(time, beatsPerSecond);
                            value *= mix(0.8, 1.2, beatPulse * kickEffect);

                            // 4. Distortion / Warp
                            float warpAmount = noiseIntensityViz * 0.3;
                            vec2 warpOffset = vec2(noise(uv + time * 0.2), noise(uv - time * 0.2 + vec2(5.0))) * warpAmount;
                            vec2 warpedUv = uv + warpOffset;
                            // Recalculate based on warped UVs for more psychedelic effect
                            float n_warped = fbm(warpedUv * baseFreq + vec2(time*speed), complexity);
                            hue = fract(time*0.05 + n_warped*0.1 + bassFilterViz*0.5);


                            // 5. Vignette / Center Focus
                            value *= pow(1.0 - dist * 0.5, 2.0);


                            // --- Final Color Calculation ---
                            vec3 hsv = vec3(hue, saturation, value);
                            vec3 rgb = hsv2rgb(hsv);

                            // Add some noise grain
                            rgb += (hash(time + vUv.x * vUv.y) - 0.5) * 0.05 * noiseIntensityViz;


                            gl_FragColor = vec4(clamp(rgb, 0.0, 1.0), 1.0);
                        }
                    ` 
                });
                const mesh = new THREE.Mesh(geometry, this.material);
                scene.add(mesh);
            }

            update(stateVector, time) {
                // Update shader uniforms
                this.material.uniforms.time.value = time;
                // Update state vector uniform (ensure it's the correct type if needed, like Float32Array)
                this.material.uniforms.stateVector.value = Float32Array.from(stateVector); // Update with current state
                renderer.render(scene, camera);
            }

            resize() {
                camera.aspect = window.innerWidth / window.innerHeight;
                camera.updateProjectionMatrix();
                renderer.setSize(window.innerWidth, window.innerHeight);
                renderer.setPixelRatio(window.devicePixelRatio);
                this.material.uniforms.resolution.value.set(window.innerWidth, window.innerHeight);
            }
        }


        // --- Audio Controller (Web Audio API + Generative) ---
        class AudioController {
            constructor() {
                this.audioWorkletNode = null;
                this.isInitialized = false;
                this.pendingState = null;
                // Attempt initial setup on first interaction
                document.body.addEventListener('pointerdown', () => this.tryInitializeAudio(), { once: true });

                // Handle visibility change for AudioContext suspension/resumption
                document.addEventListener('visibilitychange', async () => { // Make async
                    if (!this.isInitialized || !audioContext) return;
                    if (document.hidden) {
                       await audioContext.suspend().catch(e => console.warn("Error suspending AudioContext:", e));
                    } else {
                       await audioContext.resume().catch(e => console.warn("Error resuming AudioContext:", e));
                         // If context was lost or failed to resume, try re-init
                        if (audioContext.state !== 'running') {
                             console.warn("AudioContext state is not 'running' after resume, attempting re-initialization.");
                             this.showWarning("Audio Issue: Re-initializing...");
                             await this.tryInitializeAudio(true); // Force re-init
                        }
                    }
                });
            }

             showWarning(message) {
                const warningDiv = document.getElementById('warningInfo');
                if (warningDiv) {
                    warningDiv.textContent = message;
                    warningDiv.style.display = 'block';
                    audioWarningDisplayed = true; // Flag that a warning is shown
                }
             }
             hideWarning() {
                 const warningDiv = document.getElementById('warningInfo');
                 if (warningDiv && audioWarningDisplayed) { // Only hide if we showed one
                     warningDiv.style.display = 'none';
                     audioWarningDisplayed = false;
                 }
             }


            async tryInitializeAudio(force = false) {
                // Don't try if already initialized unless forced
                if (this.isInitialized && !force) return;
                // Don't try if already attempting initialization
                if (this.isInitializing && !force) return;

                this.isInitializing = true; // Prevent race conditions


                 // Cleanup previous context if forcing re-init
                 if (force && audioContext) {
                     console.log("Forcing audio re-initialization. Closing previous context.");
                     await audioContext.close().catch(e => console.error("Error closing previous AudioContext:", e));
                     audioContext = null;
                     this.audioWorkletNode = null; // Ensure node is cleared
                     this.isInitialized = false;
                 }

                 // Check if context needs creation
                if (!audioContext || audioContext.state === 'closed') {
                     console.log("Creating new AudioContext.");
                     try {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                         // If context starts suspended, try resuming after setup
                         let resumeNeeded = audioContext.state === 'suspended';

                        masterGain = audioContext.createGain();
                        masterGain.gain.setValueAtTime(0.7, audioContext.currentTime); // Default volume
                        masterGain.connect(audioContext.destination);

                        analyserNode = audioContext.createAnalyser();
                        analyserNode.fftSize = 256; // Smaller FFT for mic input state
                        analyserNode.smoothingTimeConstant = 0.3;

                        // Load the AudioWorklet processor using Blob URL
                        // *** IMPORTANT: This will likely FAIL if loading HTML from file:// ***
                        // *** Must run from a local web server (http://localhost:...) ***
                        const processorCode = `
                            // Ensure STATE_VECTOR_SIZE is available inside the worklet scope
                            const WORKLET_STATE_SIZE = ${STATE_VECTOR_SIZE};
                            const BPM = 145.0; // Beats Per Minute - classic Psy range
                            const SECONDS_PER_BEAT = 60.0 / BPM;
                            const SIXTEENTH_NOTE_DURATION = SECONDS_PER_BEAT / 4.0;

                            // Simple LFO utility
                            function sineLFO(phase, rate) {
                                return (Math.sin(2 * Math.PI * phase * rate) + 1.0) * 0.5; // 0 to 1 range
                            }

                            class GenerativeProcessor extends AudioWorkletProcessor {
                                constructor() {
                                    super();
                                    this.phase = 0; // Samples elapsed
                                    this.beatPhase = 0; // Phase within the current beat (0 to 1)
                                    this.sixteenthNoteCounter = 0; // Track 16th notes for patterns
                                    // *** FIX: Use correct state size ***
                                    this.state = new Array(WORKLET_STATE_SIZE).fill(0.5);

                                    // Simple low-pass filter state (per channel) for noise filtering
                                    this.noiseFilterState = [0.0, 0.0];


                                    this.port.onmessage = (event) => {
                                        if (event.data.state && event.data.state.length === WORKLET_STATE_SIZE) {
                                            this.state = event.data.state;
                                        }
                                    };
                                }

                                static get parameterDescriptors() {
                                    return [{ name: 'masterLevel', defaultValue: 0.7, minValue: 0, maxValue: 1 }];
                                }

                                // Simple resonant low-pass filter (approximation)
                                processFilter(input, cutoff, resonance, stateIndex) {
                                    // cutoff is 0-1 (maps to audio frequency range)
                                    // resonance is 0-1 (maps to ~0-20 Q factor)
                                    // VERY basic IIR filter - not accurate but provides some color
                                    const freq = Math.pow(cutoff, 3) * (sampleRate * 0.4); // Exponential curve for frequency
                                    const q = 1.0 + resonance * 19.0;
                                    const w0 = 2 * Math.PI * freq / sampleRate;
                                    const alpha = Math.sin(w0) / (2 * q);
                                    const cosw0 = Math.cos(w0);
                                    const b0 = (1 - cosw0) / 2;
                                    const b1 = 1 - cosw0;
                                    const b2 = (1 - cosw0) / 2;
                                    const a0 = 1 + alpha;
                                    const a1 = -2 * cosw0;
                                    const a2 = 1 - alpha;

                                    // Simplified direct form II - uses state, assumes previous input/output were 0 for simplicity
                                    // This is NOT a proper filter implementation, just a placeholder effect.
                                    // A real implementation requires storing previous inputs/outputs correctly.
                                    let filtered = input * (b0 / a0); // Crude approximation
                                    this.noiseFilterState[stateIndex] = filtered; // Store for next sample (incorrectly used here)
                                    return filtered;

                                    // NOTE: Proper biquad filter implementation is more complex and recommended for real use.
                                }


                                process(inputs, outputs, parameters) {
                                    const output = outputs[0];
                                    const bufferSize = output[0].length;
                                    const masterLevel = parameters.masterLevel.length > 1 ? parameters.masterLevel[0] : parameters.masterLevel[0]; // Handle parameter array type


                                    // --- State Vector Mapping (Examples - adjust indices as needed) ---
                                    const kickIntensity = 0.6 + (this.state[0] || 0.5) * 0.4; // State[0]: Kick loudness/presence
                                    const kickTimbre = (this.state[1] || 0.5); // State[1]: Kick decay/pitch env amt

                                    const bassFilterCutoff = 0.1 + (this.state[2] || 0.5) * 0.4; // State[2]: Bass filter base cutoff
                                    const bassFilterResonance = (this.state[3] || 0.5) * 0.8; // State[3]: Bass filter resonance
                                    const bassPatternVariation = Math.floor((this.state[4] || 0.5) * 3); // State[4]: Select bass pattern (0-2)

                                    const arpRate = 4.0 + (this.state[5] || 0.5) * 8.0; // State[5]: Arp speed (Hz)
                                    const arpModulation = (this.state[6] || 0.5); // State[6]: Arp pitch modulation amount

                                    const noiseFilterCutoff = (this.state[7] || 0.5); // State[7]: Noise filter cutoff LFO mod amount
                                    const noiseFilterResonance = (this.state[8] || 0.5) * 0.9; // State[8]: Noise filter resonance
                                    const noiseLevel = 0.05 + (this.state[9] || 0.5) * 0.2; // State[9]: Noise base level

                                    const leadSynthMod = (this.state[10] || 0.5); // State[10]: Control some aspect of a lead sound


                                    for (let channel = 0; channel < output.length; ++channel) {
                                        const outputChannel = output[channel];
                                        for (let i = 0; i < bufferSize; ++i) {
                                            const currentTime = (this.phase + i) / sampleRate; // Absolute time in seconds
                                            const samplesPerBeat = sampleRate * SECONDS_PER_BEAT;
                                            const currentBeat = currentTime / SECONDS_PER_BEAT;
                                            const beatPhase = currentBeat % 1.0; // Phase within the current beat (0-1)

                                            // Calculate 16th note timing
                                            const currentSixteenth = Math.floor(currentBeat * 4);
                                            const sixteenthPhase = (currentBeat * 4) % 1.0; // Phase within current 16th note
                                            const sixteenthTrigger = (currentSixteenth !== this.sixteenthNoteCounter);
                                            if(sixteenthTrigger) this.sixteenthNoteCounter = currentSixteenth;


                                            // --- Synthesis Components ---

                                            // 1. Kick Drum (Classic 4/4)
                                            let kick = 0.0;
                                            const kickTriggerPhase = 0.01; // Small offset to avoid clicking at exact boundary
                                            if (beatPhase < kickTriggerPhase) { // Trigger on every beat start
                                                const decay = 0.05 + kickTimbre * 0.15; // State controls decay
                                                const pitchEnvAmount = 200 + kickTimbre * 400;
                                                const pitch = 60 + pitchEnvAmount * Math.exp(-beatPhase / (decay * 0.1)); // Pitch envelope
                                                const envelope = Math.exp(-beatPhase / decay);
                                                kick = Math.sin(2 * Math.PI * pitch * beatPhase) * envelope * kickIntensity; // Use beatPhase as time within kick sound
                                            }


                                            // 2. Bassline (Rolling / Offbeat Feel)
                                            let bass = 0.0;
                                            const sixteenthNoteInMeasure = currentSixteenth % 16; // Pattern repeats every 16 sixteenths (1 measure)

                                            // Bassline patterns (triggered on 16th notes)
                                             let bassNoteTrigger = false;
                                             switch(bassPatternVariation) {
                                                  case 0: // Simple rolling bass
                                                       bassNoteTrigger = sixteenthTrigger && (sixteenthNoteInMeasure % 4 !== 0); // Play on offbeats of 8ths
                                                       break;
                                                  case 1: // More active pattern
                                                        bassNoteTrigger = sixteenthTrigger && ([1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15].includes(sixteenthNoteInMeasure));
                                                        break;
                                                  case 2: // Galloping feel variation
                                                        bassNoteTrigger = sixteenthTrigger && ([0, 2, 3, 4, 6, 7, 8, 10, 11, 12, 14, 15].includes(sixteenthNoteInMeasure));
                                                        break;
                                             }


                                            if (bassNoteTrigger) {
                                                 const bassDecay = 0.15; // Short decay for distinct notes
                                                 const bassEnvelope = Math.exp(-sixteenthPhase / bassDecay);
                                                 // Basic sawtooth oscillator for bass
                                                 const bassFreq = 55.0; // Base note (A) - could be modulated by state too
                                                 const sawPhase = (currentTime * bassFreq) % 1.0;
                                                 let rawBass = (sawPhase * 2.0 - 1.0) * bassEnvelope; // Sawtooth

                                                 // Apply simple resonant filter (using the placeholder function)
                                                 const filterCutoffMod = sineLFO(currentBeat, 0.25); // Slow LFO on filter
                                                 const finalCutoff = Math.max(0.01, Math.min(0.99, bassFilterCutoff + filterCutoffMod * 0.3));
                                                 // Apply per-channel filter approx
                                                 bass = this.processFilter(rawBass, finalCutoff, bassFilterResonance, channel);

                                                 // Reduce gain slightly
                                                 bass *= 0.5;
                                            }

                                            // 3. Simple Arp/Lead Element
                                            const arpFreq = 110.0 * Math.pow(2, Math.floor(sineLFO(currentTime, arpRate) * 8 + arpModulation * 4) / 12.0); // Pitch sequence/arp based on LFO + state mod
                                            let arpSound = Math.sin(2 * Math.PI * currentTime * arpFreq) * 0.1; // Simple sine wave
                                             // Add a basic envelope triggered rhythmically?
                                             if(sixteenthTrigger && sixteenthNoteInMeasure % 2 === 0) { // Trigger on 8th notes
                                                  arpSound *= Math.exp(-sixteenthPhase / 0.2);
                                             } else {
                                                 arpSound = 0; // Silence if not triggered
                                             }


                                            // 4. Filtered Noise Sweep / Atmospheric Layer
                                            let noise = (Math.random() * 2 - 1) * noiseLevel;
                                            const noiseCutoffLFO = sineLFO(currentTime, 0.1); // Slow sweep
                                            const finalNoiseCutoff = Math.max(0.01, Math.min(0.99, noiseCutoffLFO * noiseFilterCutoff));
                                             let filteredNoise = this.processFilter(noise, finalNoiseCutoff, noiseFilterResonance, channel);
                                             filteredNoise *= 0.6; // Adjust level

                                            // 5. Placeholder for another state-driven synth layer
                                            let leadSound = Math.sin(2 * Math.PI * currentTime * 440 * (1 + leadSynthMod * sineLFO(currentTime, 2.0))) * 0.1 * leadSynthMod;
                                            if(sixteenthTrigger && sixteenthNoteInMeasure === 0) { // Trigger on beat
                                                  leadSound *= Math.exp(-sixteenthPhase / 0.8); // Longer decay
                                            } else { leadSound = 0;}



                                            // --- Combine Signals ---
                                            let sampleValue = kick + bass + arpSound + filteredNoise + leadSound;

                                            // Apply master level parameter & simple limiter/clip
                                            outputChannel[i] = Math.max(-1.0, Math.min(1.0, sampleValue * masterLevel));
                                        }
                                    }
                                    this.phase += bufferSize; // Advance overall phase for next block

                                    // Update beat phase tracking - needed for accurate timing across blocks if bufferSize isn't multiple of beat length
                                    this.beatPhase = (this.phase / sampleRate / SECONDS_PER_BEAT) % 1.0;


                                    return true; // Keep processor alive
                                }
                            }
                            registerProcessor('generative-processor', GenerativeProcessor);
                        `;
                        const blob = new Blob([processorCode], { type: 'application/javascript' });
                        const workletURL = URL.createObjectURL(blob);

                         try {
                            await audioContext.audioWorklet.addModule(workletURL);
                            console.log("AudioWorklet Module Added.");
                            this.audioWorkletNode = new AudioWorkletNode(audioContext, 'generative-processor');

                            // Connect graph: Worklet -> Gain -> Destination
                            this.audioWorkletNode.connect(masterGain);

                            console.log("AudioContext Initialized and Worklet Node Created.");
                            this.isInitialized = true;
                            this.hideWarning(); // Hide warnings on successful init

                            // Process any pending state update immediately
                            if (this.pendingState) {
                                this.update(this.pendingState);
                                this.pendingState = null;
                            }
                            // Try setting up mic now that context exists
                            await this.setupMicrophone();

                         } catch(moduleError) {
                            console.error("!!! Failed to load AudioWorklet module from Blob URL:", moduleError);
                            console.error("!!! Ensure you are running this HTML from a local web server (http://localhost:...) NOT from file:///");
                            this.showWarning("Audio Worklet Failed! Run from local server.");
                            this.isInitialized = false; // Mark as not initialized
                            // Fallback? Could try ScriptProcessorNode but it's deprecated and less performant
                         } finally {
                             URL.revokeObjectURL(workletURL); // Clean up blob URL regardless of success/failure
                         }

                         // Resume context if it started suspended or was forced
                         if (resumeNeeded || force) {
                             if(audioContext.state === 'suspended') {
                                 console.log("Attempting to resume AudioContext...");
                                 await audioContext.resume().catch(e => {
                                     console.error("Failed to resume AudioContext after initialization:", e);
                                     this.showWarning("Audio Disabled: Failed to resume context. Click/Tap again?");
                                 });
                             }
                         }
                         // Final check
                         if (audioContext.state === 'running') {
                             console.log("AudioContext is running.");
                         } else {
                              console.warn(`AudioContext state is '${audioContext.state}' after initialization attempt.`);
                         }


                    } catch (e) {
                        console.error("Failed to initialize AudioContext:", e);
                         this.showWarning("Audio Failed: Cannot create AudioContext.");
                        this.isInitialized = false;
                    } finally {
                        this.isInitializing = false; // Mark initialization attempt as complete
                    }
                } else if (audioContext.state === 'suspended') {
                    console.log("AudioContext exists but is suspended, attempting resume...");
                    await audioContext.resume().catch(e => {
                        console.error("Failed to resume existing suspended AudioContext:", e);
                        this.showWarning("Audio Disabled: Failed resume. Click/Tap again?");
                    });
                } else {
                    // Context exists and is running or closed (if closed, logic above handles it)
                    console.log(`AudioContext state: ${audioContext.state}`);
                     this.isInitializing = false;
                }
            }

            update(stateVector) {
                if (!this.isInitialized || !this.audioWorkletNode) {
                    // console.warn("Audio not ready, queuing state update.");
                    this.pendingState = stateVector; // Store state to apply once initialized
                    // Don't try to init here, wait for user interaction or visibility change
                    return;
                }
                 // Send the entire state vector to the worklet
                 if (stateVector && stateVector.length === STATE_VECTOR_SIZE) {
                    this.audioWorkletNode.port.postMessage({ state: stateVector });
                 } else {
                    // console.warn("Invalid state vector length for audio update.");
                 }

                // Example using AudioParam (optional)
                 const levelParam = this.audioWorkletNode.parameters.get('masterLevel');
                 if (levelParam) {
                    // Map a state element to master level (e.g., state[7]) with smoothing
                    const targetLevel = Math.max(0, Math.min(1, stateVector[7] || 0.7)); // Use state[7] if available, default 0.7
                    levelParam.linearRampToValueAtTime(targetLevel, audioContext.currentTime + 0.05); // Smooth transition
                 }
            }

            getMicrophoneInput() {
                 // Ensure analyserNode exists and audio is running
                if (!this.isInitialized || !analyserNode || !currentInputState.mic.fft || audioContext?.state !== 'running') {
                     return { level: 0, fft: null };
                 }

                const bufferLength = analyserNode.frequencyBinCount;
                 // Ensure buffer is allocated correctly
                 if (!currentInputState.mic.fft || currentInputState.mic.fft.length !== bufferLength) {
                    currentInputState.mic.fft = new Float32Array(bufferLength);
                 }

                analyserNode.getFloatFrequencyData(currentInputState.mic.fft); // Get FFT data

                // Calculate average volume (RMS or peak) - handle -Infinity dB
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    if (isFinite(currentInputState.mic.fft[i])) { // Check for -Infinity
                        sum += Math.pow(10, currentInputState.mic.fft[i] / 20); // Convert dB to amplitude
                    }
                }
                let rms = Math.sqrt(sum / bufferLength);
                // Adjust multiplier for normalization - needs tuning
                currentInputState.mic.level = Math.min(1, Math.max(0, rms * 10)); // Normalize/clamp level

                // Return normalized level and the FFT data array
                return { level: currentInputState.mic.level, fft: currentInputState.mic.fft };
            }

             async setupMicrophone() {
                  // Check if already set up or if audio context isn't ready/running
                 if (this.micStreamSource || !this.isInitialized || !audioContext || audioContext.state !== 'running') {
                     if (!this.isInitialized || audioContext?.state !== 'running') {
                         console.warn("AudioContext not ready/running for microphone setup.");
                     }
                     return;
                 }
                 try {
                     console.log("Requesting microphone access...");
                     const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false }, video: false });
                     this.micStreamSource = audioContext.createMediaStreamSource(stream);
                     this.micStreamSource.connect(analyserNode); // Connect mic to analyser
                     console.log("Microphone input connected.");
                     // Allocate FFT buffer based on analyser settings
                     currentInputState.mic.fft = new Float32Array(analyserNode.frequencyBinCount);
                 } catch (err) {
                     console.error("Microphone access denied or failed:", err);
                      this.showWarning("Microphone Disabled: Access denied.");
                     // Game can proceed without mic input, `getMicrophoneInput` will return zeros
                 }
             }
        }

        // --- Input Handling ---
        function setupInputListeners() {
            const canvas = document.getElementById('renderCanvas');

            // Touch / Mouse
            const handlePointerMove = (event) => {
                // Use clientX/Y for consistency across pointer types
                currentInputState.touch.x = event.clientX / window.innerWidth;
                currentInputState.touch.y = 1.0 - (event.clientY / window.innerHeight); // Invert Y
                // Normalize pressure if available, otherwise use active state
                currentInputState.touch.pressure = (event.pressure !== undefined && event.pressure !== null) ? event.pressure : (currentInputState.touch.active ? 1.0 : 0);
                if (currentInputState.touch.active) { // Only prevent default if actively interacting
                    event.preventDefault();
                }
            };
            const handlePointerDown = (event) => {
                currentInputState.touch.active = true;
                // Ensure canvas has focus for potential keyboard events if added later
                // canvas.focus();
                handlePointerMove(event); // Update position immediately
                 // Try initializing audio on first interaction (moved to AudioController constructor)
                 audioController?.tryInitializeAudio(); // Attempt resume/init on interaction
                event.preventDefault();
            };
            const handlePointerUp = (event) => {
                currentInputState.touch.active = false;
                currentInputState.touch.pressure = 0;
                 event.preventDefault();
            };

            canvas.addEventListener('pointerdown', handlePointerDown, { passive: false });
            canvas.addEventListener('pointerup', handlePointerUp, { passive: false });
            canvas.addEventListener('pointerleave', handlePointerUp, { passive: false }); // Treat leaving as up
            canvas.addEventListener('pointermove', handlePointerMove, { passive: false });


            // Device Motion / Orientation (Requires HTTPS typically)
            const requestMotionPermission = () => {
                if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
                    DeviceOrientationEvent.requestPermission()
                        .then(permissionState => {
                            if (permissionState === 'granted') {
                                window.addEventListener('deviceorientation', handleOrientation);
                            } else {
                                console.warn("Device Orientation permission denied.");
                                showWarning("Motion Sensor Disabled: Permission Denied.");
                            }
                        })
                        .catch(console.error);
                } else {
                     // Handle standard non-iOS 13+ devices or environments where permission isn't needed/available
                    window.addEventListener('deviceorientation', handleOrientation, true);
                }
            };

            const handleOrientation = (event) => {
                    currentInputState.motion.alpha = event.alpha || 0; // Z axis (compass) - range [0, 360)
                    currentInputState.motion.beta = event.beta || 0;   // X axis (front/back tilt) - range [-180, 180)
                    currentInputState.motion.gamma = event.gamma || 0; // Y axis (left/right tilt) - range [-90, 90)
            };

            // Add a button or interaction to request motion permission on iOS 13+
            // For simplicity here, let's try requesting on the first interaction too
             canvas.addEventListener('pointerdown', requestMotionPermission, { once: true });


             // Microphone setup is now triggered after audio context is initialized successfully
        }


         // --- Multiplayer (Local Network - WebRTC Placeholder) ---
         // Remains largely the same conceptual placeholder - requires external signaling
         class LocalPeerCommunicator {
              constructor() {
                 this.connections = {}; // { peerId: RTCPeerConnection }
                 this.dataChannels = {}; // { peerId: RTCDataChannel }
                 this.myPeerId = 'user_' + Math.random().toString(36).substring(2, 9);
                 this.signalingChannel = null; // NEEDS external setup
                 this.isConnecting = new Set(); // Track peers currently in connection setup

                 console.warn(`Multiplayer requires a LOCAL signaling mechanism (not implemented here) for peer discovery and connection setup. My ID: ${this.myPeerId}`);
                 // TODO: Implement actual signaling logic here
             }

             // --- WebRTC Methods (Simplified stubs - need real signaling) ---

             async createOfferAndSend(peerId) {
                 if (this.connections[peerId] || this.isConnecting.has(peerId)) return;
                 console.log(`Creating connection to ${peerId}`);
                 this.isConnecting.add(peerId);

                 try {
                     const pc = new RTCPeerConnection({ iceServers: [] }); // Use only local candidates if possible
                     this.connections[peerId] = pc;

                     pc.onicecandidate = event => {
                         if (event.candidate) {
                             this.sendSignalingMessage({ type: 'candidate', target: peerId, candidate: event.candidate });
                         }
                     };
                     pc.onconnectionstatechange = () => {
                        if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected' || pc.connectionState === 'closed') {
                            this.cleanupConnection(peerId);
                        } else if (pc.connectionState === 'connected') {
                             this.isConnecting.delete(peerId); // Successfully connected
                        }
                    };


                     const channel = pc.createDataChannel('resonant-weave', { negotiated: false, ordered: false, maxRetransmits: 0 });
                     this.setupDataChannel(peerId, channel);

                     const offer = await pc.createOffer();
                     await pc.setLocalDescription(offer);
                     this.sendSignalingMessage({ type: 'offer', target: peerId, offer: offer });

                 } catch (e) {
                     console.error(`Error creating offer for ${peerId}:`, e);
                     this.cleanupConnection(peerId); // Clean up on error
                 }
             }

             async handleOfferAndCreateAnswer(peerId, offer) {
                  if (this.connections[peerId] || this.isConnecting.has(peerId)) return;
                  console.log(`Handling offer from ${peerId}`);
                  this.isConnecting.add(peerId);

                 try {
                     const pc = new RTCPeerConnection({ iceServers: [] });
                     this.connections[peerId] = pc;

                     pc.onicecandidate = event => {
                         if (event.candidate) {
                              this.sendSignalingMessage({ type: 'candidate', target: peerId, candidate: event.candidate });
                         }
                     };
                     pc.onconnectionstatechange = () => {
                        if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected' || pc.connectionState === 'closed') {
                            this.cleanupConnection(peerId);
                        } else if (pc.connectionState === 'connected') {
                             this.isConnecting.delete(peerId); // Successfully connected
                        }
                    };

                     pc.ondatachannel = event => {
                         this.setupDataChannel(peerId, event.channel);
                     };

                     await pc.setRemoteDescription(new RTCSessionDescription(offer));
                     const answer = await pc.createAnswer();
                     await pc.setLocalDescription(answer);
                     this.sendSignalingMessage({ type: 'answer', target: peerId, answer: answer });

                 } catch (e) {
                      console.error(`Error handling offer/creating answer for ${peerId}:`, e);
                      this.cleanupConnection(peerId); // Clean up on error
                 }
             }

             async handleAnswer(peerId, answer) {
                 const pc = this.connections[peerId];
                 if (pc && pc.signalingState === 'have-local-offer') {
                      try {
                         await pc.setRemoteDescription(new RTCSessionDescription(answer));
                         console.log(`Processed answer from ${peerId}`);
                      } catch (e) {
                          console.error(`Error setting remote description for answer from ${peerId}:`, e);
                      }
                 } else {
                     console.warn(`Received answer from ${peerId} but no connection or wrong state: ${pc?.signalingState}`);
                 }
             }

             async handleCandidate(peerId, candidate) {
                 const pc = this.connections[peerId];
                  try {
                    if (pc && pc.remoteDescription) { // Can only add candidates after remote description is set
                        await pc.addIceCandidate(new RTCIceCandidate(candidate));
                    } else if (pc) {
                        console.warn(`Queuing ICE candidate from ${peerId} until remote description is set.`);
                        // Simple queueing mechanism (could be more robust)
                        if (!pc.queuedCandidates) pc.queuedCandidates = [];
                        pc.queuedCandidates.push(candidate);
                    } else {
                         console.warn(`Received ICE candidate for unknown peer ${peerId}`);
                    }
                 } catch (e) {
                     console.error(`Error adding received ICE candidate for ${peerId}:`, e);
                 }
             }

              // Process queued candidates if any, after setting remote description
              async processQueuedCandidates(peerId) {
                 const pc = this.connections[peerId];
                 if (pc && pc.remoteDescription && pc.queuedCandidates) {
                     console.log(`Processing ${pc.queuedCandidates.length} queued candidates for ${peerId}`);
                     while(pc.queuedCandidates.length > 0) {
                         const candidate = pc.queuedCandidates.shift();
                         try {
                            await pc.addIceCandidate(new RTCIceCandidate(candidate));
                         } catch (e) {
                            console.error(`Error adding queued ICE candidate for ${peerId}:`, e);
                         }
                     }
                 }
              }


             setupDataChannel(peerId, channel) {
                 channel.onopen = () => {
                     console.log(`Data channel OPEN with ${peerId}`);
                     this.dataChannels[peerId] = channel;
                     this.isConnecting.delete(peerId); // Channel open means connection succeeded
                     // Maybe send an initial state or hello message
                 };
                 channel.onclose = () => {
                     console.log(`Data channel CLOSED with ${peerId}`);
                     this.cleanupConnection(peerId); // Ensure full cleanup
                 };
                 channel.onerror = (error) => {
                     console.error(`Data channel error with ${peerId}:`, error);
                      this.cleanupConnection(peerId); // Clean up on error too
                 };
                 channel.onmessage = (event) => {
                      this.handlePeerMessage(peerId, event.data);
                 };
                 // Store the channel reference immediately
                 this.dataChannels[peerId] = channel;
             }

            handlePeerMessage(peerId, data) {
                 try {
                     // Assume receiving intent vectors from peers as JSON arrays
                     const receivedIntentArray = JSON.parse(data);
                     if (receivedIntentArray && receivedIntentArray.length === INPUT_VECTOR_SIZE) {
                         // Dispose previous tensor for this peer if it exists
                         peers[peerId]?.intentVector?.dispose();

                         // Store the latest intent from this peer as a tensor
                         peers[peerId] = {
                             intentVector: tf.tensor1d(receivedIntentArray), // Store as tensor
                             timestamp: Date.now()
                         };
                     } else {
                        console.warn(`Received invalid data from ${peerId}`);
                     }
                 } catch (e) {
                     // console.warn(`Failed to parse message from ${peerId}:`, e, data);
                 }
            }

             broadcastIntent(intentVectorArray) {
                 if (!intentVectorArray) return;
                 // Avoid stringifying in the loop if possible
                 let message = null;
                 Object.values(this.dataChannels).forEach(channel => {
                     if (channel && channel.readyState === 'open') {
                          if (message === null) { // Stringify only once
                             message = JSON.stringify(intentVectorArray);
                         }
                         try {
                             // Check buffer amount before sending if needed (optional)
                             // if (channel.bufferedAmount < SOME_THRESHOLD)
                             channel.send(message);
                         } catch (e) {
                             console.error("Error sending message:", e)
                         }
                     }
                 });
             }

             cleanupConnection(peerId) {
                 console.log(`Cleaning up connection with ${peerId}`);
                 this.isConnecting.delete(peerId);
                 const pc = this.connections[peerId];
                 if (pc) {
                     pc.close();
                     delete this.connections[peerId];
                 }
                 delete this.dataChannels[peerId];
                 // Dispose tensor and remove from game state
                 peers[peerId]?.intentVector?.dispose();
                 delete peers[peerId];
             }

             // --- Signaling Simulation/Interface ---
             // This needs to be connected to your actual signaling implementation
              sendSignalingMessage(msg) {
                  console.log("SIGNALING (to implement):", JSON.stringify(msg));
                  // Example: If using a local WebSocket server:
                  // if (this.signalingChannel && this.signalingChannel.readyState === WebSocket.OPEN) {
                  //    this.signalingChannel.send(JSON.stringify(msg));
                  // }
              }

              // Call this when a message arrives from your signaling server/mechanism
              receiveSignalingMessage(msg) {
                  try {
                      const parsedMsg = JSON.parse(msg);
                      const peerId = parsedMsg.senderId; // Assuming senderId is part of message

                      if (!peerId || peerId === this.myPeerId) return; // Ignore own messages

                      switch(parsedMsg.type) {
                          case 'offer':
                              this.handleOfferAndCreateAnswer(peerId, parsedMsg.offer);
                              break;
                          case 'answer':
                              this.handleAnswer(peerId, parsedMsg.answer);
                              // Process queued candidates AFTER handling answer
                              this.processQueuedCandidates(peerId);
                              break;
                          case 'candidate':
                              this.handleCandidate(peerId, parsedMsg.candidate);
                              break;
                          // Add other signaling message types (discovery, leave, etc.)
                          default:
                              console.warn("Unknown signaling message type:", parsedMsg.type);
                      }
                  } catch (e) {
                      console.error("Error processing signaling message:", e, msg);
                  }
              }
         }


        // --- Game Loop ---
        let lastTimestamp = 0;
        async function gameLoop(timestamp) {
            // Ensure TF backend is ready
            await tf.ready();

            const deltaTime = (timestamp - lastTimestamp) / 1000;
            lastTimestamp = timestamp;

            // --- Declare tensors outside try block to access in finally ---
            let inputTensor = null;
            let prevStateTensor = null;
            let nextStateTensor = null;
            let frameMemory = { numBytes: 0, numTensors: 0 }; // Variable to store memory info

            try {
                // 1. Read Inputs & Update Microphone State
                const micData = audioController?.getMicrophoneInput() ?? { level: 0, fft: null };
                currentInputState.mic.level = micData.level;

                // 2. Process Inputs -> Unified Intent Vector (NN)
                inputTensor = await inputProcessorModel.process(currentInputState);
                unifiedIntentVector = await inputTensor.data(); // Get TypedArray data

                // 3. Multiplayer: Broadcast local intent & Receive/Manage peer intents
                if (localPeerCommunicator) {
                    localPeerCommunicator.broadcastIntent(Array.from(unifiedIntentVector)); // Broadcast array
                    // Cleanup old peer data
                    const now = Date.now();
                    Object.keys(peers).forEach(peerId => {
                        if (now - peers[peerId].timestamp > 10000) {
                            console.log(`Timing out peer ${peerId}`);
                            localPeerCommunicator?.cleanupConnection(peerId);
                        }
                    });
                }

                // 4. Core Logic: Update Resonant State (NN)
                prevStateTensor = tf.tensor1d(currentResonantState); // Create tensor from previous state

                const peerIntentTensors = {}; // Collect valid peer tensors
                Object.entries(peers).forEach(([id, data]) => {
                    if (data.intentVector && !data.intentVector.isDisposed) {
                        peerIntentTensors[id] = data.intentVector;
                    }
                });

                nextStateTensor = await coreLogicModel.predict(inputTensor, prevStateTensor, peerIntentTensors);
                currentResonantState = await nextStateTensor.array(); // Update JS state array

                // *** ===> Get Memory Usage HERE <=== ***
                // Capture memory info BEFORE disposing tensors for this frame
                 frameMemory = tf.memory();


            } catch(e) {
                console.error("Error during game loop computation:", e);
                 // Use fallback state if error occurred
                 if (nextStateTensor && !nextStateTensor.isDisposed) { // If tensor exists but error happened after
                    currentResonantState = await nextStateTensor.array();
                 } else if (prevStateTensor && !prevStateTensor.isDisposed) { // Fallback to prev state tensor
                    currentResonantState = await prevStateTensor.array();
                 } else {
                     // Keep existing JS state as last resort
                 }

                 // Attempt to get memory even on error, might show leaks
                  frameMemory = tf.memory();


            } finally {
                // Cleanup tensors used explicitly in this frame scope
                 // inputTensor might be null if error occurred early
                inputTensor?.dispose();
                 // prevStateTensor might be null if error occurred before its creation
                prevStateTensor?.dispose();
                 // nextStateTensor might be null if error occurred before its creation or during prediction
                nextStateTensor?.dispose();
                // NOTE: Peer tensors stored in `peers` object are disposed during cleanupConnection or when replaced
            }


            // 5. Update Audio & Visuals (using JS state array)
            const time = timestamp / 1000;
            graphicsController?.update(currentResonantState, time);
            audioController?.update(currentResonantState);


            // 6. Debug Info (Optional) - Use the captured frameMemory
            if (USE_DEBUG) {
                const debugDiv = document.getElementById('debugInfo');
                if (debugDiv) {
                     // Calculate peer tensor count separately for clarity
                     let validPeerTensors = 0;
                     Object.values(peers).forEach(p => { if (p.intentVector && !p.intentVector.isDisposed) validPeerTensors++; });

                     // Use the frameMemory captured earlier
                     const { numBytes, numTensors } = frameMemory;
                     debugDiv.textContent = `Touch: ${currentInputState.touch.active ? 'ON' : 'OFF'} (${currentInputState.touch.x.toFixed(2)}, ${currentInputState.touch.y.toFixed(2)}) | Motion: ${currentInputState.motion.beta.toFixed(0)}, ${currentInputState.motion.gamma.toFixed(0)} | Mic Level: ${currentInputState.mic.level.toFixed(2)} | Peers: ${Object.keys(peers).length} (Tensors: ${validPeerTensors}) | TF Mem Frame: ${(numBytes / 1e6).toFixed(2)}MB / ${numTensors} Tensors | Audio: ${audioContext?.state ?? 'No Context'}`;
                 }
            }

            // 7. Request Next Frame
            requestAnimationFrame(gameLoop);
        }

        // --- Initialization ---
        async function initialize() {
            console.log("Initializing Resonant Weaving Experience...");
             if (USE_DEBUG) {
                 document.getElementById('debugInfo').style.display = 'block';
             }

            graphicsController = new GraphicsController(document.getElementById('renderCanvas'));
            audioController = new AudioController(); // Initializes lazily on interaction

             // Crucial: Wait for TF backend to be ready before creating models or running operations
             await tf.ready();
             console.log(`TensorFlow.js backend: ${tf.getBackend()}`);


            // Initialize NN Models (Placeholders)
            inputProcessorModel = new PlaceholderInputProcessor();
            coreLogicModel = new PlaceholderCoreLogic();
            console.log("Neural Network placeholders ready.");

            // Setup input listeners
            setupInputListeners();

            // Initialize Multiplayer (requires local signaling setup)
            localPeerCommunicator = new LocalPeerCommunicator();
            // TODO: Add logic here to actually connect peers using the signaling mechanism
            // e.g., connect to a local WebSocket for signaling:
            // setupSignalingChannel(localPeerCommunicator); // Implement this function

            // Handle window resizing
            window.addEventListener('resize', () => {
                graphicsController?.resize();
            });

            // Check for WebGL support (basic)
            if (!renderer) {
                 showError("Error: WebGL is not supported or enabled. Cannot run experience.");
                return;
            }

            // Remove loading indicator (if any) and start the loop
            console.log("Initialization complete. Starting loop.");
            lastTimestamp = performance.now();
            requestAnimationFrame(gameLoop);
        }

        // --- Utility Functions ---
         function hasRTX3060LevelGPU() {
             try {
                 const canvas = document.createElement('canvas');
                 const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
                 if (gl) {
                     const debugInfo = gl.getExtension('WEBGL_debug_renderer_info');
                     if (debugInfo) {
                         const rendererName = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL);
                         console.log("GPU Renderer:", rendererName);
                         if (!rendererName) return false; // No renderer name found
                         // More robust checks: Look for higher-end keywords, avoid integrated keywords
                         const lowerRenderer = rendererName.toLowerCase();
                         if (lowerRenderer.includes('nvidia') && (lowerRenderer.includes('rtx') || lowerRenderer.includes('geforce gtx 16') || lowerRenderer.includes('geforce gtx 107') || lowerRenderer.includes('geforce gtx 108'))) return true;
                         if (lowerRenderer.includes('radeon') && (lowerRenderer.includes('rx 5') || lowerRenderer.includes('rx 6') || lowerRenderer.includes('rx 7') || lowerRenderer.includes('vega'))) return true; // Include 5000 series and up
                          if (lowerRenderer.includes('apple') && lowerRenderer.includes('m1') || lowerRenderer.includes('m2') || lowerRenderer.includes('m3')) return true; // Apple Silicon is quite capable
                          // Explicitly exclude common integrated graphics
                          if (lowerRenderer.includes('intel') && (lowerRenderer.includes('iris') || lowerRenderer.includes('uhd') || lowerRenderer.includes('hd graphics'))) return false;
                          if (lowerRenderer.includes('llvmpipe') || lowerRenderer.includes('swiftshader') || lowerRenderer.includes('software rasterizer')) return false; // Software rendering
                     }
                 }
             } catch (e) { console.error("Error detecting GPU:", e); }
             // Default to lower performance settings if unsure or on known integrated graphics
             return false;
         }

         function showError(message) {
             // Display error prominently to the user
             const errorDiv = document.createElement('div');
             errorDiv.style.position = 'absolute';
             errorDiv.style.top = '0';
             errorDiv.style.left = '0';
             errorDiv.style.width = '100%';
             errorDiv.style.padding = '20px';
             errorDiv.style.backgroundColor = 'rgba(255, 0, 0, 0.8)';
             errorDiv.style.color = 'white';
             errorDiv.style.textAlign = 'center';
             errorDiv.style.fontSize = '16px';
             errorDiv.style.fontFamily = 'sans-serif';
             errorDiv.style.zIndex = '1000';
             errorDiv.textContent = message;
             document.body.appendChild(errorDiv);
             // Optionally hide canvas or other elements
             document.getElementById('renderCanvas')?.remove();
         }

         function showWarning(message) {
            // Use the dedicated warning div
            const warningDiv = document.getElementById('warningInfo');
            if (warningDiv) {
                warningDiv.textContent = message;
                warningDiv.style.display = 'block';
                // Optionally hide after a few seconds
                 setTimeout(() => { warningDiv.style.display = 'none'; }, 7000);
            } else {
                console.warn(`Warning (UI element #warningInfo not found): ${message}`);
            }
        }


        // --- Start ---
        // IMPORTANT: Ensure TFJS is loaded before initializing
         if (typeof tf !== 'undefined') {
             initialize();
         } else {
            // Wait for TFJS to load if it hasn't already (less common with standard script tag)
             console.warn("TensorFlow.js not found immediately. Waiting for load event (if applicable).");
             window.addEventListener('load', () => {
                 if (typeof tf !== 'undefined') {
                    initialize();
                 } else {
                     showError("Fatal Error: TensorFlow.js failed to load.");
                 }
             });
         }


    </script>
</body>
</html>
