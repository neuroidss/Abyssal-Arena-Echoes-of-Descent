<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, viewport-fit=cover">
    <title>Infundibulum Echoes</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; cursor: none; }
        canvas { display: block; }
        #debugInfo { position: absolute; top: 10px; left: 10px; color: rgba(255,255,255,0.5); font-family: monospace; font-size: 10px; display: none; /* Enable for debugging */ }
        #warningInfo { position: absolute; bottom: 10px; left: 10px; color: yellow; font-family: sans-serif; font-size: 12px; display: none; background-color: rgba(0,0,0,0.5); padding: 5px; border-radius: 3px;}
    </style>
</head>
<body>
    <canvas id="renderCanvas"></canvas>
    <div id="debugInfo"></div>
    <div id="warningInfo"></div> <!-- Added for warnings like audio failure -->

    <!-- Libraries are now imported in the module script -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.min.js"></script> <-- REMOVED -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script> <!-- TF.js often works well globally -->
    <!-- <script src="https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1"></script> <-- REMOVED -->


    <!-- Main Game Script -->
    <script type="module"> // Use module for potential future AudioWorklet separation

        // Import necessary libraries as ES Modules
        // Use a CDN that serves ES Modules directly (like jsdelivr or skypack - unpkg might also work)
        // Note: Using unpkg requires ?module in URL sometimes, jsdelivr often works directly.
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js';
        // Import Transformers.js - we might not use it in the placeholder, but importing fixes loading errors
        // Just importing it might make its classes available globally or scoped depending on the bundle
        // Let's assume for now it exposes `Xenova` or similar, or we adapt the placeholder later.
        // For now, we just need to prevent the syntax error during load.
        // If direct import fails, we might need to dynamically import or check CDN docs.
        try {
             // Trying a dynamic import as static might interfere if library isn't pure ESM
             // const { pipeline } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1');
             // For now, let's comment out the direct usage, focus on other errors
             console.log("Transformers.js import placeholder - check if library loads without syntax error.");
        } catch (e) {
             console.error("Failed to load Transformers.js module:", e);
             // Handle inability to load transformers if needed
        }


        // --- Constants & Configuration ---
        const USE_DEBUG = true; // Enable debug overlay to see info
        const HIGH_PERFORMANCE_MODE = hasRTX3060LevelGPU(); // Simple check for PC quality
        const INPUT_VECTOR_SIZE = 128; // Size of the unified input/intent vector
        // *** FIX: Make state size match input size for placeholder NN ***
        const STATE_VECTOR_SIZE = 128; // Size of the resonant state vector (WAS 256)
        const MAX_PEERS = 64; // Max peers for local multiplayer simulation

        // --- Global State (within module scope) ---
        let renderer, scene, camera, audioContext, masterGain, analyserNode;
        let inputProcessorModel, coreLogicModel;
        let currentInputState = { touch: { x: 0.5, y: 0.5, active: false, pressure: 0 }, motion: { alpha: 0, beta: 0, gamma: 0 }, mic: { level: 0, fft: null } };
        let unifiedIntentVector = new Array(INPUT_VECTOR_SIZE).fill(0);
        let currentResonantState = new Array(STATE_VECTOR_SIZE).fill(0.5); // Initial state
        let graphicsController, audioController;
        let peers = {}; // Key: peerId, Value: { intentVector: tf.Tensor1D | null, timestamp: number }
        let localPeerCommunicator; // Placeholder for the complex local communication
        let audioWarningDisplayed = false;

        // --- NN Model Placeholders ---
        // In a real scenario, these would load external model files (TFJS format)
        // For now, they are functional placeholders.
        class PlaceholderInputProcessor {
            async process(inputData) {
                // Simple hash/mapping simulation for demonstration
                // Real model would use Transformers.js embeddings/processing
                 return tf.tidy(() => { // Wrap in tidy
                    const vec = new Array(INPUT_VECTOR_SIZE).fill(0);
                    vec[0] = inputData.touch.x;
                    vec[1] = inputData.touch.y;
                    vec[2] = inputData.touch.active ? 1 : 0;
                    vec[3] = inputData.motion.alpha / 360;
                    vec[4] = inputData.motion.beta / 180;
                    vec[5] = inputData.motion.gamma / 90;
                    vec[6] = inputData.mic.level;
                    // Add more complex mapping based on input combination...
                    // Ensure vector length matches INPUT_VECTOR_SIZE
                    return tf.tensor1d(vec.slice(0, INPUT_VECTOR_SIZE)); // Return a TF.js Tensor
                });
            }
        }

        class PlaceholderCoreLogic {
            async predict(intentVectorTensor, currentStateTensor, peerStateTensors) {
                 // Simple simulation: blend inputs, add noise, apply activation
                 // Real model would be a stateful TF.js model (RNN/LSTM/etc.)
                 // currentStateTensor and intentVectorTensor now have same shape (INPUT_VECTOR_SIZE)
                 return tf.tidy(() => {
                    let combinedIntent = intentVectorTensor.clone();

                    // Aggregate peer influence (simple averaging for demo)
                    let peerCount = 0;
                    let avgPeerIntent = tf.zerosLike(intentVectorTensor); // Use intentVectorTensor shape
                    Object.values(peerStateTensors).forEach(peerTensor => {
                        if(peerTensor) { // Check if tensor exists
                            avgPeerIntent = avgPeerIntent.add(peerTensor);
                            peerCount++;
                        }
                    });
                     if (peerCount > 0) {
                        avgPeerIntent = avgPeerIntent.div(tf.scalar(peerCount));
                        combinedIntent = combinedIntent.add(avgPeerIntent).div(tf.scalar(2)); // Blend local and peer intent
                    } else {
                        // Ensure avgPeerIntent is disposed if not used
                        avgPeerIntent.dispose();
                    }


                    // Simple state update simulation
                    const noise = tf.randomUniform(currentStateTensor.shape, -0.05, 0.05);
                    // *** FIX: Slice size now matches combinedIntent size (implicitly INPUT_VECTOR_SIZE) ***
                    // We don't even need slice if we use the whole intent vector for influence directly
                    // const influence = combinedIntent.slice([0], [INPUT_VECTOR_SIZE]); // Slice fixed
                    const influence = combinedIntent; // Use the whole combined intent

                    let nextState = currentStateTensor
                        .mul(tf.scalar(0.98)) // Dampen previous state
                        .add(influence.mul(tf.scalar(0.1))) // Add scaled influence (Shapes now match)
                        .add(noise) // Add randomness
                        .clipByValue(0, 1); // Keep state within bounds

                    // Don't dispose combinedIntent if it's avgPeerIntent + original intentVectorTensor
                    // intentVectorTensor is disposed outside the call
                    // avgPeerIntent is disposed above if peerCount is 0
                    // currentStateTensor is previous state, should be managed outside typically
                    // influence is just an alias to combinedIntent (or a slice)

                    return nextState; // Return TF.js Tensor
                 });
            }
        }

        // --- Graphics Controller (Three.js) ---
        class GraphicsController {
            constructor(canvas) {
                this.canvas = canvas;
                // Basic Three.js setup (using imported THREE)
                scene = new THREE.Scene();
                camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
                camera.position.z = 5;
                renderer = new THREE.WebGLRenderer({ canvas: this.canvas, antialias: !HIGH_PERFORMANCE_MODE }); // Adjust AA based on perf
                renderer.setSize(window.innerWidth, window.innerHeight);
                renderer.setPixelRatio(window.devicePixelRatio);

                // *** Unique Visuals Setup ***
                const geometry = new THREE.PlaneGeometry(window.innerWidth/100, window.innerHeight/100, 1, 1); // Scaled plane
                this.material = new THREE.ShaderMaterial({
                    uniforms: {
                        time: { value: 0.0 },
                        resolution: { value: new THREE.Vector2(window.innerWidth, window.innerHeight) },
                        // *** FIX: Pass correctly sized array ***
                        stateVector: { value: new Float32Array(STATE_VECTOR_SIZE).fill(0.5) } // Pass state to shader
                    },
                    vertexShader: `
                        varying vec2 vUv;
                        void main() {
                            vUv = uv;
                            gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
                        }
                    `,
                    // Fragment shader creates the unique visuals based on stateVector
                    fragmentShader: `
                        uniform float time;
                        uniform vec2 resolution;
                        // *** FIX: Define array with correct size in shader ***
                        uniform float stateVector[${STATE_VECTOR_SIZE}]; // Receive state
                        varying vec2 vUv;

                        // GLSL hash, noise, fbm functions (as before)
                        float hash(float n) { return fract(sin(n) * 43758.5453); }
                        float noise(vec2 x) {
                            vec2 p = floor(x);
                            vec2 f = fract(x);
                            f = f * f * (3.0 - 2.0 * f);
                            float n = p.x + p.y * 57.0;
                            return mix(mix(hash(n + 0.0), hash(n + 1.0), f.x),
                                       mix(hash(n + 57.0), hash(n + 58.0), f.x), f.y);
                        }
                        float fbm(vec2 p, float H) {
                            float G = exp2(-H);
                            float f = 1.0;
                            float a = 1.0;
                            float t = 0.0;
                            // Use stateVector elements safely within bounds
                            int iterations = 3 + int(stateVector[6] * 5.0); // Example: State[6] controls iterations
                            for (int i = 0; i < 10; i++) { // Max loop iterations (GLSL limitation)
                                if (i >= iterations) break;
                                t += a * noise(f * p);
                                f *= 2.0;
                                a *= G;
                            }
                            return t;
                        }

                        vec3 hsv2rgb(vec3 c) {
                            vec4 K = vec4(1.0, 2.0 / 3.0, 1.0 / 3.0, 3.0);
                            vec3 p = abs(fract(c.xxx + K.xyz) * 6.0 - K.www);
                            return c.z * mix(K.xxx, clamp(p - K.xxx, 0.0, 1.0), c.y);
                        }

                        void main() {
                            vec2 uv = vUv;
                            // Safely access stateVector elements (indices 0 to STATE_VECTOR_SIZE-1)
                            float baseFreq = stateVector[0] * 5.0 + 1.0; // State[0] controls frequency
                            float hue = stateVector[1]; // State[1] controls color hue (0-1)
                            float complexity = stateVector[2] * 0.5 + 0.5; // State[2] controls H param
                            float speed = stateVector[3] * 2.0; // State[3] controls animation speed
                            float saturation = mix(0.5, 1.0, stateVector[4]); // State[4] controls saturation
                            float value = mix(0.2, 0.8, stateVector[5]); // State[5] controls brightness/value

                            float n = fbm(uv * baseFreq + vec2(time * speed * 0.1, 0.0), complexity);

                            vec3 hsv = vec3(fract(n + hue), saturation, value);
                            vec3 rgb = hsv2rgb(hsv);

                            gl_FragColor = vec4(rgb, 1.0);
                        }
                    `
                });
                const mesh = new THREE.Mesh(geometry, this.material);
                scene.add(mesh);
            }

            update(stateVector, time) {
                // Update shader uniforms
                this.material.uniforms.time.value = time;
                // Update state vector uniform (ensure it's the correct type if needed, like Float32Array)
                this.material.uniforms.stateVector.value = Float32Array.from(stateVector); // Update with current state
                renderer.render(scene, camera);
            }

            resize() {
                camera.aspect = window.innerWidth / window.innerHeight;
                camera.updateProjectionMatrix();
                renderer.setSize(window.innerWidth, window.innerHeight);
                renderer.setPixelRatio(window.devicePixelRatio);
                this.material.uniforms.resolution.value.set(window.innerWidth, window.innerHeight);
            }
        }


        // --- Audio Controller (Web Audio API + Generative) ---
        class AudioController {
            constructor() {
                this.audioWorkletNode = null;
                this.isInitialized = false;
                this.pendingState = null;
                // Attempt initial setup on first interaction
                document.body.addEventListener('pointerdown', () => this.tryInitializeAudio(), { once: true });

                // Handle visibility change for AudioContext suspension/resumption
                document.addEventListener('visibilitychange', async () => { // Make async
                    if (!this.isInitialized || !audioContext) return;
                    if (document.hidden) {
                       await audioContext.suspend().catch(e => console.warn("Error suspending AudioContext:", e));
                    } else {
                       await audioContext.resume().catch(e => console.warn("Error resuming AudioContext:", e));
                         // If context was lost or failed to resume, try re-init
                        if (audioContext.state !== 'running') {
                             console.warn("AudioContext state is not 'running' after resume, attempting re-initialization.");
                             this.showWarning("Audio Issue: Re-initializing...");
                             await this.tryInitializeAudio(true); // Force re-init
                        }
                    }
                });
            }

             showWarning(message) {
                const warningDiv = document.getElementById('warningInfo');
                if (warningDiv) {
                    warningDiv.textContent = message;
                    warningDiv.style.display = 'block';
                    audioWarningDisplayed = true; // Flag that a warning is shown
                }
             }
             hideWarning() {
                 const warningDiv = document.getElementById('warningInfo');
                 if (warningDiv && audioWarningDisplayed) { // Only hide if we showed one
                     warningDiv.style.display = 'none';
                     audioWarningDisplayed = false;
                 }
             }


            async tryInitializeAudio(force = false) {
                // Don't try if already initialized unless forced
                if (this.isInitialized && !force) return;
                // Don't try if already attempting initialization
                if (this.isInitializing && !force) return;

                this.isInitializing = true; // Prevent race conditions


                 // Cleanup previous context if forcing re-init
                 if (force && audioContext) {
                     console.log("Forcing audio re-initialization. Closing previous context.");
                     await audioContext.close().catch(e => console.error("Error closing previous AudioContext:", e));
                     audioContext = null;
                     this.audioWorkletNode = null; // Ensure node is cleared
                     this.isInitialized = false;
                 }

                 // Check if context needs creation
                if (!audioContext || audioContext.state === 'closed') {
                     console.log("Creating new AudioContext.");
                     try {
                        audioContext = new (window.AudioContext || window.webkitAudioContext)();
                         // If context starts suspended, try resuming after setup
                         let resumeNeeded = audioContext.state === 'suspended';

                        masterGain = audioContext.createGain();
                        masterGain.gain.setValueAtTime(0.7, audioContext.currentTime); // Default volume
                        masterGain.connect(audioContext.destination);

                        analyserNode = audioContext.createAnalyser();
                        analyserNode.fftSize = 256; // Smaller FFT for mic input state
                        analyserNode.smoothingTimeConstant = 0.3;

                        // Load the AudioWorklet processor using Blob URL
                        // *** IMPORTANT: This will likely FAIL if loading HTML from file:// ***
                        // *** Must run from a local web server (http://localhost:...) ***
                        const processorCode = `
                            // Ensure STATE_VECTOR_SIZE is available inside the worklet scope
                            const WORKLET_STATE_SIZE = ${STATE_VECTOR_SIZE};

                            class GenerativeProcessor extends AudioWorkletProcessor {
                                constructor() {
                                    super();
                                    this.phase = 0;
                                    // *** FIX: Use correct state size ***
                                    this.state = new Array(WORKLET_STATE_SIZE).fill(0.5);
                                    this.port.onmessage = (event) => {
                                        if (event.data.state && event.data.state.length === WORKLET_STATE_SIZE) {
                                            this.state = event.data.state;
                                        } else {
                                            // console.warn('Worklet received invalid state data');
                                        }
                                    };
                                }

                                static get parameterDescriptors() {
                                    // Example parameter - not strictly necessary if state controls everything
                                    return [{ name: 'masterLevel', defaultValue: 0.7, minValue: 0, maxValue: 1 }];
                                }

                                process(inputs, outputs, parameters) {
                                    const output = outputs[0]; // Assuming stereo output
                                    const bufferSize = output[0].length;
                                    const masterLevel = parameters.masterLevel[0];

                                    // *** Generative Psytrance Core (using state) ***
                                    // Safely access state elements (indices 0 to WORKLET_STATE_SIZE-1)
                                    const mainFreq = 40 * (1 + (this.state[0] || 0) * 4); // State[0] modifies pitch (40-200Hz range)
                                    const filterQ = 1 + (this.state[1] || 0) * 10;        // State[1] controls a filter resonance/Q
                                    const rhythmIntensity = this.state[2] || 0;           // State[2] controls rhythmic element presence
                                    const noiseAmount = this.state[3] || 0;               // State[3] controls noise level
                                    const fmModIndex = (this.state[4] || 0) * 500;        // State[4] FM modulation index
                                    const fmModFreq = 20 + (this.state[5] || 0) * 100;    // State[5] FM modulation frequency


                                    for (let channel = 0; channel < output.length; ++channel) {
                                        const outputChannel = output[channel];
                                        for (let i = 0; i < bufferSize; ++i) {
                                            const currentPhase = (this.phase + i) / sampleRate; // Time in seconds for this sample

                                            // --- Synthesis Components ---
                                            // Basic FM Oscillator
                                            let fmModulator = Math.sin(2 * Math.PI * fmModFreq * currentPhase);
                                            let fmCarrier = Math.sin(2 * Math.PI * mainFreq * currentPhase + fmModulator * fmModIndex);

                                            // Simple Rhythmic Kick (more like a pulse)
                                            let timeInBeat = currentPhase % (60.0 / 120.0); // Assuming 120 BPM for simplicity
                                            let kickEnv = Math.exp(-timeInBeat * 25.0);
                                            let kickOsc = Math.sin(2 * Math.PI * 60 * timeInBeat); // Basic 60Hz sine kick pitch env not included
                                            let kick = kickOsc * kickEnv * rhythmIntensity; // Gate effect from envelope

                                            // Filtered Noise (very basic state-controlled filter placeholder)
                                            let noise = (Math.random() * 2 - 1) * noiseAmount;
                                            // Simplistic filter idea (replace with proper biquad in real use)
                                            // This won't sound like a resonant filter, just scaling noise freq roughly
                                            let filteredNoise = noise * (1.0 / (1.0 + filterQ * 0.1)); // Rough Q effect


                                            // Combine signals
                                            let sampleValue = (fmCarrier * 0.4 + kick * 0.5 + filteredNoise * 0.1);

                                            // Apply master level parameter
                                            outputChannel[i] = sampleValue * masterLevel;
                                        }
                                    }
                                    this.phase += bufferSize; // Advance overall phase for next block

                                    return true; // Keep processor alive
                                }
                            }
                            registerProcessor('generative-processor', GenerativeProcessor);
                        `;
                        const blob = new Blob([processorCode], { type: 'application/javascript' });
                        const workletURL = URL.createObjectURL(blob);

                         try {
                            await audioContext.audioWorklet.addModule(workletURL);
                            console.log("AudioWorklet Module Added.");
                            this.audioWorkletNode = new AudioWorkletNode(audioContext, 'generative-processor');

                            // Connect graph: Worklet -> Gain -> Destination
                            this.audioWorkletNode.connect(masterGain);

                            console.log("AudioContext Initialized and Worklet Node Created.");
                            this.isInitialized = true;
                            this.hideWarning(); // Hide warnings on successful init

                            // Process any pending state update immediately
                            if (this.pendingState) {
                                this.update(this.pendingState);
                                this.pendingState = null;
                            }
                            // Try setting up mic now that context exists
                            await this.setupMicrophone();

                         } catch(moduleError) {
                            console.error("!!! Failed to load AudioWorklet module from Blob URL:", moduleError);
                            console.error("!!! Ensure you are running this HTML from a local web server (http://localhost:...) NOT from file:///");
                            this.showWarning("Audio Worklet Failed! Run from local server.");
                            this.isInitialized = false; // Mark as not initialized
                            // Fallback? Could try ScriptProcessorNode but it's deprecated and less performant
                         } finally {
                             URL.revokeObjectURL(workletURL); // Clean up blob URL regardless of success/failure
                         }

                         // Resume context if it started suspended or was forced
                         if (resumeNeeded || force) {
                             if(audioContext.state === 'suspended') {
                                 console.log("Attempting to resume AudioContext...");
                                 await audioContext.resume().catch(e => {
                                     console.error("Failed to resume AudioContext after initialization:", e);
                                     this.showWarning("Audio Disabled: Failed to resume context. Click/Tap again?");
                                 });
                             }
                         }
                         // Final check
                         if (audioContext.state === 'running') {
                             console.log("AudioContext is running.");
                         } else {
                              console.warn(`AudioContext state is '${audioContext.state}' after initialization attempt.`);
                         }


                    } catch (e) {
                        console.error("Failed to initialize AudioContext:", e);
                         this.showWarning("Audio Failed: Cannot create AudioContext.");
                        this.isInitialized = false;
                    } finally {
                        this.isInitializing = false; // Mark initialization attempt as complete
                    }
                } else if (audioContext.state === 'suspended') {
                    console.log("AudioContext exists but is suspended, attempting resume...");
                    await audioContext.resume().catch(e => {
                        console.error("Failed to resume existing suspended AudioContext:", e);
                        this.showWarning("Audio Disabled: Failed resume. Click/Tap again?");
                    });
                } else {
                    // Context exists and is running or closed (if closed, logic above handles it)
                    console.log(`AudioContext state: ${audioContext.state}`);
                     this.isInitializing = false;
                }
            }

            update(stateVector) {
                if (!this.isInitialized || !this.audioWorkletNode) {
                    // console.warn("Audio not ready, queuing state update.");
                    this.pendingState = stateVector; // Store state to apply once initialized
                    // Don't try to init here, wait for user interaction or visibility change
                    return;
                }
                 // Send the entire state vector to the worklet
                 if (stateVector && stateVector.length === STATE_VECTOR_SIZE) {
                    this.audioWorkletNode.port.postMessage({ state: stateVector });
                 } else {
                    // console.warn("Invalid state vector length for audio update.");
                 }

                // Example using AudioParam (optional)
                 const levelParam = this.audioWorkletNode.parameters.get('masterLevel');
                 if (levelParam) {
                    // Map a state element to master level (e.g., state[7]) with smoothing
                    const targetLevel = Math.max(0, Math.min(1, stateVector[7] || 0.7)); // Use state[7] if available, default 0.7
                    levelParam.linearRampToValueAtTime(targetLevel, audioContext.currentTime + 0.05); // Smooth transition
                 }
            }

            getMicrophoneInput() {
                 // Ensure analyserNode exists and audio is running
                if (!this.isInitialized || !analyserNode || !currentInputState.mic.fft || audioContext?.state !== 'running') {
                     return { level: 0, fft: null };
                 }

                const bufferLength = analyserNode.frequencyBinCount;
                 // Ensure buffer is allocated correctly
                 if (!currentInputState.mic.fft || currentInputState.mic.fft.length !== bufferLength) {
                    currentInputState.mic.fft = new Float32Array(bufferLength);
                 }

                analyserNode.getFloatFrequencyData(currentInputState.mic.fft); // Get FFT data

                // Calculate average volume (RMS or peak) - handle -Infinity dB
                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    if (isFinite(currentInputState.mic.fft[i])) { // Check for -Infinity
                        sum += Math.pow(10, currentInputState.mic.fft[i] / 20); // Convert dB to amplitude
                    }
                }
                let rms = Math.sqrt(sum / bufferLength);
                // Adjust multiplier for normalization - needs tuning
                currentInputState.mic.level = Math.min(1, Math.max(0, rms * 10)); // Normalize/clamp level

                // Return normalized level and the FFT data array
                return { level: currentInputState.mic.level, fft: currentInputState.mic.fft };
            }

             async setupMicrophone() {
                  // Check if already set up or if audio context isn't ready/running
                 if (this.micStreamSource || !this.isInitialized || !audioContext || audioContext.state !== 'running') {
                     if (!this.isInitialized || audioContext?.state !== 'running') {
                         console.warn("AudioContext not ready/running for microphone setup.");
                     }
                     return;
                 }
                 try {
                     console.log("Requesting microphone access...");
                     const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false }, video: false });
                     this.micStreamSource = audioContext.createMediaStreamSource(stream);
                     this.micStreamSource.connect(analyserNode); // Connect mic to analyser
                     console.log("Microphone input connected.");
                     // Allocate FFT buffer based on analyser settings
                     currentInputState.mic.fft = new Float32Array(analyserNode.frequencyBinCount);
                 } catch (err) {
                     console.error("Microphone access denied or failed:", err);
                      this.showWarning("Microphone Disabled: Access denied.");
                     // Game can proceed without mic input, `getMicrophoneInput` will return zeros
                 }
             }
        }

        // --- Input Handling ---
        function setupInputListeners() {
            const canvas = document.getElementById('renderCanvas');

            // Touch / Mouse
            const handlePointerMove = (event) => {
                // Use clientX/Y for consistency across pointer types
                currentInputState.touch.x = event.clientX / window.innerWidth;
                currentInputState.touch.y = 1.0 - (event.clientY / window.innerHeight); // Invert Y
                // Normalize pressure if available, otherwise use active state
                currentInputState.touch.pressure = (event.pressure !== undefined && event.pressure !== null) ? event.pressure : (currentInputState.touch.active ? 1.0 : 0);
                if (currentInputState.touch.active) { // Only prevent default if actively interacting
                    event.preventDefault();
                }
            };
            const handlePointerDown = (event) => {
                currentInputState.touch.active = true;
                // Ensure canvas has focus for potential keyboard events if added later
                // canvas.focus();
                handlePointerMove(event); // Update position immediately
                 // Try initializing audio on first interaction (moved to AudioController constructor)
                 audioController?.tryInitializeAudio(); // Attempt resume/init on interaction
                event.preventDefault();
            };
            const handlePointerUp = (event) => {
                currentInputState.touch.active = false;
                currentInputState.touch.pressure = 0;
                 event.preventDefault();
            };

            canvas.addEventListener('pointerdown', handlePointerDown, { passive: false });
            canvas.addEventListener('pointerup', handlePointerUp, { passive: false });
            canvas.addEventListener('pointerleave', handlePointerUp, { passive: false }); // Treat leaving as up
            canvas.addEventListener('pointermove', handlePointerMove, { passive: false });


            // Device Motion / Orientation (Requires HTTPS typically)
            const requestMotionPermission = () => {
                if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
                    DeviceOrientationEvent.requestPermission()
                        .then(permissionState => {
                            if (permissionState === 'granted') {
                                window.addEventListener('deviceorientation', handleOrientation);
                            } else {
                                console.warn("Device Orientation permission denied.");
                                showWarning("Motion Sensor Disabled: Permission Denied.");
                            }
                        })
                        .catch(console.error);
                } else {
                     // Handle standard non-iOS 13+ devices or environments where permission isn't needed/available
                    window.addEventListener('deviceorientation', handleOrientation, true);
                }
            };

            const handleOrientation = (event) => {
                    currentInputState.motion.alpha = event.alpha || 0; // Z axis (compass) - range [0, 360)
                    currentInputState.motion.beta = event.beta || 0;   // X axis (front/back tilt) - range [-180, 180)
                    currentInputState.motion.gamma = event.gamma || 0; // Y axis (left/right tilt) - range [-90, 90)
            };

            // Add a button or interaction to request motion permission on iOS 13+
            // For simplicity here, let's try requesting on the first interaction too
             canvas.addEventListener('pointerdown', requestMotionPermission, { once: true });


             // Microphone setup is now triggered after audio context is initialized successfully
        }


         // --- Multiplayer (Local Network - WebRTC Placeholder) ---
         // Remains largely the same conceptual placeholder - requires external signaling
         class LocalPeerCommunicator {
              constructor() {
                 this.connections = {}; // { peerId: RTCPeerConnection }
                 this.dataChannels = {}; // { peerId: RTCDataChannel }
                 this.myPeerId = 'user_' + Math.random().toString(36).substring(2, 9);
                 this.signalingChannel = null; // NEEDS external setup
                 this.isConnecting = new Set(); // Track peers currently in connection setup

                 console.warn(`Multiplayer requires a LOCAL signaling mechanism (not implemented here) for peer discovery and connection setup. My ID: ${this.myPeerId}`);
                 // TODO: Implement actual signaling logic here
             }

             // --- WebRTC Methods (Simplified stubs - need real signaling) ---

             async createOfferAndSend(peerId) {
                 if (this.connections[peerId] || this.isConnecting.has(peerId)) return;
                 console.log(`Creating connection to ${peerId}`);
                 this.isConnecting.add(peerId);

                 try {
                     const pc = new RTCPeerConnection({ iceServers: [] }); // Use only local candidates if possible
                     this.connections[peerId] = pc;

                     pc.onicecandidate = event => {
                         if (event.candidate) {
                             this.sendSignalingMessage({ type: 'candidate', target: peerId, candidate: event.candidate });
                         }
                     };
                     pc.onconnectionstatechange = () => {
                        if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected' || pc.connectionState === 'closed') {
                            this.cleanupConnection(peerId);
                        } else if (pc.connectionState === 'connected') {
                             this.isConnecting.delete(peerId); // Successfully connected
                        }
                    };


                     const channel = pc.createDataChannel('resonant-weave', { negotiated: false, ordered: false, maxRetransmits: 0 });
                     this.setupDataChannel(peerId, channel);

                     const offer = await pc.createOffer();
                     await pc.setLocalDescription(offer);
                     this.sendSignalingMessage({ type: 'offer', target: peerId, offer: offer });

                 } catch (e) {
                     console.error(`Error creating offer for ${peerId}:`, e);
                     this.cleanupConnection(peerId); // Clean up on error
                 }
             }

             async handleOfferAndCreateAnswer(peerId, offer) {
                  if (this.connections[peerId] || this.isConnecting.has(peerId)) return;
                  console.log(`Handling offer from ${peerId}`);
                  this.isConnecting.add(peerId);

                 try {
                     const pc = new RTCPeerConnection({ iceServers: [] });
                     this.connections[peerId] = pc;

                     pc.onicecandidate = event => {
                         if (event.candidate) {
                              this.sendSignalingMessage({ type: 'candidate', target: peerId, candidate: event.candidate });
                         }
                     };
                     pc.onconnectionstatechange = () => {
                        if (pc.connectionState === 'failed' || pc.connectionState === 'disconnected' || pc.connectionState === 'closed') {
                            this.cleanupConnection(peerId);
                        } else if (pc.connectionState === 'connected') {
                             this.isConnecting.delete(peerId); // Successfully connected
                        }
                    };

                     pc.ondatachannel = event => {
                         this.setupDataChannel(peerId, event.channel);
                     };

                     await pc.setRemoteDescription(new RTCSessionDescription(offer));
                     const answer = await pc.createAnswer();
                     await pc.setLocalDescription(answer);
                     this.sendSignalingMessage({ type: 'answer', target: peerId, answer: answer });

                 } catch (e) {
                      console.error(`Error handling offer/creating answer for ${peerId}:`, e);
                      this.cleanupConnection(peerId); // Clean up on error
                 }
             }

             async handleAnswer(peerId, answer) {
                 const pc = this.connections[peerId];
                 if (pc && pc.signalingState === 'have-local-offer') {
                      try {
                         await pc.setRemoteDescription(new RTCSessionDescription(answer));
                         console.log(`Processed answer from ${peerId}`);
                      } catch (e) {
                          console.error(`Error setting remote description for answer from ${peerId}:`, e);
                      }
                 } else {
                     console.warn(`Received answer from ${peerId} but no connection or wrong state: ${pc?.signalingState}`);
                 }
             }

             async handleCandidate(peerId, candidate) {
                 const pc = this.connections[peerId];
                  try {
                    if (pc && pc.remoteDescription) { // Can only add candidates after remote description is set
                        await pc.addIceCandidate(new RTCIceCandidate(candidate));
                    } else if (pc) {
                        console.warn(`Queuing ICE candidate from ${peerId} until remote description is set.`);
                        // Simple queueing mechanism (could be more robust)
                        if (!pc.queuedCandidates) pc.queuedCandidates = [];
                        pc.queuedCandidates.push(candidate);
                    } else {
                         console.warn(`Received ICE candidate for unknown peer ${peerId}`);
                    }
                 } catch (e) {
                     console.error(`Error adding received ICE candidate for ${peerId}:`, e);
                 }
             }

              // Process queued candidates if any, after setting remote description
              async processQueuedCandidates(peerId) {
                 const pc = this.connections[peerId];
                 if (pc && pc.remoteDescription && pc.queuedCandidates) {
                     console.log(`Processing ${pc.queuedCandidates.length} queued candidates for ${peerId}`);
                     while(pc.queuedCandidates.length > 0) {
                         const candidate = pc.queuedCandidates.shift();
                         try {
                            await pc.addIceCandidate(new RTCIceCandidate(candidate));
                         } catch (e) {
                            console.error(`Error adding queued ICE candidate for ${peerId}:`, e);
                         }
                     }
                 }
              }


             setupDataChannel(peerId, channel) {
                 channel.onopen = () => {
                     console.log(`Data channel OPEN with ${peerId}`);
                     this.dataChannels[peerId] = channel;
                     this.isConnecting.delete(peerId); // Channel open means connection succeeded
                     // Maybe send an initial state or hello message
                 };
                 channel.onclose = () => {
                     console.log(`Data channel CLOSED with ${peerId}`);
                     this.cleanupConnection(peerId); // Ensure full cleanup
                 };
                 channel.onerror = (error) => {
                     console.error(`Data channel error with ${peerId}:`, error);
                      this.cleanupConnection(peerId); // Clean up on error too
                 };
                 channel.onmessage = (event) => {
                      this.handlePeerMessage(peerId, event.data);
                 };
                 // Store the channel reference immediately
                 this.dataChannels[peerId] = channel;
             }

            handlePeerMessage(peerId, data) {
                 try {
                     // Assume receiving intent vectors from peers as JSON arrays
                     const receivedIntentArray = JSON.parse(data);
                     if (receivedIntentArray && receivedIntentArray.length === INPUT_VECTOR_SIZE) {
                         // Dispose previous tensor for this peer if it exists
                         peers[peerId]?.intentVector?.dispose();

                         // Store the latest intent from this peer as a tensor
                         peers[peerId] = {
                             intentVector: tf.tensor1d(receivedIntentArray), // Store as tensor
                             timestamp: Date.now()
                         };
                     } else {
                        console.warn(`Received invalid data from ${peerId}`);
                     }
                 } catch (e) {
                     // console.warn(`Failed to parse message from ${peerId}:`, e, data);
                 }
            }

             broadcastIntent(intentVectorArray) {
                 if (!intentVectorArray) return;
                 // Avoid stringifying in the loop if possible
                 let message = null;
                 Object.values(this.dataChannels).forEach(channel => {
                     if (channel && channel.readyState === 'open') {
                          if (message === null) { // Stringify only once
                             message = JSON.stringify(intentVectorArray);
                         }
                         try {
                             // Check buffer amount before sending if needed (optional)
                             // if (channel.bufferedAmount < SOME_THRESHOLD)
                             channel.send(message);
                         } catch (e) {
                             console.error("Error sending message:", e)
                         }
                     }
                 });
             }

             cleanupConnection(peerId) {
                 console.log(`Cleaning up connection with ${peerId}`);
                 this.isConnecting.delete(peerId);
                 const pc = this.connections[peerId];
                 if (pc) {
                     pc.close();
                     delete this.connections[peerId];
                 }
                 delete this.dataChannels[peerId];
                 // Dispose tensor and remove from game state
                 peers[peerId]?.intentVector?.dispose();
                 delete peers[peerId];
             }

             // --- Signaling Simulation/Interface ---
             // This needs to be connected to your actual signaling implementation
              sendSignalingMessage(msg) {
                  console.log("SIGNALING (to implement):", JSON.stringify(msg));
                  // Example: If using a local WebSocket server:
                  // if (this.signalingChannel && this.signalingChannel.readyState === WebSocket.OPEN) {
                  //    this.signalingChannel.send(JSON.stringify(msg));
                  // }
              }

              // Call this when a message arrives from your signaling server/mechanism
              receiveSignalingMessage(msg) {
                  try {
                      const parsedMsg = JSON.parse(msg);
                      const peerId = parsedMsg.senderId; // Assuming senderId is part of message

                      if (!peerId || peerId === this.myPeerId) return; // Ignore own messages

                      switch(parsedMsg.type) {
                          case 'offer':
                              this.handleOfferAndCreateAnswer(peerId, parsedMsg.offer);
                              break;
                          case 'answer':
                              this.handleAnswer(peerId, parsedMsg.answer);
                              // Process queued candidates AFTER handling answer
                              this.processQueuedCandidates(peerId);
                              break;
                          case 'candidate':
                              this.handleCandidate(peerId, parsedMsg.candidate);
                              break;
                          // Add other signaling message types (discovery, leave, etc.)
                          default:
                              console.warn("Unknown signaling message type:", parsedMsg.type);
                      }
                  } catch (e) {
                      console.error("Error processing signaling message:", e, msg);
                  }
              }
         }


        // --- Game Loop ---
        let lastTimestamp = 0;
        async function gameLoop(timestamp) {
             // Ensure TF backend is ready (especially important for WebGPU)
            await tf.ready();

            const deltaTime = (timestamp - lastTimestamp) / 1000; // Delta time in seconds
            lastTimestamp = timestamp;

            // 1. Read Inputs & Update Microphone State
            const micData = audioController?.getMicrophoneInput() ?? { level: 0, fft: null };
            currentInputState.mic.level = micData.level;
            // Feed micData.fft into inputProcessorModel if designed for it

            // 2. Process Inputs -> Unified Intent Vector (NN)
            let inputTensor = null;
            try {
                inputTensor = await inputProcessorModel.process(currentInputState);
                // Don't await array() here, keep it as tensor for NN input
                // unifiedIntentVector = await inputTensor.array(); // Keep JS array for broadcast?
                 // Let's try broadcasting the tensor data directly if possible or stick to array
                unifiedIntentVector = await inputTensor.data(); // Get TypedArray data for broadcast
            } catch (e) {
                console.error("Error processing input:", e);
                // Use default vector if processing fails?
                inputTensor = tf.zeros([INPUT_VECTOR_SIZE]); // Fallback tensor
                unifiedIntentVector = new Array(INPUT_VECTOR_SIZE).fill(0);
            }


            // 3. Multiplayer: Broadcast local intent & Receive peer intents
            if (localPeerCommunicator) {
                 // Broadcast the JS array/TypedArray representation
                 // Convert TypedArray back to plain array if needed for JSON
                 localPeerCommunicator.broadcastIntent(Array.from(unifiedIntentVector));

                 // Cleanup old peer data
                 const now = Date.now();
                 Object.keys(peers).forEach(peerId => {
                     if (now - peers[peerId].timestamp > 10000) { // Increase timeout (10s)
                         console.log(`Timing out peer ${peerId}`);
                         localPeerCommunicator?.cleanupConnection(peerId); // Use cleanup method
                     }
                 });
            }

            // 4. Core Logic: Update Resonant State (NN)
            let nextStateTensor = null;
             let prevStateTensor = null;
            try {
                 prevStateTensor = tf.tensor1d(currentResonantState); // Create tensor from previous state

                 const peerIntentTensors = {}; // Collect valid peer tensors
                 Object.entries(peers).forEach(([id, data]) => {
                    if (data.intentVector && !data.intentVector.isDisposed) { // Check tensor validity
                         peerIntentTensors[id] = data.intentVector;
                     }
                 });

                 nextStateTensor = await coreLogicModel.predict(inputTensor, prevStateTensor, peerIntentTensors);
                 currentResonantState = await nextStateTensor.array(); // Update JS state array

            } catch(e) {
                console.error("Error during core logic prediction:", e);
                 // Keep previous state if prediction fails?
                 if (nextStateTensor) nextStateTensor.dispose(); // Ensure disposal if error occurred after creation
                 nextStateTensor = prevStateTensor ? prevStateTensor.clone() : tf.tensor1d(currentResonantState); // Fallback to previous state
                 currentResonantState = await nextStateTensor.array(); // Update JS state array with fallback
            } finally {
                // Cleanup tensors used in this step
                inputTensor?.dispose();
                prevStateTensor?.dispose(); // Dispose the tensor created from the previous JS state array
                nextStateTensor?.dispose();
                 // Peer tensors stored in `peers` object are disposed during cleanupConnection or when replaced
            }


            // 5. Update Audio & Visuals
            const time = timestamp / 1000; // Time in seconds for shaders/audio
            graphicsController?.update(currentResonantState, time);
            audioController?.update(currentResonantState);


            // 6. Debug Info (Optional)
            if (USE_DEBUG) {
                const debugDiv = document.getElementById('debugInfo');
                if (debugDiv) {
                     const { numBytes, numTensors } = tf.memory();
                    debugDiv.textContent = `Touch: ${currentInputState.touch.active ? 'ON' : 'OFF'} (${currentInputState.touch.x.toFixed(2)}, ${currentInputState.touch.y.toFixed(2)}) | Motion: ${currentInputState.motion.beta.toFixed(0)}, ${currentInputState.motion.gamma.toFixed(0)} | Mic Level: ${currentInputState.mic.level.toFixed(2)} | Peers: ${Object.keys(peers).length} | FPS: ${(1/deltaTime).toFixed(1)} | TF Mem: ${(numBytes / 1e6).toFixed(2)}MB / ${numTensors} Tensors | Audio: ${audioContext?.state ?? 'No Context'}`;
                }
            }

            // 7. Request Next Frame
            requestAnimationFrame(gameLoop);
        }

        // --- Initialization ---
        async function initialize() {
            console.log("Initializing Resonant Weaving Experience...");
             if (USE_DEBUG) {
                 document.getElementById('debugInfo').style.display = 'block';
             }

            graphicsController = new GraphicsController(document.getElementById('renderCanvas'));
            audioController = new AudioController(); // Initializes lazily on interaction

             // Crucial: Wait for TF backend to be ready before creating models or running operations
             await tf.ready();
             console.log(`TensorFlow.js backend: ${tf.getBackend()}`);


            // Initialize NN Models (Placeholders)
            inputProcessorModel = new PlaceholderInputProcessor();
            coreLogicModel = new PlaceholderCoreLogic();
            console.log("Neural Network placeholders ready.");

            // Setup input listeners
            setupInputListeners();

            // Initialize Multiplayer (requires local signaling setup)
            localPeerCommunicator = new LocalPeerCommunicator();
            // TODO: Add logic here to actually connect peers using the signaling mechanism
            // e.g., connect to a local WebSocket for signaling:
            // setupSignalingChannel(localPeerCommunicator); // Implement this function

            // Handle window resizing
            window.addEventListener('resize', () => {
                graphicsController?.resize();
            });

            // Check for WebGL support (basic)
            if (!renderer) {
                 showError("Error: WebGL is not supported or enabled. Cannot run experience.");
                return;
            }

            // Remove loading indicator (if any) and start the loop
            console.log("Initialization complete. Starting loop.");
            lastTimestamp = performance.now();
            requestAnimationFrame(gameLoop);
        }

        // --- Utility Functions ---
         function hasRTX3060LevelGPU() {
             try {
                 const canvas = document.createElement('canvas');
                 const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
                 if (gl) {
                     const debugInfo = gl.getExtension('WEBGL_debug_renderer_info');
                     if (debugInfo) {
                         const rendererName = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL);
                         console.log("GPU Renderer:", rendererName);
                         if (!rendererName) return false; // No renderer name found
                         // More robust checks: Look for higher-end keywords, avoid integrated keywords
                         const lowerRenderer = rendererName.toLowerCase();
                         if (lowerRenderer.includes('nvidia') && (lowerRenderer.includes('rtx') || lowerRenderer.includes('geforce gtx 16') || lowerRenderer.includes('geforce gtx 107') || lowerRenderer.includes('geforce gtx 108'))) return true;
                         if (lowerRenderer.includes('radeon') && (lowerRenderer.includes('rx 5') || lowerRenderer.includes('rx 6') || lowerRenderer.includes('rx 7') || lowerRenderer.includes('vega'))) return true; // Include 5000 series and up
                          if (lowerRenderer.includes('apple') && lowerRenderer.includes('m1') || lowerRenderer.includes('m2') || lowerRenderer.includes('m3')) return true; // Apple Silicon is quite capable
                          // Explicitly exclude common integrated graphics
                          if (lowerRenderer.includes('intel') && (lowerRenderer.includes('iris') || lowerRenderer.includes('uhd') || lowerRenderer.includes('hd graphics'))) return false;
                          if (lowerRenderer.includes('llvmpipe') || lowerRenderer.includes('swiftshader') || lowerRenderer.includes('software rasterizer')) return false; // Software rendering
                     }
                 }
             } catch (e) { console.error("Error detecting GPU:", e); }
             // Default to lower performance settings if unsure or on known integrated graphics
             return false;
         }

         function showError(message) {
             // Display error prominently to the user
             const errorDiv = document.createElement('div');
             errorDiv.style.position = 'absolute';
             errorDiv.style.top = '0';
             errorDiv.style.left = '0';
             errorDiv.style.width = '100%';
             errorDiv.style.padding = '20px';
             errorDiv.style.backgroundColor = 'rgba(255, 0, 0, 0.8)';
             errorDiv.style.color = 'white';
             errorDiv.style.textAlign = 'center';
             errorDiv.style.fontSize = '16px';
             errorDiv.style.fontFamily = 'sans-serif';
             errorDiv.style.zIndex = '1000';
             errorDiv.textContent = message;
             document.body.appendChild(errorDiv);
             // Optionally hide canvas or other elements
             document.getElementById('renderCanvas')?.remove();
         }

         function showWarning(message) {
            // Use the dedicated warning div
            const warningDiv = document.getElementById('warningInfo');
            if (warningDiv) {
                warningDiv.textContent = message;
                warningDiv.style.display = 'block';
                // Optionally hide after a few seconds
                 setTimeout(() => { warningDiv.style.display = 'none'; }, 7000);
            } else {
                console.warn(`Warning (UI element #warningInfo not found): ${message}`);
            }
        }


        // --- Start ---
        // IMPORTANT: Ensure TFJS is loaded before initializing
         if (typeof tf !== 'undefined') {
             initialize();
         } else {
            // Wait for TFJS to load if it hasn't already (less common with standard script tag)
             console.warn("TensorFlow.js not found immediately. Waiting for load event (if applicable).");
             window.addEventListener('load', () => {
                 if (typeof tf !== 'undefined') {
                    initialize();
                 } else {
                     showError("Fatal Error: TensorFlow.js failed to load.");
                 }
             });
         }


    </script>
</body>
</html>