# vibe coding prompt
```
in another unique universe, think of what life could be like and choose setting for new psytrance game. there should be nothing that is in any other games, that is, completely unique game. it should be playable on xiaomi 13 lite (or pc with rtx 3060), javascript, everything in one html block. make full implementation. for maximum immersion in user interface don't use concepts from our universe in that unique universe. don't use symbols in user interface as they can be barely visible. use transformers.js and tensorflow.js. for online only link existing public assets and libraries which in future could be downloaded and inlined in single html, to work offline without internet. no download of music, generate music internally. only neural networks modes can be external. use all game and user inputs in neural networks and between neural networks uniformly, so possible to connect any new inputs like eeg. make in multiplayer but without internet, only existing game sensors and actuators. make audio context auto recreatable when user changes browser screen. make massive multiplayer, on openair. make sound more generative.

add some psytrance feeling. and no peers seems connected, but think that sound from one somehow related to near others

and shows tf mem 0 MB / 0 tensors

add more full on, and make it more generative, as there neural networks

as there neural networks then no need any random, just use everywhere neural networks. so all must be determined by streams from user, game state and environment and multiplayer

here Infundibulum_Echoes_-_Deterministic_Resonance and other implementation where shown how insistence of bot captured by ai, one gpu accelerated and another more cortical inspired. idea to make openair jam where someone join another ones and them start jamming and when if they synchronized enough then created echoes of each other so then they end jamming these echoes continue accompaniment as artifacts, and these accompaniment synchronized only with owner. for llm idea that llm could rule creation and management of these artifacts but must be as small as possible and failproof as small may hallucinate.

maybe make artifacts collection as llm embeddings to be searchable by RAG to choose between suited artifacts for accompaniment, in visual, enabled artifacts may be feels just in visualization parts spatially, temporally, colors, structures, in sync with audio.

can hear even music on mobile, but very low sound volume even on maximum phone volume, same low volume on pc. music started when i played psytrance radio in speakers. without that external music was no sound on mobile, but no visual on mobile even with user input, and sound on mobile seems not react on user input. but on pc even when low volume, there visual depends on mouse position, and like they remembered that external music from radio and now pulsating in rhythm, and i can change these pulsations, and many artifacts appeared visually, with different color, overlapping each other and pulsating, nut seems one main pulsation but not sure is it complex pattern of pulsation or just binary. and not sure is pulsation tuned on new song on radio, maybe it stays on same pulsation from previous song, and i not touching mouse. so not sure how much complex pattern of vibration can be learned. or maybe needed different artifacts for each instrument. not sure, but when complex pattern on radio, it seems pulsates not correctly. at start without external music it has some internal rhythm but without pulsation, so pulsation maybe artifacts from interactions.
make real multiplayer to create artifacts on real synchrony by using real complex feeling intents ai. for better divide where is external player intents you can use visuals in sync with audio, but audio more stable as visual can be not present at any period of time or can be not from nearby player. so make maximum complex structure of ai, as seems processing power is enough, at least at rtx 3060, and maybe make dynamic complexity for how much fps stable, and when fps very high then possible to increase power of ai to use processing power on maximum but with realtime stability. not sure but maybe make also localstorage to make browser page reload not lost all patterns, but not sure how to make reset of data at least for debug purposes or when user just want to forget all past remembered intents or artifacts. on current low volume i mean it can be x100 increased and will be okay maybe it need to be silence around to hear it now at full volume on device.

maybe make not button but recognize verbal command to remove some artifact, so make verbal artifact management. as buttons can be accidentally pressed while jamming. and there possibility to translate this visualization on super big projector so better be no buttons, as it will ruin immersiveness. make what user playing more important than external patterns, so if player not playing in rhythm of external music then maybe he don't like to be involved. but if there like accelerometer or any other user related acting in tact with external music then maybe user seems involved so user interested to have this artifact and then make some help in synchronization, but little help, so may user synchronize mostly himself, or it will be not really his artifact. so not sure how to manage artifact and same time not ruin immersiveness, but should be something works automagically for user to feel in another unique universe, not searching buttons on screen, as he may disabled screen all time of jam. see that there gyroscope but not see is there accelerometer, and accelerometer will be good for rhythm sensing. gyroscope for more precise control, but gyroscope some time may be impossible to feel rhythm if phone orientation not changed. but all streams should act together not just one for one another for another. after saved artifacts maybe make possibility to exchange artifacts via maybe nfc not sure, but also automagically without loose of immersiveness, without symbols and buttons on screen, as remember it can be streamed on super large projector. maybe also artifact management via some verbal commands, but execute commands silently without ruin immersiveness as music also can be streamed on large acoustic systems. so system output is what represents another unique universe and it not speaks and not showing usual symbols. idea of artifact management is something what maybe need to be manageable more immersively, but without eeg will be hard to feel real user intents, with eeg there llm could manage artifacts as user feels. about ai models maybe choose between all known different architectures, which are fast at least on rtx 3060. maybe range artifacts for their complexity, it is when user sharing his artifacts another user may try to reduce their complexity for his hardware, but who has more powerful hardware will improve this artifact, also llm may improve artifacts.
```
