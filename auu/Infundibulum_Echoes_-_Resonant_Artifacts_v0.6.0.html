<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, viewport-fit=cover">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- <<< CHANGE: Version incremented -->
    <title>Infundibulum Echoes - Resonant Artifacts v0.6.0</title>
    <style>
        html, body { margin: 0; padding: 0; overflow: hidden; height: 100%; width: 100%; background-color: #000; color: #fff; font-family: sans-serif; cursor: none; -webkit-tap-highlight-color: transparent; touch-action: manipulation; /* Changed from none to allow taps but prevent zooming etc */ }
        canvas { display: block; width: 100%; height: 100%; }
        /* <<< CHANGE: Adjusted debug info style for portrait mobile visibility */
        #debugInfo { position: absolute; top: 5px; left: 5px; color: rgba(200,200,200,0.7); font-family: monospace; font-size: 7px; line-height: 1.2; background-color: rgba(0,0,0,0.5); padding: 3px 5px; border-radius: 3px; display: none; z-index: 10; pointer-events: none; max-width: calc(100% - 10px); white-space: normal; word-wrap: break-word; }
        #warningInfo { position: absolute; bottom: 10px; left: 10px; color: yellow; font-family: sans-serif; font-size: 12px; background-color: rgba(0,0,0,0.6); padding: 8px; border-radius: 5px; display: block; z-index: 10; pointer-events: none; }
        #loadingInfo { position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: #eee; font-size: 14px; background-color: rgba(0,0,0,0.7); padding: 15px; border-radius: 8px; display: none; z-index: 10; text-align: center; pointer-events: none; }
        #resetButton { position: absolute; bottom: -999px; left: -999px; width: 1px; height: 1px; opacity: 0; pointer-events: none; }
        #speechStatus { position: absolute; top: 10px; right: 10px; color: rgba(180, 180, 255, 0.6); font-family: monospace; font-size: 9px; background-color: rgba(0,0,50,0.4); padding: 2px 4px; border-radius: 3px; display: none; z-index: 10; pointer-events: none; }
    </style>
</head>
<body>
    <canvas id="renderCanvas"></canvas>
    <div id="debugInfo">Debug Info Placeholder</div>
    <div id="warningInfo">Interact to initialize. Long Press + Tap to Reset. Say "Reset Echoes" for voice command.</div>
    <div id="loadingInfo">Loading Assets...<br><span id="loadingProgress"></span></div>
    <div id="speechStatus">Speech: Idle</div>
    <button id="resetButton">Reset</button> <!-- Triggered programmatically -->

    <!-- External Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.17.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js" type="module"></script>
    <!-- Transformers.js imported dynamically -->

    <!-- Main Game Logic -->
    <script type="module">

        // Import Three.js
        import * as THREE from 'https://cdn.jsdelivr.net/npm/three@0.163.0/build/three.module.js';

        // --- Constants & Configuration ---
        // <<< CHANGE: Version incremented
        const VERSION = "0.6.0";
        const USE_DEBUG = true;
        // <<< CHANGE: Slightly lower target FPS to prioritize audio stability under load
        const TARGET_FPS = 55;
        const STATE_VECTOR_SIZE = 64;
        const INPUT_VECTOR_SIZE = 64;
        const EMBEDDING_DIM = 384; // From all-MiniLM-L6-v2
        const MAX_ARTIFACTS = 16;
        const MAX_ACTIVE_ARTIFACTS_LOGIC = 4; // Max artifacts influencing NN logic
        let MAX_ARTIFACTS_SHADER = 1; // Initial value, detected later dynamically
        const ARTIFACT_SIMILARITY_THRESHOLD = 0.46; // Slightly lower threshold
        const ARTIFACT_CREATION_INTERVAL_MS = 9000; // Slightly shorter interval
        // <<< CHANGE: Adjusted activity thresholds slightly
        const ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MIN = 0.25;
        const ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MAX = 0.80;
        const EMBEDDING_MODEL_NAME = 'Xenova/all-MiniLM-L6-v2';
        const MIC_FFT_SIZE = 256;
        const ACCEL_FFT_SIZE = 64;
        // <<< CHANGE: Versioned localStorage key
        const LOCAL_STORAGE_KEY = `infundibulumEchoesState_v${VERSION}`;
        const SPEECH_COMMANDS = { CREATE: ["create artifact", "make echo", "capture this", "remember this"], FORGET_OLDEST: ["forget oldest", "remove last echo", "clear history", "forget last"], RESET: ["reset echoes", "start over", "clear all", "forget everything"], };
        const SYNC_THRESHOLD = 0.3; // Threshold for "high sync" artifact creation message
        const SYNC_DECAY = 0.98; // How fast sync factor fades
        const ACCEL_ANALYSIS_INTERVAL_S = ACCEL_FFT_SIZE / (TARGET_FPS * 0.9); // Interval for accel rhythm analysis
        const LONG_PRESS_DURATION_MS = 2000;
        const RESET_SECOND_TAP_WINDOW_MS = 400;
        // <<< CHANGE: Added fullscreen request key
        const FULLSCREEN_REQUESTED_KEY = `infundibulumEchoesFullscreenReq_v${VERSION}`;

        // --- Global State ---
        let renderer, scene, camera, audioContext, masterGain, analyserNode;
        let inputProcessorModel, coreLogicModel;
        let graphicsController, audioController, artifactManager, embeddingProvider, featureExtractor, speechController;
        let interactionOccurred = false;
        let embeddingsReady = false;
        let lastArtifactCreationTime = 0;
        let complexityLevel = 0.5; // Start at medium complexity, adjusts dynamically
        let lastFpsTime = 0; let frameCount = 0; let currentFPS = TARGET_FPS;
        // <<< CHANGE: Added currentTime to input state for NN
        let currentInputState = {
            touch: { x: 0.5, y: 0.5, active: false, pressure: 0, dx: 0, dy: 0, lastX: 0.5, lastY: 0.5 },
            motion: { alpha: 0, beta: 0, gamma: 0, available: false },
            mic: { level: 0, fft: new Float32Array(MIC_FFT_SIZE / 2).fill(-140), available: false, rhythmPeak: 0, rhythmTempo: 0 },
            accelerometer: { x: 0, y: 0, z: 0, magnitude: 0, available: false, history: new Array(ACCEL_FFT_SIZE).fill(0), rhythmPeak: 0, rhythmTempo: 0 },
            syncFactor: 0.0,
            currentTime: 0.0 // <<< Added global time
        };
        let unifiedIntentVector = tf.keep(tf.zeros([1, INPUT_VECTOR_SIZE]));
        let currentResonantState = tf.keep(tf.fill([1, STATE_VECTOR_SIZE], 0.5));
        let activeArtifactInfo = { ids: [], stateArrays: [], similarities: [] };
        let resetGestureState = { pointerDownTime: 0, longPressDetected: false, longPressReleaseTime: 0, resetTimeout: null };
        let lastAccelTime = 0;
        let visualFeedback = { active: false, intensity: 0, startTime: 0, duration: 0.1 };
        let memoryInfo = { numBytes: 0, numTensors: 0 };
        // <<< CHANGE: Added state load tracking flags
        let stateLoadAttempted = false;
        let stateLoadSucceeded = false;

        // --- Rhythm Analysis Helper ---
        function analyzeRhythm(data, sampleRateOrFreq, dataSize) {
            let peakFreqBin = -1; let peakMag = -Infinity; let totalEnergy = 0;
            const freqResolution = (sampleRateOrFreq / 2) / (dataSize / 2);
            for (let i = 0; i < data.length; i++) {
                const magnitude = data[i];
                const linearMag = isFinite(magnitude) ? (magnitude > -100 ? Math.pow(10, magnitude / 20) : 0) : 0;
                totalEnergy += linearMag * linearMag;
                const freq = i * freqResolution;
                // Focus on typical rhythm frequencies (60-600 BPM -> 1-10 Hz)
                if (magnitude > peakMag && freq >= 1.0 && freq <= 10.0) { peakMag = magnitude; peakFreqBin = i; }
            }
            const peakFrequency = peakFreqBin * freqResolution;
            let normalizedPeak;
            // Normalize based on type (dB for Mic, raw magnitude for Accel)
            if (sampleRateOrFreq > 1000) { // Assume Mic (dB)
                normalizedPeak = clamp((peakMag + 80.0) / 80.0, 0, 1); // Normalize dB from -80 to 0 range
            } else { // Assume Accel (magnitude)
                const rmsEnergy = data.length > 0 ? Math.sqrt(totalEnergy / data.length) : 0;
                normalizedPeak = clamp(rmsEnergy / 5.0, 0, 1); // Normalize RMS magnitude (adjust divisor empirically)
            }
            // <<< CHANGE: Wider tempo range for darkpsy
            const estimatedTempo = clamp(peakFrequency * 60, 60, 240); // Estimate BPM (Wider range for darkpsy)
            if (peakFreqBin === -1) { // No clear peak found
                return { peak: normalizedPeak, tempo: 120 }; // Return normalized energy, default tempo
            }
            return { peak: normalizedPeak, tempo: estimatedTempo };
        }

        // --- Feature Extractor ---
         class FeatureExtractor {
             constructor(stateVectorSize) {
                 this.stateVectorSize = stateVectorSize;
                 // Indices mapping state vector positions to concepts (used for tag generation)
                 // Keep this relatively stable between versions if possible
                 this.indices = {
                     kick: 0, arpSpeed: 1, bassCut: 2, bright: 3, sat: 4, hue: 5, flow: 6, warp: 7, complexity: 8, tempo: 9,
                     reverb: 10, leadDecay: 11, noiseLevel: 12, noiseCut: 13, noiseRes: 14, masterVol: 15, leadPresence: 16,
                     leadOctave: 17, leadPitchMod: 18, leadPattern: 19, bassOctave: 20, leadLfoRate: 21, bassDecay: 22,
                     hat1Level: 23, noiseLfoSpeed: 24, hat2Level: 25, hatPattern: 26, snarePresence: 27, hatDecay1: 28,
                     hatDecay2: 29, hatHpCutoff: 30, delayTime: 31, delayFeedback: 32, delayMix: 33, combFeedback: 34,
                     combDamping: 35, fxLfoRate: 36, apFeedback: 37, snareTone: 38
                 };
             }
             _getCategory(v, thresholds, labels) { for (let i = 0; i < thresholds.length; i++) { if (v < thresholds[i]) return labels[i]; } return labels[labels.length - 1]; }
             extractTags(arr) {
                 if (!arr || arr.length !== this.stateVectorSize) return "";
                 const tags = new Set(); const i = this.indices;
                 const getVal = (idx, def) => (idx !== undefined && arr[idx] !== undefined) ? arr[idx] : def;

                 const tempo = getVal(i.tempo, 0.5); tags.add(this._getCategory(tempo, [0.25, 0.5, 0.75], ["slow", "mid", "fast", "very_fast"])); // Adjusted tempo categories
                 if (getVal(i.kick, 0.5) > 0.75) tags.add("drive");
                 const bc = getVal(i.bassCut, 0.5); const br = getVal(i.bright, 0.5);
                 if (bc < 0.3 && br > 0.6) tags.add("dark_bass"); else if (bc > 0.5 && br > 0.7) tags.add("acidic"); else if (bc > 0.4) tags.add("bright_bass");
                 if (getVal(i.sat, 0.5) > 0.65) tags.add("saturated");
                 if (getVal(i.leadPresence, 0.5) > 0.6) {
                     const leadDecay = getVal(i.leadDecay, 0.1); if (leadDecay < 0.15) tags.add("plucky_lead"); else tags.add("long_lead");
                 }
                 if (getVal(i.noiseLevel, 0.1) > 0.3) tags.add("noisy");
                 const comp = getVal(i.complexity, 0.5); tags.add(this._getCategory(comp, [0.4, 0.8], ["simple", "mid", "complex"]));
                 const hue = getVal(i.hue, 0.5); tags.add(this._getCategory(hue, [0.15, 0.3, 0.45, 0.6, 0.75, 0.9], ["red", "orange", "yellow", "green", "blue", "purple", "red"]));
                 if (getVal(i.flow, 0.0) > 0.7) tags.add("flow"); if (getVal(i.warp, 0.0) > 0.6) tags.add("warp"); if (getVal(i.reverb, 0.0) > 0.5) tags.add("reverb");
                 const vol = getVal(i.masterVol, 0.6); if (vol < 0.3) tags.add("quiet"); else if (vol > 0.8) tags.add("loud");
                 // <<< CHANGE: Check for snare presence using index 27
                 if (getVal(i.snarePresence, 0.5) > 0.6) tags.add("snare");

                 return Array.from(tags).join(' ');
             }
         }

        // --- Embedding Provider ---
        class EmbeddingProvider {
            constructor(modelName) { this.modelName = modelName; this.pipeline = null; this.isInitializing = false; this.loadingDiv = document.getElementById('loadingInfo'); this.loadingProgress = document.getElementById('loadingProgress'); }
            async init() { if (this.pipeline || this.isInitializing) return; this.isInitializing = true; showLoading(true, `Loading Embedding: ${this.modelName}...`); console.log(`Loading model: ${this.modelName}...`); try { const { pipeline, env } = await import('https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.1'); env.allowLocalModels = false; env.allowRemoteModels = true; env.backends.onnx.wasm.numThreads = 1; env.backends.onnx.wasm.simd = true; console.log("Transformer env configured."); this.pipeline = await pipeline('feature-extraction', this.modelName, { quantized: true, progress_callback: (p) => { if (this.loadingProgress) { let s=p.status; if(p.file) s+=`: ${p.file}`; if(p.loaded&&p.total){s+=` (${((p.loaded/p.total)*100).toFixed(1)}%)`;} this.loadingProgress.textContent=s; } } }); console.log("Embedding pipeline loaded."); embeddingsReady = true; showLoading(false); /* Check if state load already happened */ if (!stateLoadAttempted || stateLoadSucceeded) showWarning("System Ready. Interact or Speak. Say 'Reset Echoes' to reset.", 6000); } catch (e) { console.error("FATAL: Embedding model failed:", e); showError("Embedding Model Failed! Artifacts disabled."); embeddingsReady = false; showLoading(false); artifactManager = null; if (e.message && e.message.includes("CacheStorage")) { console.warn("Note: CacheStorage error detected during embedding model load. This is often a browser issue and may resolve itself. Functionality might be okay unless model loading fully failed."); showWarning("Warning: Browser cache issue detected (Embeddings).", 7000); } } finally { this.isInitializing = false; } }
            async embed(text) { if (!this.pipeline || !embeddingsReady) return null; if (!text || typeof text !== 'string' || text.trim().length === 0) { console.warn("Embed skip: Invalid text."); return null; } try { const r = await this.pipeline(text, { pooling: 'mean', normalize: true }); if (r && r.data && r.data.length === EMBEDDING_DIM) return r.data; else { console.warn("Embed failed/bad shape:", text); return null; } } catch (e) { console.error("Embedding error:", e); return null; } }
         }

        // --- Artifact Manager ---
        class ArtifactManager {
             constructor(maxArtifacts, stateVectorSize, embeddingDim, featureExtractor, embeddingProvider) { this.maxArtifacts=maxArtifacts; this.stateVectorSize=stateVectorSize; this.embeddingDim=embeddingDim; this.featureExtractor=featureExtractor; this.embeddingProvider=embeddingProvider; this.artifacts=[]; this.nextId=0; }
             async createArtifact(stateVectorTensor) { if (!embeddingsReady || !this.featureExtractor || !this.embeddingProvider) return false; if (!stateVectorTensor || stateVectorTensor.isDisposed) { console.warn("Artifact create skip: Invalid tensor."); return false; } const stateArr = await stateVectorTensor.data(); const tags = this.featureExtractor.extractTags(stateArr); if (!tags) return false; const emb = await this.embeddingProvider.embed(tags); if (!emb || emb.length !== this.embeddingDim) return false; const newArt = { id: this.nextId++, stateVector: Array.from(stateArr), featureTags: tags, embedding: emb, timestamp: Date.now() }; this.artifacts.push(newArt); console.log(`Artifact ${newArt.id} created: "${tags.substring(0,50)}..."`); triggerVisualFeedback(0.6); if (this.artifacts.length > this.maxArtifacts) { this.artifacts.sort((a,b) => a.timestamp - b.timestamp); const removed = this.artifacts.shift(); console.log(`Pruned oldest artifact ${removed.id}. Count: ${this.artifacts.length}`); } saveStateToLocalStorage(); return true; }
             _cosineSimilarity(a,b) { if (!a||!b||a.length !== b.length||a.length===0) return 0; let dot=0, nA=0, nB=0; for (let i=0; i<a.length; i++) { dot+=a[i]*b[i]; nA+=a[i]*a[i]; nB+=b[i]*b[i]; } if (nA===0||nB===0) return 0; return dot / ((Math.sqrt(nA)*Math.sqrt(nB))+1e-9); }
             async findRelevantArtifacts(stateTensor, thresh, maxCnt) {
                 const result = { ids:[], stateArrays:[], similarities:[] };
                 if (!embeddingsReady || this.artifacts.length===0 || !stateTensor || stateTensor.isDisposed) return result;
                 const stateArr = await stateTensor.data();
                 const tags = this.featureExtractor.extractTags(stateArr); if (!tags) return result;
                 const queryEmb = await this.embeddingProvider.embed(tags); if (!queryEmb) return result;
                 const candidates = this.artifacts.map(art => ({ art: art, similarity: this._cosineSimilarity(queryEmb, art.embedding) }));
                 const relevant = candidates.filter(c => c.similarity >= thresh).sort((a,b) => b.similarity - a.similarity);
                 const selected = relevant.slice(0, maxCnt);
                 selected.forEach(item => { result.ids.push(item.art.id); result.stateArrays.push(item.art.stateVector); result.similarities.push(item.similarity); });
                 return result;
             }
             getArtifactCount() { return this.artifacts.length; }
             // <<< CHANGE: Improved validation during artifact setting
             setArtifacts(loaded) {
                 if (!Array.isArray(loaded)) {
                     console.error("Cannot set artifacts: input is not an array.", loaded);
                     this.artifacts = []; this.nextId = 0;
                     return;
                 }
                 this.artifacts = loaded
                     .filter(a => a && typeof a === 'object') // Filter out non-objects first
                     .map(a => {
                         // Validate each field with defaults
                         const id = typeof a.id === 'number' ? a.id : 0;
                         const stateVector = Array.isArray(a.stateVector) && a.stateVector.length === this.stateVectorSize
                             ? a.stateVector.map(v => typeof v === 'number' && isFinite(v) ? clamp(v, 0, 1) : 0.5) // Ensure numeric & clamp
                             : new Array(this.stateVectorSize).fill(0.5);
                         const featureTags = typeof a.featureTags === 'string' ? a.featureTags : '';
                         const embedding = Array.isArray(a.embedding) && a.embedding.length === this.embeddingDim
                             ? a.embedding.map(v => typeof v === 'number' && isFinite(v) ? v : 0) // Ensure numeric
                             : new Array(this.embeddingDim).fill(0);
                         const timestamp = typeof a.timestamp === 'number' ? a.timestamp : Date.now();

                         // If size didn't match originally, log a warning
                         if (!Array.isArray(a.stateVector) || a.stateVector.length !== this.stateVectorSize) {
                             console.warn(`Loaded artifact ${id} had invalid stateVector (size ${a.stateVector?.length}), using default.`);
                         }
                         if (!Array.isArray(a.embedding) || a.embedding.length !== this.embeddingDim) {
                             console.warn(`Loaded artifact ${id} had invalid embedding (size ${a.embedding?.length}), using default.`);
                         }

                         return { id, stateVector, featureTags, embedding, timestamp };
                     })
                     .filter(a => a !== null); // Remove any that completely failed validation

                 this.nextId = this.artifacts.reduce((maxId, art) => Math.max(maxId, art.id), -1) + 1;
                 console.log(`Loaded ${this.artifacts.length} validated artifacts. Next ID: ${this.nextId}`);
             }
             forgetOldestArtifact() { if (this.artifacts.length > 0) { this.artifacts.sort((a, b) => a.timestamp - b.timestamp); const removed = this.artifacts.shift(); console.log(`Forgot oldest artifact ${removed.id} via command.`); triggerVisualFeedback(0.3, 0.2); saveStateToLocalStorage(); return true; } console.log("No artifacts to forget."); return false; }
         }

        // --- Speech Recognition Controller ---
        // <<< CHANGE: Added Speech Recognition Controller >>>
        class SpeechRecognitionController {
            constructor() { this.recognition = null; this.isSupported = ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window); this.isListening = false; this.isActive = false; this.isStarting = false; this.isStopping = false; this.permissionGranted = false; this.statusDiv = document.getElementById('speechStatus'); this.commandCallback = null; this.consecutiveErrorCount = 0; this.MAX_CONSECUTIVE_ERRORS = 8; this.restartTimeoutId = null; if (!this.isSupported) { console.warn("Speech Recognition API not supported."); this.updateStatus("Unsupported"); } }
            updateStatus(status) { if (this.statusDiv) { this.statusDiv.textContent = `Speech: ${status}`; this.statusDiv.style.display = 'block'; } return status; } // Return status for debug log
            async requestPermissionAndInit() { if (!this.isSupported || this.recognition) return; try {  await navigator.mediaDevices.getUserMedia({ audio: true }); this.permissionGranted = true; console.log("Audio permission likely granted for Speech Rec."); this.initializeRecognition(); } catch (err) { console.error("Mic permission denied or failed for Speech Rec:", err.name, err.message); this.updateStatus("Perm Denied"); if (interactionOccurred) showWarning("Voice commands disabled: Mic permission needed.", 5000); this.permissionGranted = false; } }
            initializeRecognition() { if (!this.isSupported || !this.permissionGranted || this.recognition) return; const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition; this.recognition = new SpeechRecognition(); this.recognition.continuous = true; this.recognition.interimResults = false; this.recognition.lang = 'en-US'; this.recognition.maxAlternatives = 1; this.recognition.onstart = () => { console.log("Speech recognition actually started."); this.isActive = true; this.isStarting = false; this.updateStatus("Listening"); }; this.recognition.onend = () => { console.log("Speech recognition ended."); this.isActive = false; this.isStarting = false; this.isStopping = false; if (this.isListening) { console.log("...restarting listening after short delay."); this.scheduleRestart(150 + Math.random() * 100); } else { this.updateStatus("Idle"); } }; this.recognition.onresult = (event) => { this.consecutiveErrorCount = 0; const last = event.results.length - 1; const transcript = event.results[last][0].transcript.trim().toLowerCase(); console.log(`Speech Result: "${transcript}"`); this.handleCommand(transcript); }; this.recognition.onerror = (event) => { console.error('Speech Recognition Error:', event.error); const error = event.error; this.isActive = false; this.isStarting = false; this.isStopping = false; let autoRestart = true; if (error === 'no-speech') { this.updateStatus("Quiet"); this.consecutiveErrorCount++; } else if (error === 'audio-capture') { this.updateStatus("Mic Problem"); this.consecutiveErrorCount++; } else if (error === 'network') { this.updateStatus("Network Issue"); this.consecutiveErrorCount++; } else if (error === 'not-allowed' || error === 'service-not-allowed') { this.updateStatus("Blocked/Disabled"); this.permissionGranted = false; this.isListening = false; autoRestart = false; showError(`Voice commands ${error}! Check permissions.`); } else { this.updateStatus(`Error (${error})`); this.consecutiveErrorCount++; } if (this.consecutiveErrorCount > this.MAX_CONSECUTIVE_ERRORS) { console.error("Too many consecutive speech errors, stopping attempts."); this.updateStatus("Stopped (Errors)"); this.isListening = false; autoRestart = false; showWarning("Speech recognition stopped due to repeated errors.", 6000); } if (this.isListening && autoRestart) { this.scheduleRestart(750 + Math.random() * 500); } }; console.log("Speech Recognition initialized."); this.updateStatus("Initialized"); }
            setCommandCallback(callback) { this.commandCallback = callback; }
            scheduleRestart(delay) { if (this.restartTimeoutId) clearTimeout(this.restartTimeoutId); this.restartTimeoutId = setTimeout(() => { this.restartTimeoutId = null; console.log("Executing scheduled speech restart."); this.startListening(); }, delay); }
            startListening() { if (!this.isSupported || this.isActive || this.isStarting || this.isStopping) return; if (!this.permissionGranted) { console.log("Speech start deferred: Requesting permission first."); this.requestPermissionAndInit().then(() => { if (this.permissionGranted) { this.startListening(); } }); return; } if (!this.recognition) { console.warn("Speech start failed: Recognition not initialized."); return; } console.log("Attempting to start speech recognition..."); this.isListening = true; this.isStarting = true; this.updateStatus("Starting..."); try { if (this.restartTimeoutId) { clearTimeout(this.restartTimeoutId); this.restartTimeoutId = null; } this.recognition.start(); } catch (e) { console.error("Error executing recognition.start():", e); this.isListening = false; this.isStarting = false; this.isActive = false; this.updateStatus("Start Failed"); if (e.name === 'InvalidStateError') { console.warn("InvalidStateError caught, likely rapid start/stop. Waiting for onend."); } else { this.scheduleRestart(1000 + Math.random() * 500); } } }
            stopListening() { console.log("Attempting to stop speech recognition..."); this.isListening = false; if (this.restartTimeoutId) { clearTimeout(this.restartTimeoutId); this.restartTimeoutId = null; } if (!this.recognition || (!this.isActive && !this.isStarting) || this.isStopping) { if (!this.isActive && !this.isStarting) this.updateStatus("Idle"); return; } this.isStopping = true; this.updateStatus("Stopping"); try { this.recognition.stop(); } catch(e) { console.error("Error executing recognition.stop():", e); this.isStopping = false; this.isActive = false; this.updateStatus("Stop Failed"); } }
            handleCommand(transcript) { let matchedCommand = null; const cleanTranscript = transcript.trim(); if (cleanTranscript.length === 0) return; for (const commandType in SPEECH_COMMANDS) { if (SPEECH_COMMANDS[commandType].some(phrase => cleanTranscript.includes(phrase))) { matchedCommand = commandType; console.log(`Matched Command: ${matchedCommand} from "${cleanTranscript}"`); break; } } if (matchedCommand && this.commandCallback) { this.commandCallback(matchedCommand); triggerVisualFeedback(0.2, 0.1); } else { console.log(`Unrecognized command: "${cleanTranscript}"`); } }
        }

        // --- NN Model Placeholders ---
        class PlaceholderInputProcessor {
             constructor(inputDim, outputDim) { this.inputDim = inputDim; this.outputDim = outputDim; }
             // <<< CHANGE: Added currentTime parameter
             process(inputData, currentTime) {
                 // Creates a [1, INPUT_VECTOR_SIZE] tensor based on current sensor inputs
                 return tf.tidy(() => {
                     const vec = new Array(INPUT_VECTOR_SIZE).fill(0);
                     const touchFactor = inputData.touch.active ? (inputData.touch.pressure || 1.0) : 0;
                     const micLevel = inputData.mic.available ? inputData.mic.level : 0;
                     const motionAvailable = inputData.motion.available;
                     const alphaNorm = motionAvailable ? (inputData.motion.alpha / 360.0) % 1.0 : 0.5;
                     const betaNorm = motionAvailable ? clamp((inputData.motion.beta + 180.0) / 360.0, 0, 1) : 0.5;
                     const gammaNorm = motionAvailable ? clamp((inputData.motion.gamma + 90.0) / 180.0, 0, 1) : 0.5;
                     const touchVelMag = clamp(Math.sqrt(inputData.touch.dx**2 + inputData.touch.dy**2) * 25, 0, 1);
                     const accelAvailable = inputData.accelerometer.available;
                     const accelNormX = accelAvailable ? clamp((inputData.accelerometer.x + 20) / 40, 0, 1) : 0.5;
                     const accelNormY = accelAvailable ? clamp((inputData.accelerometer.y + 20) / 40, 0, 1) : 0.5;
                     const accelNormZ = accelAvailable ? clamp((inputData.accelerometer.z + 20) / 40, 0, 1) : 0.5;
                     const accelMagNorm = accelAvailable ? clamp(inputData.accelerometer.magnitude / 25, 0, 1) : 0;
                     const micRhythmPeak = inputData.mic.available ? inputData.mic.rhythmPeak : 0;
                     const micRhythmTempoNorm = inputData.mic.available ? clamp((inputData.mic.rhythmTempo - 60) / (240 - 60), 0, 1) : 0.5; // Use wider tempo range
                     const accelRhythmPeak = accelAvailable ? inputData.accelerometer.rhythmPeak : 0;
                     const accelRhythmTempoNorm = accelAvailable ? clamp((inputData.accelerometer.rhythmTempo - 60) / (240 - 60), 0, 1) : 0.5; // Use wider tempo range

                     // Map sensor data into the vector using modulo and specific assignments
                     for (let i = 0; i < INPUT_VECTOR_SIZE; i++) {
                          switch (i % 16) { // Distribute core inputs cyclically
                             case 0: vec[i] = inputData.touch.x * (1.0 + touchFactor * 0.1); break; // Slightly boost active touch X
                             case 1: vec[i] = inputData.touch.y * (1.0 + touchFactor * 0.1); break; // Slightly boost active touch Y
                             case 2: vec[i] = touchFactor; break;
                             case 3: vec[i] = alphaNorm; break;
                             case 4: vec[i] = betaNorm; break;
                             case 5: vec[i] = gammaNorm; break;
                             case 6: vec[i] = micLevel; break;
                             case 7: vec[i] = touchVelMag; break;
                             case 8: vec[i] = accelNormX; break;
                             case 9: vec[i] = accelNormY; break;
                             case 10: vec[i] = accelNormZ; break;
                             case 11: vec[i] = accelMagNorm; break;
                             case 12: vec[i] = micRhythmPeak; break; // Inject rhythm features
                             case 13: vec[i] = micRhythmTempoNorm; break;
                             case 14: vec[i] = accelRhythmPeak; break;
                             case 15: vec[i] = accelRhythmTempoNorm; break;
                          }
                          // Simple non-linear mixing (example, can be replaced by a learned layer)
                          const prevVal = vec[(i + INPUT_VECTOR_SIZE - 1) % INPUT_VECTOR_SIZE] !== undefined ? vec[(i + INPUT_VECTOR_SIZE - 1) % INPUT_VECTOR_SIZE] : 0;
                          const otherVal = vec[(i + 5) % INPUT_VECTOR_SIZE] !== undefined ? vec[(i + 5) % INPUT_VECTOR_SIZE] : 0;
                          // <<< CHANGE: Use passed currentTime for time influence instead of undefined 'timestamp'
                          vec[i] = fract(vec[i]*1.1 + prevVal*0.3 + Math.sin(otherVal * 5.1 + i*0.1 + currentTime * 0.1) * 0.1); // Add time influence
                          vec[i] = 1.0 / (1.0 + Math.exp(-(vec[i] * 2.0 - 1.0) * 1.8)); // Sigmoid activation
                          vec[i] = clamp(vec[i] || 0, 0, 1); // Ensure valid range
                     }

                     // Inject Mic FFT peaks into the vector (last few elements)
                     if (inputData.mic.available && inputData.mic.fft) {
                         const fftData = inputData.mic.fft; const fftLen = fftData.length; const segments = 8;
                         const binsPerSegment = Math.max(1, Math.floor(fftLen / segments));
                         for(let seg = 0; seg < segments; seg++) {
                             let peakDb = -140; const start = seg * binsPerSegment; const end = Math.min(start + binsPerSegment, fftLen);
                             for(let k = start; k < end; k++) { if(isFinite(fftData[k])) peakDb = Math.max(peakDb, fftData[k]); }
                             const normPeak = clamp((peakDb + 100) / 100, 0, 1); // Normalize -100dB to 0dB range
                             const tIdxStart = INPUT_VECTOR_SIZE - 1 - seg * 2; // Map to end of vector
                             if (tIdxStart >= 0) vec[tIdxStart] = (vec[tIdxStart] * 0.5 + normPeak * 0.5); // Mix with existing value
                             if (tIdxStart - 1 >= 0) vec[tIdxStart - 1] = (vec[tIdxStart - 1] * 0.3 + normPeak * 0.7); // Mix with neighbour
                         }
                     }
                     return tf.tensor1d(vec).expandDims(0); // Shape: [1, INPUT_VECTOR_SIZE]
                 });
             }
         }

        class PlaceholderCoreLogic {
            constructor(stateSize) { this.stateSize = stateSize; }
            predict(intentVectorTensor, currentStateTensor, activeArtifactStateArrays, activeArtifactSimilarities) {
                // Predicts next [1, STATE_VECTOR_SIZE] tensor based on intent, current state, and active artifacts
                return tf.tidy(() => {
                    // <<< CHANGE: Slightly weaker decay, allowing state to persist longer
                    const decayFactor = 0.994;
                    const intentInfluence = 0.24;
                    const artifactInfluenceFactor = 0.16;

                    // --- Process Intent ---
                    let processedIntent = intentVectorTensor;
                    // Ensure intent vector matches state size (pad or truncate if needed)
                    if (intentVectorTensor.shape[1] !== this.stateSize) {
                       console.warn(`Intent size ${intentVectorTensor.shape[1]} mismatch with state size ${this.stateSize}. Adapting.`);
                       processedIntent = tf.tidy(() => {
                           const currentIntent = intentVectorTensor.squeeze([0]); // Remove batch dim
                           if (intentVectorTensor.shape[1] > this.stateSize) {
                               return currentIntent.slice([0], [this.stateSize]).expandDims(0); // Truncate
                           } else {
                               const padding = this.stateSize - intentVectorTensor.shape[1];
                               return currentIntent.pad([[0, padding]], 0.5).expandDims(0); // Pad with neutral
                           }
                       });
                    }
                    // Calculate how the intent pushes the state away from neutral (0.5)
                    const intentInfluenceVec = processedIntent.sub(tf.scalar(0.5)).mul(tf.scalar(intentInfluence));

                    // --- Process Artifacts ---
                    let combinedArtifactInfluence = tf.zerosLike(currentStateTensor);
                    if (activeArtifactStateArrays.length > 0) {
                        // Blend influence from active artifacts based on similarity
                        for(let i = 0; i < activeArtifactStateArrays.length; i++) {
                            const stateArr = activeArtifactStateArrays[i]; const sim = activeArtifactSimilarities[i];
                            if (!stateArr || stateArr.length !== this.stateSize) { console.warn(`Artifact ${i} has wrong size: ${stateArr?.length}, expected ${this.stateSize}`); continue; }
                            const artVecTensor = tf.tensor1d(stateArr).expandDims(0);
                            // Calculate how the artifact state pushes away from neutral, scaled by similarity
                            const influence = artVecTensor.sub(tf.scalar(0.5)).mul(tf.scalar(sim * artifactInfluenceFactor));
                            combinedArtifactInfluence = combinedArtifactInfluence.add(influence);
                        }
                        // Slightly normalize total artifact influence if many are active
                        combinedArtifactInfluence = combinedArtifactInfluence.mul(tf.scalar(1.0 / (1.0 + activeArtifactStateArrays.length * 0.1)));
                    }

                    // --- Internal Dynamics (Simplified Recurrence) ---
                    // Simple spatial mixing/blurring within the state vector, influenced by complexity
                    const shiftedRight = currentStateTensor.slice([0, 1], [1, this.stateSize - 1]).pad([[0, 0], [1, 0]], 0.5);
                    const shiftedLeft = currentStateTensor.slice([0, 0], [1, this.stateSize - 1]).pad([[0, 0], [0, 1]], 0.5);
                    // <<< CHANGE: Cross-talk influenced by complexity level
                    const crossTalk = shiftedLeft.sub(shiftedRight).mul(tf.scalar(0.035 + complexityLevel * 0.03));

                    // --- Combine Influences ---
                    let nextState = currentStateTensor
                        .sub(tf.scalar(0.5)).mul(tf.scalar(decayFactor)).add(tf.scalar(0.5)) // Apply decay towards neutral
                        .add(intentInfluenceVec) // Add user intent influence
                        .add(combinedArtifactInfluence) // Add artifact influence
                        .add(crossTalk); // Add internal dynamics

                    // Ensure state stays within valid bounds [0.01, 0.99]
                    nextState = nextState.clipByValue(0.01, 0.99);
                    return nextState;
                });
            }
        }

        // --- Graphics Controller ---
        class GraphicsController {
            constructor(canvas) {
                this.canvas = canvas;
                scene = new THREE.Scene();
                camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, 0.1, 100);
                camera.position.z = 4;
                try {
                    // <<< CHANGE: Removed antialias dependence on HIGH_PERFORMANCE_MODE
                    renderer = new THREE.WebGLRenderer({ canvas: this.canvas, antialias: true, powerPreference: "high-performance" });
                }
                catch (e) { console.error("!!! Failed to create WebGLRenderer:", e); showError("WebGL Renderer Failed!"); throw e; }
                renderer.setSize(window.innerWidth, window.innerHeight);
                // <<< CHANGE: Removed pixel ratio dependence on HIGH_PERFORMANCE_MODE, use a moderate cap
                renderer.setPixelRatio(Math.min(window.devicePixelRatio, 1.6));

                MAX_ARTIFACTS_SHADER = this.detectMaxUniformCapacity(); // Detect *after* renderer exists
                console.log(`Graphics: Detected max shader artifacts = ${MAX_ARTIFACTS_SHADER}`);

                const geometry = new THREE.PlaneGeometry(2, 2);
                this.material = new THREE.ShaderMaterial({
                    defines: { MAX_ARTIFACTS_SHADER: MAX_ARTIFACTS_SHADER, STATE_VEC_SIZE: STATE_VECTOR_SIZE },
                    uniforms: {
                        time: { value: 0.0 }, resolution: { value: new THREE.Vector2(window.innerWidth * renderer.getPixelRatio(), window.innerHeight * renderer.getPixelRatio()) },
                        mainState: { value: new Float32Array(STATE_VECTOR_SIZE).fill(0.5) }, numActiveArtifacts: { value: 0 },
                        artifactStates: { value: this._createArtifactUniformArray() }, artifactSimilarities: { value: new Float32Array(MAX_ARTIFACTS_SHADER).fill(0.0) },
                        complexity: { value: complexityLevel }, syncFactor: { value: 0.0 }, feedbackIntensity: { value: 0.0 }
                    },
                    vertexShader: `varying vec2 vUv; void main() { vUv = uv; gl_Position = vec4(position.xy, 0.0, 1.0); }`,
                    // <<< CHANGE: Shader code v0.6.0 - Increased dynamics, complexity influence, artifact impact >>>
                    fragmentShader: `
                        precision highp float;
                        uniform float time; uniform vec2 resolution; uniform float mainState[STATE_VEC_SIZE];
                        uniform int numActiveArtifacts;
                        uniform float artifactStates[MAX_ARTIFACTS_SHADER * STATE_VEC_SIZE];
                        uniform float artifactSimilarities[MAX_ARTIFACTS_SHADER];
                        uniform float complexity; // Now dynamically controlled (0.05 to 1.0)
                        uniform float syncFactor; uniform float feedbackIntensity;
                        varying vec2 vUv;

                        #define PI 3.14159265359
                        #define STATE_VEC_SIZE_FLOAT float(STATE_VEC_SIZE)

                        // Utility functions
                        float hash1(float n){ return fract(sin(n)*43758.5453); }
                        vec2 hash2(vec2 p){ p=vec2(dot(p,vec2(127.1,311.7)),dot(p,vec2(269.5,183.3))); return -1.+2.*fract(sin(p)*43758.5453); }
                        float noise(vec2 x){ vec2 p=floor(x); vec2 f=fract(x); f=f*f*(3.-2.*f); float n=p.x+p.y*57.; return mix(mix(hash1(n),hash1(n+1.),f.x),mix(hash1(n+57.),hash1(n+58.),f.x),f.y); }
                        float fbm(vec2 p, float H, int octaves){ float G=exp2(-H); float f=1.; float a=1.; float t=0.; for(int i=0; i<10; i++){ if(i>=octaves) break; t+=a*noise(f*p); f*=2.; a*=G; } return t; }
                        vec3 hsv2rgb(vec3 c){ vec4 K=vec4(1.,2./3.,1./3.,3.); vec3 p=abs(fract(c.xxx+K.xyz)*6.-K.www); return c.z*mix(K.xxx,clamp(p-K.xxx,0.,1.),c.y); }
                        float pulse(float t, float freq){ return 0.5+0.5*cos(t*freq*2.*PI); }
                        mat2 rotate2d(float a){ float s=sin(a); float c=cos(a); return mat2(c,-s,s,c); }

                        // Getters for state arrays
                        float getArtifactState(int artIdx, int stateIdx){
                            if(artIdx < 0 || artIdx >= MAX_ARTIFACTS_SHADER || stateIdx < 0 || stateIdx >= STATE_VEC_SIZE) return 0.5;
                            int flatIdx = artIdx * STATE_VEC_SIZE + stateIdx;
                            return artifactStates[flatIdx];
                        }
                        float getMainStateSafe(int idx, float defaultVal) {
                            if (idx >= 0 && idx < STATE_VEC_SIZE) { return mainState[idx]; }
                            return defaultVal;
                        }

                        void main() {
                            vec2 uv = vUv;
                            vec2 centerUv = uv - 0.5;
                            float distCenter = length(centerUv);

                            // --- Extract state values safely ---
                            // Use featureExtractor indices for clarity if possible (requires manual mapping here)
                            float kick = getMainStateSafe(0, 0.5);
                            float arpSpeed = 0.1 + getMainStateSafe(1, 0.5) * 2.5; // Faster potential arp
                            float bassCut = getMainStateSafe(2, 0.5);
                            float bright = 0.1 + getMainStateSafe(3, 0.5) * 0.7; // More brightness range
                            float sat = 0.3 + getMainStateSafe(4, 0.5) * 0.7; // More saturation range
                            float hueBase = getMainStateSafe(5, 0.5);
                            // Complexity now strongly influences dynamic parameters
                            float flowSpeed = 0.02 + getMainStateSafe(6, 0.0) * 0.45 * (0.5 + complexity * 1.5); // More dynamic flow
                            float warpAmt = getMainStateSafe(7, 0.0) * 0.55 * (0.5 + complexity * 1.8); // Stronger warp, more complexity scaling
                            float compVal = getMainStateSafe(8, 0.5); // Raw complexity from state
                            float tempo = 80. + getMainStateSafe(9, 0.5) * 160.; // Wider tempo range (80-240 bpm)
                            float reverb = getMainStateSafe(10, 0.2) * (0.6 + complexity * 0.8); // Reverb amount scales with complexity
                            float leadDecay = getMainStateSafe(11, 0.5);
                            float noiseInt = getMainStateSafe(12, 0.1) * 1.1 * (0.4 + complexity * 1.6); // More noise, more complexity scaling
                            float vignette = 0.15 + getMainStateSafe(13, 0.5) * 0.8; // Adjusted vignette range
                            float pulseInt = getMainStateSafe(14, 0.5) * 0.9;
                            float masterVol = getMainStateSafe(15, 0.6);
                            float grain = getMainStateSafe(20, 0.0) * 0.12 * (0.3 + complexity * 1.7); // More grain, more complexity scaling
                            float rotationSpeed = (getMainStateSafe(21, 0.5) - 0.5) * 0.25 * (0.5 + complexity * 1.5); // Faster rotation, more complexity scaling

                            // --- Calculate complexity-influenced FBM parameters ---
                            float compH = 0.3 + compVal * 0.6 * (0.3 + complexity * 1.2); // Wider Hurst range
                            int compOct = 1 + int(compVal * 6.0 * (0.4 + complexity * 1.4)); compOct = clamp(compOct, 1, 9); // Wider octave range

                            // --- Core Visual Logic ---
                            // Global time-based effects
                            float globalPulse = pulse(time, 0.08 + complexity * 0.1); // Faster global pulse based on complexity
                            float globalRot = time * rotationSpeed; // Use dynamic rotationSpeed
                            vec2 rotatedUv = rotate2d(globalRot) * centerUv + 0.5; // Apply rotation

                            // Flow and Warp
                            float n = fbm(rotatedUv * (2.0 + complexity * 1.0) + time * 0.05, 0.5, 3); // Basic noise for flow seed
                            vec2 flowVec = vec2(cos(time * flowSpeed + globalPulse * 0.8), sin(time * flowSpeed * 1.2 + n * 0.1)) * (0.4 + complexity * 0.5);
                            vec2 warpDir = hash2(rotatedUv * (2.5 + complexity * 1.5) + time * (0.05 + complexity * 0.1)); // Faster warp dir change
                            float warpEffect = warpAmt * (0.6 + noise(rotatedUv * (2.0+complexity*2.0) + time * (0.03+complexity*0.05)) * 0.4) * pow(1.0 - distCenter * 1.1, 2.8) * (1.0 + complexity * 1.2); // Stronger warp, more dynamic
                            vec2 warpOffset = warpDir * warpEffect;
                            vec2 warpedUv = (rotatedUv - 0.5) * (1.0 - bassCut * 0.2 + kick * 0.1) + 0.5 + warpOffset; // Kick/bass subtly push UV

                            // Noise pattern
                            float baseFreq = 1.5 + arpSpeed * 4.0 * (0.7 + complexity * 0.6); // Higher base freq range
                            n = fbm(warpedUv * baseFreq + flowVec + time*(0.02 + complexity * 0.04), compH, compOct); // Faster time shift (recalculate n with new params)

                            // Color and Brightness
                            float hue = fract(hueBase + time * (0.02 + complexity * 0.03) + n * (0.08 + complexity*0.1) - bassCut * 0.25 + syncFactor * 0.15); // Faster hue shift
                            float beatPulse = pulse(time, tempo / 60.0); // Main beat pulse
                            float kickPulse = beatPulse * pow(kick, 1.8) * pulseInt; // Stronger kick pulse

                            float finalBright = bright * (1.0 + kickPulse * 1.0 - pulseInt * 0.3 + masterVol * 0.3 + globalPulse * 0.15); // Stronger pulses
                            finalBright *= pow(max(0., 1.0 - distCenter * distCenter * vignette * 3.0), 1.8); // Adjusted vignette
                            finalBright += (hash1(time * (15.0 + complexity * 10.0)) - 0.5) * (0.02 + complexity * 0.03); // Faster flicker

                            // Reverb/Decay visual effect: Trails/Glow
                            float trailEffect = reverb * leadDecay * (0.15 + complexity * 0.2);
                            float trailN = fbm(warpedUv * (baseFreq*0.7) + flowVec*0.6 - time*0.03, compH*0.7, compOct-1);
                            vec3 trailColor = hsv2rgb(vec3(fract(hue + 0.15 + complexity * 0.1), sat * 0.7, finalBright * 0.5));
                            vec3 finalColor = hsv2rgb(vec3(hue, sat, finalBright)) + trailColor * trailEffect * smoothstep(0.35, 0.65, trailN);

                            // --- Artifact Blending (More Impactful) ---
                            for(int i = 0; i < numActiveArtifacts; ++i) {
                                if (i >= MAX_ARTIFACTS_SHADER) break;
                                float sim = artifactSimilarities[i]; if(sim <= 0.05) continue;

                                // Get artifact state values
                                float artHueBase = getArtifactState(i, 5);
                                float artSat = 0.3 + getArtifactState(i, 4) * 0.8;
                                float artBright = 0.1 + getArtifactState(i, 3) * 0.8 * (0.6 + complexity * 0.7);
                                float artTempo = 80. + getArtifactState(i, 9) * 160.;
                                float artKick = getArtifactState(i, 0);
                                float artCompVal = getArtifactState(i, 8);
                                float artFlowSpeed = 0.01 + getArtifactState(i, 6) * 0.4 * (0.8 + complexity);

                                // Unique seed/offset
                                float artSeed = hash1(float(i) * 1.37 + getArtifactState(i, 30)); // Use a different state index if needed

                                // Noise mask for blending
                                float maskFreq = 2.0 + float(i) * 2.0 + artSeed * 3.0 + artCompVal * 4.0 * (0.8 + complexity * 0.5); // More dynamic freq
                                float maskTime = time * 0.05 * (0.5 + float(i+1) * 0.7 + artSeed * 0.8 + artFlowSpeed * 6.0) * (0.8 + complexity * 0.8); // Faster mask movement
                                float artMask = noise(rotatedUv * maskFreq + maskTime + artSeed * 7.0);
                                artMask = smoothstep(0.38, 0.62, artMask); // Adjust mask edges
                                // <<< CHANGE: Stronger mask influence based on similarity and complexity
                                artMask *= sim * (0.6 + complexity * 0.8);

                                // Artifact color
                                float artHue = fract(artHueBase + time * (0.01 + complexity*0.01) + n * 0.05 + artSeed * 0.2); // Slightly faster hue shift
                                vec3 artColor = hsv2rgb(vec3(artHue, artSat, artBright * (1.0 + artKick * 0.6) )); // Artifact kick boost

                                // Blend artifact color
                                // <<< CHANGE: Stronger blend factor, more complexity influence
                                finalColor = mix(finalColor, artColor, artMask * clamp(0.4 + complexity * 0.7, 0.1, 0.95));

                                // Artifact "echo pulse" - additive light
                                float artBeatPulse = pulse(time, artTempo / 60.0);
                                // <<< CHANGE: Much stronger echo pulse, more complexity influence
                                float echoIntensity = artBeatPulse * pow(artKick, 1.6) * sim * clamp(0.3 + complexity * 0.9, 0.1, 0.85);
                                finalColor += vec3(echoIntensity * artMask) * artColor * 2.5; // <<< Much brighter, colorful pulse
                            }

                            // --- Final Touches ---
                            // Grain / Noise
                            finalColor += (hash1(dot(rotatedUv, vec2(12.9898, 78.233)) + time*(1.5 + complexity*0.5)) - 0.5) * grain * noiseInt * (0.8 + complexity * 0.8); // Faster grain
                            // Sync factor adds a subtle cool overlay
                            finalColor = mix(finalColor, vec3(0.6, 0.7, 0.9), syncFactor * 0.15); // Slightly stronger sync effect
                            // Feedback flash effect
                            finalColor += feedbackIntensity * vec3(1.0, 1.0, 0.9);

                            gl_FragColor = vec4(clamp(finalColor, 0.0, 1.0), 1.0);
                        }

                    ` // <<< CHANGE: Removed end-of-line comments inside shader string
                });
                const mesh = new THREE.Mesh(geometry, this.material);
                scene.add(mesh);

                 // Log potential uniform limits after context exists
                 try { const gl = renderer.getContext(); const maxVec = gl.getParameter(gl.MAX_FRAGMENT_UNIFORM_VECTORS); console.log(`Max Fragment Uniform Vectors: ${maxVec}`); const neededVec = Math.ceil(STATE_VECTOR_SIZE / 4) + MAX_ARTIFACTS_SHADER * Math.ceil(STATE_VECTOR_SIZE / 4) + Math.ceil(MAX_ARTIFACTS_SHADER / 4) + 25; console.log(`Estimated Needed Vectors: ~${neededVec}`); if (maxVec < neededVec) { console.warn(`Potential Uniform Limit Issue: Max uniforms (${maxVec}) might be less than estimated need (~${neededVec}). Reducing shader artifacts might help.`); } } catch (e) { console.warn("Could not get WebGL uniform limits."); }
            }

            detectMaxUniformCapacity() {
                let detectedMax = 1;
                try {
                    const gl = renderer.getContext(); // Renderer should exist now
                    if (!gl) { console.warn("Detect uniform capacity: No GL context available yet."); return 1; } // Fallback if no context
                    const maxUniformVectors = gl.getParameter(gl.MAX_FRAGMENT_UNIFORM_VECTORS);
                    // Estimate base uniforms needed
                    const baseVectorsUsed = Math.ceil(STATE_VECTOR_SIZE / 4.0) + 25;
                    // Estimate uniforms per artifact
                    const vectorsPerArtifact = Math.ceil(STATE_VECTOR_SIZE / 4.0) + 1;
                    const availableVectors = maxUniformVectors - baseVectorsUsed;

                    if (availableVectors > 0 && vectorsPerArtifact > 0) {
                         // Be conservative (use 80% capacity)
                        detectedMax = Math.max(1, Math.floor((availableVectors / vectorsPerArtifact) * 0.8));
                    } else {
                        detectedMax = 1; // Fallback if calculation fails
                    }
                    // <<< CHANGE: Removed HIGH_PERFORMANCE_MODE check, use a fixed reasonable cap
                    const reasonableCap = 4; // Cap at 4 shader artifacts regardless of detected limits
                    detectedMax = Math.min(detectedMax, reasonableCap, MAX_ARTIFACTS); // Ensure it doesn't exceed overall max artifacts or reasonable cap
                } catch (e) {
                    console.warn("Error detecting uniform capacity:", e);
                    detectedMax = 1; // Fallback on error
                }
                return detectedMax;
            }

            _createArtifactUniformArray() { return new Float32Array(MAX_ARTIFACTS_SHADER * STATE_VECTOR_SIZE).fill(0.5); }

            update(mainStateArray, time, currentActiveArtifactInfo, currentComplexity, currentSyncFactor, currentFeedbackIntensity) {
                if (!this.material || !renderer) return;
                this.material.uniforms.time.value = time;
                this.material.uniforms.complexity.value = currentComplexity;
                this.material.uniforms.syncFactor.value = currentSyncFactor;
                this.material.uniforms.feedbackIntensity.value = currentFeedbackIntensity;

                // Update main state uniform
                if (mainStateArray?.length === STATE_VECTOR_SIZE) {
                    this.material.uniforms.mainState.value = Float32Array.from(mainStateArray); // Ensure it's a Float32Array
                    this.material.uniforms.mainState.needsUpdate = true;
                }

                // Update artifact uniforms
                const artifactStatesFlat = this.material.uniforms.artifactStates.value;
                const artifactSimilarities = this.material.uniforms.artifactSimilarities.value;
                artifactStatesFlat.fill(0.5); // Reset artifact states
                artifactSimilarities.fill(0.0); // Reset similarities

                const numToSend = Math.min(currentActiveArtifactInfo.ids.length, MAX_ARTIFACTS_SHADER);
                this.material.uniforms.numActiveArtifacts.value = numToSend;

                for (let i = 0; i < numToSend; ++i) {
                    const stateArr = currentActiveArtifactInfo.stateArrays[i];
                    const sim = currentActiveArtifactInfo.similarities[i];
                    if (stateArr?.length === STATE_VECTOR_SIZE) {
                        artifactStatesFlat.set(stateArr, i * STATE_VECTOR_SIZE); // Copy state into flat array
                        artifactSimilarities[i] = sim; // Set similarity
                    } else {
                        artifactSimilarities[i] = 0.0; // Mark as invalid/inactive if data is bad
                        if(stateArr) console.warn(`Artifact data invalid size (${stateArr.length}) for shader slot ${i}`);
                    }
                }
                this.material.uniforms.artifactStates.needsUpdate = true;
                this.material.uniforms.artifactSimilarities.needsUpdate = true;

                // Render the scene
                try {
                    if (renderer.getContext().isContextLost()) {
                        console.error("WebGL Context Lost! Cannot render.");
                        showError("WebGL Context Lost!");
                        return; // Stop rendering if context is lost
                    }
                    renderer.render(scene, camera);
                }
                catch (e) {
                    console.error("THREE.WebGLRenderer.render error:", e);
                    if (e.message && e.message.includes("context lost")) {
                        showError("WebGL Context Lost!");
                    } else {
                        showError(`Render Error: ${e.message}`); // Show other render errors
                    }
                }
            }

            resize() {
                const w=window.innerWidth; const h=window.innerHeight;
                camera.aspect=w/h; camera.updateProjectionMatrix();
                // <<< CHANGE: Use consistent pixel ratio from constructor
                const pr=renderer.getPixelRatio();
                renderer.setSize(w,h); // Already sets pixel ratio internally if done in constructor
                if (this.material) this.material.uniforms.resolution.value.set(w*pr,h*pr);
            }
        }

        // --- Audio Controller ---
        class AudioController {
             constructor() {
                 this.audioWorkletNode = null; this.isInitialized = false; this.isInitializing = false; this.micStreamSource = null;
                 this.audioWarningDisplayed = false; this.warningTimeout = null;
                 // Add interaction listeners to initialize audio context
                 document.body.addEventListener('pointerdown', () => this.tryInitializeAudio(), { once: true, passive: true });
                 document.body.addEventListener('touchstart', () => this.tryInitializeAudio(), { once: true, passive: true });
                 // Handle page visibility changes
                 document.addEventListener('visibilitychange', async () => {
                     if (!audioContext) return;
                     if (document.hidden) {
                         if (audioContext.state === 'running') {
                             console.log("Suspending AC due to page visibility");
                             await audioContext.suspend().catch(e=>console.warn("AC suspend err:", e));
                         }
                     } else {
                         // Attempt to re-initialize/resume when page becomes visible again
                         await this.tryInitializeAudio();
                     }
                 });
             }

             showWarning(msg, dur=5000) {
                 const w=document.getElementById('warningInfo');
                 if(w){
                     if(this.warningTimeout) clearTimeout(this.warningTimeout);
                     w.textContent=msg; w.style.display='block'; w.style.color='yellow';
                     this.audioWarningDisplayed=true;
                     if(dur>0) this.warningTimeout=setTimeout(()=>this.hideWarning(), dur);
                 }
             }
             hideWarning() {
                 const w=document.getElementById('warningInfo');
                 if(w && this.audioWarningDisplayed){
                     w.style.display='none'; this.audioWarningDisplayed=false;
                     if(this.warningTimeout){ clearTimeout(this.warningTimeout); this.warningTimeout=null; }
                 }
             }

             // <<< CHANGE: More robust AudioContext initialization/recreation >>>
             async tryInitializeAudio() {
                 if (this.isInitializing || (this.isInitialized && audioContext?.state === 'running')) return; // Already running or init in progress
                 this.isInitializing = true; interactionOccurred = true;
                 console.log("Trying AudioContext init/resume...");

                 try {
                     // Close existing context if it's in a bad state (interrupted, closed)
                     if (audioContext && audioContext.state !== 'running') {
                         console.log(`Closing previous AudioContext (state: ${audioContext.state})`);
                         await audioContext.close().catch(e => console.error("AudioContext close error:", e));
                         // Reset related components
                         audioContext=null; this.audioWorkletNode=null; this.isInitialized=false;
                         this.micStreamSource=null; analyserNode=null; masterGain=null;
                         currentInputState.mic.available=false;
                     }

                     // Create new context if needed
                     if (!audioContext) {
                         audioContext = new (window.AudioContext || window.webkitAudioContext)({ latencyHint: 'interactive', sampleRate: 44100 });
                         console.log(`AudioContext created. State: ${audioContext.state}, Sample Rate: ${audioContext.sampleRate}`);
                         // Recreate essential nodes
                         masterGain = audioContext.createGain();
                         masterGain.gain.setValueAtTime(1.0, audioContext.currentTime); // Start at default gain
                         masterGain.connect(audioContext.destination);
                         analyserNode = audioContext.createAnalyser();
                         analyserNode.fftSize = MIC_FFT_SIZE;
                         analyserNode.smoothingTimeConstant = 0.5;
                         currentInputState.mic.fft = new Float32Array(analyserNode.frequencyBinCount); // Ensure FFT array matches analyser
                     }

                     // Resume if suspended
                     if (audioContext.state === 'suspended') {
                         console.log("AudioContext suspended, resuming...");
                         await audioContext.resume();
                         console.log(`AudioContext resumed. State: ${audioContext.state}`);
                     }
                 } catch (e) {
                     console.error("AudioContext create/resume failed:", e);
                     showError("Audio Error: Context init failed.");
                     this.isInitializing = false;
                     return; // Don't proceed if context failed
                 }

                 // Proceed with Worklet setup only if context is running and not already initialized
                 if (audioContext.state === 'running' && !this.isInitialized) {
                     console.log("Adding AudioWorklet...");
                     try {
                          // <<< CHANGE: Audio Worklet Code v0.6.0 - Boosted gains, wider BPM, generative snare, more dynamics >>>
                          const processorCode = `
                              const WORKLET_STATE_SIZE = ${STATE_VECTOR_SIZE};
                              // <<< CHANGE: Adjusted base BPM
                              const BASE_BPM = 80.0;
                              // Utility functions
                              function fract(n){return n-Math.floor(n);}
                              function lerp(a,b,t){return a+(b-a)*t;}
                              function clamp(v,min,max){return Math.min(max,Math.max(min,v));}
                              function hash(n){return fract(Math.sin(n*12.9898)*43758.5);}
                              function hashSigned(n){return Math.sin(n*78.233)*.5+.5;} // -1 to 1 range approx
                              // <<< CHANGE: Slightly stronger soft clip default
                              function softClip(x, k = 1.8) { return Math.tanh(x * k); }

                              // LFO shapes
                              function sineLFO(p){return(Math.sin(p*2*Math.PI)+1)*.5;} // 0 to 1
                              function sawLFO(p){return p;} // 0 to 1
                              function triLFO(p){return 1-Math.abs(fract(p+.5)*2-1);} // 0 to 1
                              function sqrLFO(p,d=.5){return p<d?1:0;} // 0 or 1

                              // State Variable Filter (SVF) - for versatile filtering
                              class SVF{
                                  constructor(){this.z1=0;this.z2=0;this.g=0;this.k=0;this.inv_den=0;}
                                  setParams(cutoffNorm, resNorm, sampleRate){
                                      const f = 20 * Math.pow(1000, clamp(cutoffNorm,.01,.99)); // Map 0-1 to 20-20k Hz exponentially
                                      const q = 0.5 + clamp(resNorm,0,.98)*19.5; // Map 0-1 to Q 0.5-20
                                      this.g=Math.tan(Math.PI*f/sampleRate);
                                      this.k=1/q;
                                      const g2=this.g*this.g;
                                      this.inv_den=1/(1+this.k*this.g+g2);
                                  }
                                  process(input){
                                      const v0=input;
                                      const v1=(this.z1+this.g*(v0-this.z2))*this.inv_den;
                                      const v2=(this.z2+this.g*v1)*this.inv_den;
                                      this.z1=v1+(v1-this.z1); // Update state with 2x integrator
                                      this.z2=v2+(v2-this.z2);
                                      return{lowpass:v2, bandpass:v1, highpass:(v0-this.k*v1-v2), notch:v2+(v0-this.k*v1-v2)};
                                  }
                              }

                              class GenerativeProcessor extends AudioWorkletProcessor {
                                  constructor(options) {
                                      super(options);
                                      this.sr=sampleRate;
                                      this.phase=0; // Master phase counter
                                      this.state=new Array(WORKLET_STATE_SIZE).fill(.5); // Current resonant state
                                      this.lastBeatPhase=0; this.lastSixteenthPhase=0;
                                      this.beatCounter=0; this.sixteenthCounter=0;
                                      this.complexity=0.5; // Global complexity level

                                      // Filters for different synth sections (stereo)
                                      this.filters={
                                          bass:[new SVF(),new SVF()],
                                          noise:[new SVF(),new SVF()],
                                          lead:[new SVF(),new SVF()],
                                          hat:[new SVF(),new SVF()],
                                          snare:[new SVF(), new SVF()] // <<< CHANGE: Added snare filter
                                      };

                                      // Delay effect
                                      const maxDelaySeconds = 1.2;
                                      this.delayBuffer=[new Float32Array(Math.ceil(this.sr*maxDelaySeconds)), new Float32Array(Math.ceil(this.sr*maxDelaySeconds))];
                                      this.delayWritePos=[0,0];
                                      this.delayTimeSamples=[this.sr*.375,this.sr*.375]; // Current delay time per channel
                                      this.delayFeedback=[.5,.5]; this.delayMix=.3;

                                      // Reverb (simplified using Comb filters + Allpass filters)
                                      const combDelaysSeconds=[.0311,.0383,.0427,.0459]; // Comb delay lengths (seconds)
                                      const createCombBuf=s=>new Float32Array(Math.ceil(this.sr*s));
                                      this.combBuffers=combDelaysSeconds.map(l=>[createCombBuf(l),createCombBuf(l)]);
                                      this.combWritePos=combDelaysSeconds.map(()=>[0,0]);
                                      this.combFeedback=.8; this.combDamping=.3; this.combLastSample=combDelaysSeconds.map(()=>[0,0]); // For LPF in feedback

                                      const apDelaysSeconds=[.0053,.0121]; // Allpass delay lengths
                                      const createAPBuf=s=>new Float32Array(Math.ceil(this.sr*s));
                                      this.apBuffers=apDelaysSeconds.map(l=>[createAPBuf(l),createAPBuf(l)]);
                                      this.apWritePos=apDelaysSeconds.map(()=>[0,0]);
                                      this.apFeedback=.5; this.reverbMix=.2;

                                      // LFOs for modulation
                                      this.lfoPhase={filter:0, pitch:0, noise:0, fx:0};
                                      this.lfoRate={filter:.1, pitch:3, noise:.2, fx:.05}; // Base rates Hz
                                      this.lastMasterLevel = 1.0; // Store last level for smoothing

                                      // Handle messages from main thread (state updates)
                                      this.port.onmessage=(e)=>{
                                          if(e.data.state?.length===WORKLET_STATE_SIZE) this.state=e.data.state;
                                          if(typeof e.data.complexity === 'number') this.complexity=clamp(e.data.complexity,0,1);
                                      };
                                      console.log("Worklet Processor Initialized. SR:", this.sr);
                                  }

                                  // Define automatable parameters
                                  static get parameterDescriptors(){
                                      // <<< CHANGE: Increased max gain slightly more -> 3.0
                                      return[{name:'masterLevel', defaultValue:1.0, minValue:0, maxValue:3.0, automationRate: 'a-rate'}];
                                  }

                                  // Helper to safely get state values
                                  getStateVal(index, defaultVal) {
                                       const s = this.state; // Cache locally
                                       return (s && index >= 0 && index < s.length && s[index] !== undefined) ? s[index] : defaultVal;
                                  }

                                  process(inputs, outputs, parameters) {
                                      const output=outputs[0]; // Assuming stereo output
                                      const bufferSize=output[0].length;
                                      const masterLevelParam=parameters.masterLevel; // Get master level parameter array
                                      const srInv = 1.0 / this.sr; // Inverse sample rate
                                      const g = this.getStateVal.bind(this); // Alias for shorter calls
                                      const comp = this.complexity; // Cache complexity

                                      // --- Extract state parameters safely ---
                                      // Parameters influenced by state vector AND complexity
                                      const kickIntensity = 0.9 + g(0,.5) * 0.7; // Kick presence/gain (more range)
                                      const kickTightness = g(1,.5); // Kick decay/pitch env
                                      const bassCutoffBase = .015 + g(2,.5) * .45; // Bass filter cutoff base (slightly lower base)
                                      const bassResBase = (g(3,.5)*0.95); // Bass filter resonance base (index 3 from feature map)
                                      const bassRes = bassResBase * (0.7 + comp * 0.8); // Resonance increases with complexity
                                      const bassPattern = Math.floor(g(4,0) * 6); // Bass rhythm pattern index (added one more pattern)
                                      const bassOctave = Math.floor(g(20,.5) * 3) - 1; // Bass octave offset
                                      const bassDecay = g(22,.1) * .8 * (0.7 + comp * 0.6); // Bass envelope decay, more complexity influence

                                      const noiseLevelBase = .0005 + g(12,.1) * .18; // Noise level base (wider range)
                                      const noiseLevel = noiseLevelBase * (0.5 + comp * 1.5); // Noise level increases significantly with complexity
                                      const noiseCutoffBase = .04 + g(13,.5) * .9; // Noise filter cutoff base (wider range)
                                      const noiseResBase = (g(14,.5) * 0.95); // Noise filter resonance base
                                      const noiseRes = noiseResBase * (0.7 + comp * 0.7); // Resonance increases with complexity
                                      const noiseLfoSpeed = (.04 + g(24,.5) * .7) * (0.6 + comp * 1.0); // Noise LFO speed increases more with complexity

                                      const leadPresenceBase = g(16,.5) * 1.4; // Lead presence/gain base (more range)
                                      const leadPresence = leadPresenceBase * (0.7 + comp*0.5); // Lead gets slightly louder with complexity
                                      const leadCutoffBase = .06 + g(6,.5) * .8; // Lead filter cutoff base (wider range)
                                      const leadResBase = (g(7,.5) * 0.92); // Lead filter resonance base
                                      const leadRes = leadResBase * (0.7 + comp * 0.7); // Resonance increases with complexity
                                      const leadPitchModDepthBase = g(18,.2) * .9; // Lead pitch LFO depth base (more range)
                                      const leadPitchModDepth = leadPitchModDepthBase * (0.7 + comp * 0.8); // Modulation increases more with complexity
                                      const leadPattern = Math.floor(g(19,0) * 6); // Lead rhythm pattern index (added one more)
                                      const leadDecayBase = (.02 + g(11,.5) * .7); // Lead envelope decay base (wider range)
                                      const leadDecay = leadDecayBase * (0.6 + comp * 0.8); // Decay time influenced more by complexity
                                      const leadLfoRateBase = (.08 + g(21,.5) * 8); // Lead LFO rate base (wider range)
                                      const leadLfoRate = leadLfoRateBase * (0.5 + comp * 1.2); // LFO rate increases significantly with complexity

                                      const hat1LevelBase = g(23,.5) * 0.9; // Hat 1 level base (more range)
                                      const hat1Level = hat1LevelBase * (0.8 + comp * 0.6); // Hats louder with complexity
                                      const hat2LevelBase = g(25,.5) * 0.8; // Hat 2 level base (more range)
                                      const hat2Level = hat2LevelBase * (0.8 + comp * 0.5);
                                      const hatPattern = Math.floor(g(26,0) * 6); // Hat rhythm pattern index (added one more)
                                      const hatDecay1 = .006 + g(28,.5) * .06; // Hat 1 decay (wider range)
                                      const hatDecay2 = .03 + g(29,.5) * .22; // Hat 2 decay (wider range)
                                      const hatHpCutoff = .2 + g(30,.5) * .7; // Hat highpass filter cutoff (wider range)

                                      // <<< CHANGE: Added Snare Parameters >>>
                                      const snarePresence = g(27, 0.5) * (0.8 + comp * 0.5); // How likely/loud the snare is
                                      const snareTone = g(38, 0.5); // Controls noise filter cutoff for snare body
                                      const snareSnap = 0.003 + (1.0 - g(1, 0.5)) * 0.005; // Snappy decay based on kickTightness inversely
                                      const snareBodyDecay = 0.05 + snareTone * 0.15;

                                      this.lfoRate.fx = (0.02 + g(36,.5)*.18) * (0.8 + comp * 0.8); // FX LFO rate (wider range)
                                      const fxLfoVal = sineLFO(this.lfoPhase.fx); // Current FX LFO value

                                      const delayTimeBase = .04 + g(31,.5) * .9; // Base delay time (wider range)
                                      const delayFeedbackBase = clamp((g(32,.5)*(0.8+comp*0.3) + fxLfoVal*.1), 0, .985); // Feedback base + LFO mod, complexity influence
                                      this.delayFeedback[0] = this.delayFeedback[1] = delayFeedbackBase; // Use calculated value directly
                                      const delayMixBase = clamp(g(33,.3)*.9, 0, 1); // Delay mix level base (more range)
                                      this.delayMix = delayMixBase * (0.7 + comp * 0.5); // Mix increases with complexity

                                      const reverbMixBase = clamp(g(10,.2)*.8, 0, 1); // Reverb mix level base (more range)
                                      this.reverbMix = reverbMixBase * (0.7 + comp * 0.5); // Mix increases with complexity
                                      const combFeedbackBase = (.65+clamp(g(34,.5),0,1)*.32); // Comb feedback (wider range)
                                      this.combFeedback = combFeedbackBase * (0.85 + comp * 0.25);
                                      const combDampingBase = (.03+clamp(g(35,.5),0,1)*.75*(.4+comp*.6)); // Comb damping (wider range, more comp influence)
                                      this.combDamping = combDampingBase * (0.7 + comp * 0.5);
                                      const apFeedbackBase = (.35+g(37,.5)*.35); // Allpass feedback (wider range)
                                      this.apFeedback = apFeedbackBase * (0.85 + comp * 0.25);

                                      // Calculate tempo and timing
                                      // <<< CHANGE: Wider BPM range
                                      const bpm = BASE_BPM + g(9,.5) * 160.0; // Approx 80-240 BPM
                                      const secondsPerBeat = 60.0 / clamp(bpm, 40, 260); // Clamp tempo for stability
                                      const samplesPerBeat = secondsPerBeat * this.sr;

                                      // Update LFO phases
                                      this.lfoPhase.filter = fract(this.lfoPhase.filter + bufferSize * this.lfoRate.filter * srInv * (0.8 + comp * 0.4));
                                      this.lfoPhase.pitch = fract(this.lfoPhase.pitch + bufferSize * leadLfoRate * srInv); // Use complexity-influenced rate
                                      this.lfoPhase.noise = fract(this.lfoPhase.noise + bufferSize * noiseLfoSpeed * srInv); // Use complexity-influenced rate
                                      this.lfoPhase.fx = fract(this.lfoPhase.fx + bufferSize * this.lfoRate.fx * srInv); // Use complexity-influenced rate

                                      // Calculate LFO values for this block
                                      const filterLfoVal = triLFO(this.lfoPhase.filter);
                                      const leadPitchLfoVal = sineLFO(this.lfoPhase.pitch);
                                      const noiseLfoVal = triLFO(this.lfoPhase.noise);

                                      // Calculate modulated filter parameters
                                      const fBassCut = clamp(bassCutoffBase + filterLfoVal*.3,.01,.95); // Increased LFO influence
                                      const fNoiseCut = clamp(noiseCutoffBase + noiseLfoVal*.6,.01,.95); // More LFO influence on noise filter
                                      const fLeadCut = clamp(leadCutoffBase + leadPitchLfoVal*.6,.01,.95); // Increased LFO influence
                                      // <<< CHANGE: Added Snare Filter Params >>>
                                      const fSnareCut = clamp(0.1 + snareTone * 0.8, 0.05, 0.9); // Map snareTone to cutoff
                                      const fSnareRes = clamp(0.3 + (1.0 - snareTone) * 0.6, 0.1, 0.9); // Opposite resonance

                                      // Set filter parameters
                                      this.filters.bass.forEach(f=>f.setParams(fBassCut, bassRes, this.sr));
                                      this.filters.noise.forEach(f=>f.setParams(fNoiseCut, noiseRes, this.sr));
                                      this.filters.lead.forEach(f=>f.setParams(fLeadCut, leadRes, this.sr));
                                      this.filters.hat.forEach(f=>f.setParams(hatHpCutoff, 0.1, this.sr)); // Fixed low resonance for HP
                                      // <<< CHANGE: Set snare filter params
                                      this.filters.snare.forEach(f=>f.setParams(fSnareCut, fSnareRes, this.sr));

                                      // --- Main Processing Loop ---
                                      for(let ch=0; ch<output.length; ++ch){ // Stereo processing
                                          const outCh=output[ch];
                                          // Get channel-specific buffers/state
                                          const delayB=this.delayBuffer[ch]; let dwp=this.delayWritePos[ch];
                                          const combBs=this.combBuffers.map(b=>b[ch]); let cwp=this.combWritePos.map((p, idx)=>this.combWritePos[idx][ch]); let cls=this.combLastSample.map((s, idx)=>this.combLastSample[idx][ch]);
                                          const apBs=this.apBuffers.map(b=>b[ch]); let awp=this.apWritePos.map((p, idx)=>this.apWritePos[idx][ch]);
                                          // Get channel-specific filters
                                          const bassF=this.filters.bass[ch]; const noiseF=this.filters.noise[ch];
                                          const leadF=this.filters.lead[ch]; const hatF=this.filters.hat[ch];
                                          const snareF=this.filters.snare[ch]; // <<< CHANGE: Get snare filter
                                          // Calculate panning LFO for delay time variation
                                          const panLfo=ch===0?triLFO(this.lfoPhase.noise):triLFO(fract(this.lfoPhase.noise+.5));
                                          const currentDelayTimeSamples = Math.floor(clamp((delayTimeBase * (1.0 + panLfo*0.2)) * this.sr, 1, delayB.length-1)); // More LFO range
                                          this.delayTimeSamples[ch] = currentDelayTimeSamples;

                                          for(let i=0; i<bufferSize; ++i){
                                              // Get current master level - smooth changes
                                              const targetMasterLevel = masterLevelParam.length > 1 ? masterLevelParam[i] : masterLevelParam[0];
                                              const currentMasterLevel = this.lastMasterLevel + (targetMasterLevel - this.lastMasterLevel) * 0.005; // Simple low-pass smoothing
                                              this.lastMasterLevel = currentMasterLevel;

                                              // Timing calculations
                                              const currentPhase = this.phase+i;
                                              const currentTime = currentPhase * srInv;
                                              const currentBeat = currentTime / secondsPerBeat;
                                              const beatPhase = fract(currentBeat); // Phase within the current beat (0-1)
                                              const sixteenthPhase = fract(currentBeat * 4.0); // Phase within the current 16th note
                                              const currentSixteenthNum = Math.floor(currentBeat * 4.0);
                                              const sixteenthInMeasure = currentSixteenthNum % 16; // 0-15

                                              // Trigger detection
                                              const beatTrigger = beatPhase < this.lastBeatPhase;
                                              const sixteenthTrigger = sixteenthPhase < this.lastSixteenthPhase;
                                              if(beatTrigger) this.beatCounter = Math.floor(currentBeat) % 16; // Counter for patterns (0-15)
                                              if(sixteenthTrigger) this.sixteenthCounter = sixteenthInMeasure; // Use direct 16th number
                                              this.lastBeatPhase = beatPhase; this.lastSixteenthPhase = sixteenthPhase;

                                              // Initialize sound sources
                                              let kick=0, bass=0, noise=0, lead=0, hat1=0, hat2=0, snare=0; // <<< CHANGE: Added snare

                                              // Kick Drum Synthesis (More Punch)
                                              if(beatTrigger && (this.beatCounter % 4 === 0 || (this.beatCounter % 4 === 2 && hash(this.beatCounter + comp * 5.0) > .5))){ // Slightly complexity influenced pattern
                                                  const decay = .008 + kickTightness * .12; // Tight decay range
                                                  const pitchEnvAmt = 60 + kickTightness * 900; // More pitch sweep
                                                  const startPitch = 50 + kickTightness * 25; // Starting pitch
                                                  const pitch = startPitch + pitchEnvAmt * Math.exp(-beatPhase / (decay * .05 + .0005)); // Faster exponential pitch sweep
                                                  const amp = Math.exp(-beatPhase / decay); // Exponential amplitude decay
                                                  const click = (hash(currentTime * 9123.4) - .5) * Math.exp(-beatPhase / .001) * .5; // Sharper click component
                                                  const noiseComp = (hash(currentTime * 4567.8) - .5) * Math.exp(-beatPhase / .005) * .3 * (1.0-kickTightness); // More prominent noise burst
                                                  kick = Math.sin(pitch * beatPhase * 2 * Math.PI * (1.0 + Math.sin(beatPhase * Math.PI * 8.0)*0.02) ) * amp + click + noiseComp; // Add FM wobble
                                                  // <<< CHANGE: Boosted kick gain SIGNIFICANTLY
                                                  kick *= kickIntensity * 8.0;
                                              }

                                              // Bass Synth Synthesis
                                              let playBass = false;
                                              const baseBassFreq = 41.2 * Math.pow(2, bassOctave); // Base frequency with octave offset
                                              switch(bassPattern){
                                                  case 0: playBass = sixteenthTrigger && (sixteenthInMeasure % 4 !== 0); break; // Offbeat
                                                  case 1: playBass = sixteenthTrigger && [1,2,5,6,9,10,13,14].includes(sixteenthInMeasure); break; // Running
                                                  case 2: playBass = sixteenthTrigger && (sixteenthInMeasure % 2 === 1); break; // Every other 16th
                                                  case 3: playBass = sixteenthTrigger && (hash(currentSixteenthNum + comp * 3.0) > .35); break; // Randomish (complexity influence)
                                                  case 4: playBass = sixteenthTrigger && [0,4,8,12,14].includes(sixteenthInMeasure); break; // Syncopated
                                                  case 5: playBass = sixteenthTrigger && sixteenthInMeasure % 8 === 0; break; // On the beat (sparse)
                                              }
                                              if(playBass){
                                                  const bassEnv = Math.exp(-sixteenthPhase / (.03 + bassDecay * .18)); // Envelope
                                                  const sawPhase = fract(currentTime * baseBassFreq); // Basic saw oscillator phase
                                                  const sawPhase2 = fract(currentTime * baseBassFreq * (1.003 + comp*0.004)); // Slightly detuned (comp influence)
                                                  let rawBass = (sawPhase * 2 - 1)*0.5 + (sawPhase2 * 2 - 1)*0.5; // Mix two saws
                                                  rawBass = softClip(rawBass * (1.2 + bassDecay * 5.0)) * bassEnv; // Apply env + drive
                                                  // <<< CHANGE: Boosted bass gain
                                                  bass = bassF.process(rawBass * 6.0).lowpass; // Filter and gain
                                              }

                                              // Hat Synthesis (Filtered Noise)
                                              let playHat1=false, playHat2=false;
                                              switch(hatPattern){
                                                  case 0: playHat1=sixteenthTrigger; playHat2=sixteenthTrigger&&(sixteenthInMeasure%4===2); break; // Standard
                                                  case 1: playHat1=sixteenthTrigger&&(sixteenthInMeasure%2===1); playHat2=sixteenthTrigger&&[6,14].includes(sixteenthInMeasure); break; // Offbeat
                                                  case 2: playHat1=sixteenthTrigger&&sixteenthInMeasure%3!==0; playHat2=sixteenthTrigger&&sixteenthInMeasure===10; break; // Triplet feel
                                                  case 3: playHat1=sixteenthTrigger&&hash(currentSixteenthNum*1.9 + comp)>.4; playHat2=sixteenthTrigger&&hash(currentSixteenthNum*2.1+.5 + comp)>.7; break; // Random (comp influence)
                                                  case 4: playHat1=sixteenthTrigger&&[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15].filter(x=>x%2==0||x%3==0).includes(sixteenthInMeasure); playHat2=sixteenthTrigger&&sixteenthInMeasure%8===6; break; // Busy
                                                  case 5: playHat1=sixteenthTrigger&&[0,2,4,6,8,10,12,14].includes(sixteenthInMeasure); playHat2=sixteenthTrigger&&sixteenthInMeasure%8===4; break; // Straight 8ths/accent
                                              }
                                              if(playHat1){
                                                  hat1 = hatF.process((hash(currentTime*5432.1)*2-1)*Math.exp(-sixteenthPhase/hatDecay1)).highpass;
                                                  // <<< CHANGE: Boosted hat gain
                                                  hat1 *= hat1Level * 5.5;
                                              }
                                              if(playHat2){
                                                  hat2 = hatF.process((hash(currentTime*9876.5)*2-1)*Math.exp(-sixteenthPhase/hatDecay2)).highpass;
                                                   // <<< CHANGE: Boosted hat gain
                                                  hat2 *= hat2Level * 5.0;
                                              }

                                              // <<< CHANGE: Generative Snare/Clap Synthesis >>>
                                              if(beatTrigger && snarePresence > 0.1 && (this.beatCounter % 4 === 1 || this.beatCounter % 4 === 3) ) { // On 2 and 4
                                                  if(hash(this.beatCounter + comp * 2.0) < snarePresence) { // Probability based on state/complexity
                                                      const noiseBurst = (hash(currentTime * 1234.5) * 2 - 1) * Math.exp(-beatPhase / snareSnap);
                                                      const bodyNoise = (hash(currentTime * 6789.0) * 2 - 1) * Math.exp(-beatPhase / snareBodyDecay);
                                                      const rawSnare = noiseBurst * 0.7 + bodyNoise * 0.3;
                                                      const filteredSnare = snareF.process(rawSnare).bandpass; // Use bandpass
                                                      snare = filteredSnare * (snarePresence + 0.5) * 6.0; // <<< Boosted Snare Gain
                                                  }
                                              }

                                              // Noise Source Synthesis
                                              noise = noiseF.process((hash(currentPhase*.5)-.5)*2*noiseLevel).bandpass; // Filtered noise
                                              // <<< CHANGE: Boosted noise gain
                                              noise *= 5.5;

                                              // Lead Synth Synthesis
                                              let playLead = false;
                                              const leadBaseFreq = 87.3 * Math.pow(2, g(17,0.5) * 2 - 1); // Approx F#2 +/- Octave controlled by index 17
                                              switch(leadPattern){
                                                  case 0: playLead = sixteenthTrigger && [0,3,7,10].includes(sixteenthInMeasure); break; // Arp-like
                                                  case 1: playLead = sixteenthTrigger && (hash(Math.floor(currentBeat*2 + comp)) > .6); break; // Sparser (comp influence)
                                                  case 2: playLead = sixteenthTrigger && (sixteenthInMeasure===0||sixteenthInMeasure===8||sixteenthInMeasure===12); break; // Syncopated
                                                  case 3: playLead = sixteenthTrigger && (hash(currentSixteenthNum*1.1 + comp * 2.0) > .8); break; // Random (comp influence)
                                                  case 4: playLead = sixteenthTrigger && Math.sin(currentBeat*Math.PI*(1.5 + comp*0.5) + Math.sin(currentBeat*Math.PI*(.4+comp*0.2))*3) > .4; break; // Weirder rhythm (comp influence)
                                                  case 5: playLead = sixteenthTrigger && [0, 1, 3, 5, 8, 11, 13].includes(sixteenthInMeasure); break; // More melodic pattern
                                              }
                                              if(playLead && leadPresence > .05){
                                                  const leadEnv = Math.exp(-sixteenthPhase / leadDecay);
                                                  const pitchMod = leadPitchLfoVal * leadPitchModDepth; // LFO pitch mod
                                                  const noteChoice = hash(Math.floor(currentSixteenthNum / (2 + comp * 2.0))); // Change note less often with lower complexity
                                                  const scale = [0, 2, 4, 5, 7, 9, 11]; // Minor pentatonic-ish steps
                                                  const noteIndex = Math.floor(noteChoice * scale.length);
                                                  const freq = leadBaseFreq * Math.pow(2, pitchMod + scale[noteIndex % scale.length] / 12.0);
                                                  const sawPhase = fract(currentTime * freq);
                                                  const pulseWidth = 0.5 + Math.sin(currentTime * freq * 0.1) * 0.4 * comp; // PWM based on complexity
                                                  const sqrOsc = (sawPhase < pulseWidth ? 1.0 : -1.0) * 0.5; // Simple square wave
                                                  const sawOsc = (sawPhase * 2.0 - 1.0) * 0.5; // Saw wave
                                                  const rawLead = (sawOsc + sqrOsc) * leadEnv; // Mix saw and square
                                                  // <<< CHANGE: Boosted Lead Gain
                                                  lead = leadF.process(rawLead * 2.5 * leadPresence).bandpass;
                                              }

                                              // --- Summing and Effects ---
                                              // <<< CHANGE: Added snare to the mix
                                              let drySignal = kick + bass + hat1 + hat2 + snare + noise + lead;
                                              // <<< CHANGE: Increased drive on summed dry signal
                                              drySignal = softClip(drySignal * 1.8);

                                              // Delay Processing
                                              const delayReadPos = (dwp - currentDelayTimeSamples + delayB.length) % delayB.length;
                                              const delayedSample = delayB[Math.floor(delayReadPos)];
                                              delayB[dwp] = clamp(drySignal + delayedSample * this.delayFeedback[ch], -1, 1); // Write current sample + feedback

                                              // Reverb Processing (Comb + Allpass)
                                              let combSum = 0;
                                              for(let c=0; c<this.combBuffers.length; c++){
                                                  const combLenSamples = Math.floor(this.combBuffers[c][ch].length);
                                                  const combReadPos = (cwp[c] - combLenSamples + combLenSamples) % combLenSamples;
                                                  let combOut = this.combBuffers[c][ch][combReadPos];
                                                  // Simple Low-pass filter in feedback loop
                                                  cls[c] = combOut * (1.0 - this.combDamping) + cls[c] * this.combDamping;
                                                  this.combBuffers[c][ch][cwp[c]] = clamp(drySignal + cls[c] * this.combFeedback, -1, 1);
                                                  combSum += combOut;
                                                  cwp[c] = (cwp[c] + 1) % combLenSamples;
                                              }
                                              combSum *= (1.0 / this.combBuffers.length); // Average output of combs

                                              let apInput = combSum; let apOutput = 0;
                                              for(let a=0; a<this.apBuffers.length; a++){
                                                  const apLenSamples = Math.floor(this.apBuffers[a][ch].length);
                                                  const apReadPos = (awp[a] - apLenSamples + apLenSamples) % apLenSamples;
                                                  apOutput = this.apBuffers[a][ch][apReadPos];
                                                  const apProcessed = clamp(apInput + apOutput * this.apFeedback, -1, 1);
                                                  this.apBuffers[a][ch][awp[a]] = apProcessed;
                                                  apInput = apOutput - apProcessed * this.apFeedback; // Allpass structure
                                                  awp[a] = (awp[a] + 1) % apLenSamples;
                                              }
                                              const wetSignal = apInput; // Final reverb output

                                              // Mix dry, delay, and reverb
                                              let finalSignal = drySignal * (1.0 - this.delayMix - this.reverbMix)
                                                              + delayedSample * this.delayMix
                                                              + wetSignal * this.reverbMix;

                                              // Apply master gain and final soft clip
                                              finalSignal *= currentMasterLevel;
                                              // <<< CHANGE: Boosted final gain before output clip SIGNIFICANTLY
                                              outCh[i] = softClip(finalSignal * 2.8);

                                              // Increment delay write pointer
                                              dwp = (dwp + 1) % delayB.length;
                                          } // End sample loop

                                          // Store updated write pointers and last samples for next block
                                          this.delayWritePos[ch]=dwp;
                                          this.combWritePos.forEach((_,c)=>this.combWritePos[c][ch]=cwp[c]);
                                          this.apWritePos.forEach((_,a)=>this.apWritePos[a][ch]=awp[a]);
                                          this.combLastSample.forEach((_,c)=>this.combLastSample[c][ch]=cls[c]);

                                      } // End channel loop

                                      this.phase += bufferSize; // Update master phase
                                      return true; // Indicate processor should continue
                                  } // End process
                              }
                              registerProcessor('generative-processor', GenerativeProcessor);
                          `;
                          const blob = new Blob([processorCode], { type: 'application/javascript' });
                          const workletURL = URL.createObjectURL(blob);
                          await audioContext.audioWorklet.addModule(workletURL);
                          console.log("AudioWorklet module added.");

                          this.audioWorkletNode = new AudioWorkletNode(audioContext, 'generative-processor', {
                              outputChannelCount:[2], // Stereo output
                              parameterData:{ masterLevel: 1.0 } // Initial master level
                          });
                          this.audioWorkletNode.connect(masterGain);
                          console.log("AudioWorklet Node connected.");
                          URL.revokeObjectURL(workletURL); // Clean up blob URL
                          this.isInitialized = true;
                          this.hideWarning(); // Hide initial interaction warning
                          await this.setupMicrophone(); // Try setting up mic now that worklet is ready

                     } catch (err) {
                         console.error("!!! AudioWorklet Initialization Failed:", err);
                         if(err.message && err.message.includes("SyntaxError")) {
                             showError("Audio Error: Engine syntax error. Check console.");
                         } else {
                              showError("Audio Error: Worklet setup failed.");
                         }
                         this.isInitialized=false;
                         // Clean up potentially broken nodes
                         this.audioWorkletNode?.disconnect(); this.audioWorkletNode=null;
                         this.micStreamSource?.disconnect(); this.micStreamSource=null;
                         currentInputState.mic.available=false;
                     }
                 } else if (audioContext.state === 'running' && this.isInitialized) {
                     // If context was resumed and already initialized, just ensure mic is set up
                     await this.setupMicrophone();
                 }
                 this.isInitializing = false; // Mark initialization attempt as complete
             }

             update(stateVectorTensor, complexityValue) {
                 if (!this.isInitialized || !this.audioWorkletNode || !stateVectorTensor || stateVectorTensor.isDisposed) return;

                 // Send state vector and complexity to the worklet
                 if (this.audioWorkletNode.port) {
                     stateVectorTensor.data()
                         .then(stateVectorArray => {
                             if(stateVectorArray.length === STATE_VECTOR_SIZE) {
                                 // Check port again in case it becomes invalid between async calls
                                 if (this.audioWorkletNode?.port) {
                                     this.audioWorkletNode.port.postMessage({ state: stateVectorArray, complexity: complexityValue });
                                 }
                             } else {
                                 console.warn(`State array size mismatch for worklet: got ${stateVectorArray.length}, expected ${STATE_VECTOR_SIZE}`);
                             }
                         })
                         .catch(e => console.error("Error getting tensor data for audio:", e));
                 }

                 // Update the masterLevel AudioParam based on the state vector
                 const levelParam = this.audioWorkletNode.parameters?.get('masterLevel');
                 if(levelParam && audioContext?.currentTime !== undefined) {
                     const masterVolIndex = featureExtractor?.indices?.masterVol; // Use feature extractor's index map
                     if (masterVolIndex !== undefined && masterVolIndex >= 0 && masterVolIndex < STATE_VECTOR_SIZE) {
                         stateVectorTensor.slice([0, masterVolIndex], [1, 1]).data()
                             .then(data => {
                                 // <<< CHANGE: Updated gain mapping -> Higher base, wider range, increased max
                                 const targetLevel = clamp( ( (data[0] !== undefined ? data[0] : 0.8) * 1.8 + 0.4 ) * 1.5 , 0.0, 3.0); // Maps state[0..1] to approx gain [0.6 .. 3.3] clamped to [0..3.0]
                                 // Use linearRamp for smoother transitions
                                 levelParam.linearRampToValueAtTime(targetLevel, audioContext.currentTime + 0.04); // Slightly faster ramp time
                             })
                             .catch(e => console.error("Error reading gain state for audio param:", e));
                     }
                 }
             }

             getMicrophoneInput() {
                 if(!this.isInitialized || !analyserNode || !currentInputState.mic.fft || audioContext?.state !=='running' || !currentInputState.mic.available){
                     currentInputState.mic.fft.fill(-140); // Reset FFT data if unavailable
                     return{level:0, fft:currentInputState.mic.fft, available:false, rhythmPeak:0, rhythmTempo:0};
                 }
                 analyserNode.getFloatFrequencyData(currentInputState.mic.fft); // Get current frequency data

                 // Calculate RMS level (simple volume)
                 let sumSquares = 0; let count = 0;
                 const len = analyserNode.frequencyBinCount;
                 for(let i=0; i<len; i++){
                     const dB = currentInputState.mic.fft[i];
                     if(isFinite(dB) && dB > -100){ // Ignore very low values
                         sumSquares += Math.pow(10, dB / 10); // Convert dB to power
                         count++;
                     } else {
                         currentInputState.mic.fft[i] = -140; // Floor invalid values
                     }
                 }
                 let rmsPower = count > 0 ? sumSquares / count : 0;
                 currentInputState.mic.level = clamp(Math.sqrt(rmsPower) * 11.0, 0, 1); // Convert power to amplitude-like level (boosted multiplier)

                 // Analyze rhythm
                 const micRhythm = analyzeRhythm(currentInputState.mic.fft, audioContext.sampleRate, MIC_FFT_SIZE);
                 currentInputState.mic.rhythmPeak = micRhythm.peak;
                 currentInputState.mic.rhythmTempo = micRhythm.tempo;

                 return {
                     level: currentInputState.mic.level,
                     fft: currentInputState.mic.fft,
                     available: true,
                     rhythmPeak: micRhythm.peak,
                     rhythmTempo: micRhythm.tempo
                 };
             }

             async setupMicrophone() {
                 // Only setup if initialized, context running, analyser exists, and not already setup
                 if(this.micStreamSource || !this.isInitialized || !audioContext || audioContext.state !== 'running' || !analyserNode){
                     return;
                 }
                 console.log("Requesting microphone access...");
                 try{
                     const stream = await navigator.mediaDevices.getUserMedia({
                         audio:{ // Try disabling processing for raw input
                             echoCancellation: false, noiseSuppression: false, autoGainControl: false
                         },
                         video:false
                     });
                     this.micStreamSource = audioContext.createMediaStreamSource(stream);
                     this.micStreamSource.connect(analyserNode); // Connect mic to analyser
                     console.log("Microphone connected to analyser.");
                     currentInputState.mic.available = true;
                     // Ensure FFT buffer size matches analyser node setting after connection
                     if(currentInputState.mic.fft.length !== analyserNode.frequencyBinCount){
                         currentInputState.mic.fft = new Float32Array(analyserNode.frequencyBinCount);
                         console.log("Mic FFT buffer resized to:", analyserNode.frequencyBinCount);
                     }
                     // Now that mic is likely granted, try initializing speech recognition again if needed
                     speechController?.requestPermissionAndInit();
                 } catch(err){
                     console.error("Microphone access denied or failed:", err);
                     showWarning("Mic Disabled/Denied.", 5000);
                     currentInputState.mic.available = false;
                     currentInputState.mic.fft.fill(-140); // Clear FFT data
                     // Still attempt speech init - it might use a different internal path or fail gracefully
                     speechController?.requestPermissionAndInit();
                 }
             }
        }

        // --- Input Handling ---
        // <<< CHANGE: Added Fullscreen Request Logic >>>
        function requestFullscreen() {
            const elem = document.documentElement; // Get the root element (html)
            if (elem.requestFullscreen) {
                elem.requestFullscreen().catch(err => console.warn(`Fullscreen request failed: ${err.message}`));
            } else if (elem.webkitRequestFullscreen) { /* Safari */
                elem.webkitRequestFullscreen().catch(err => console.warn(`Fullscreen request failed (webkit): ${err.message}`));
            } else if (elem.msRequestFullscreen) { /* IE11 */
                elem.msRequestFullscreen().catch(err => console.warn(`Fullscreen request failed (ms): ${err.message}`));
            } else {
                console.warn("Fullscreen API not supported on this browser.");
            }
        }

        function setupInputListeners() {
            const canvas = document.getElementById('renderCanvas');
            const resetButton = document.getElementById('resetButton'); // Hidden button for programmatic reset

            // --- Pointer (Mouse/Touch) Handlers ---
            const handlePointerMove = (e) => {
                const x = clamp(e.clientX / window.innerWidth, 0, 1);
                const y = 1.0 - clamp(e.clientY / window.innerHeight, 0, 1); // Invert Y
                currentInputState.touch.dx = x - currentInputState.touch.lastX;
                currentInputState.touch.dy = y - currentInputState.touch.lastY;
                currentInputState.touch.x = x;
                currentInputState.touch.y = y;
                currentInputState.touch.lastX = x;
                currentInputState.touch.lastY = y;
                // Use pressure if available (touch), default to 1 if active
                currentInputState.touch.pressure = (e.pressure !== undefined && e.pointerType === 'touch') ? e.pressure : (currentInputState.touch.active ? 1.0 : 0);
                if (currentInputState.touch.active) { e.preventDefault(); } // Prevent scrolling while dragging
            };

            const handlePointerDown = (e) => {
                e.preventDefault(); // Prevent default actions like text selection
                const now = performance.now();

                // --- One-time Initializations on first interaction ---
                if (!interactionOccurred) {
                    audioController?.tryInitializeAudio(); // Init AudioContext
                    requestMotionPermission(); // Request Device Orientation
                    requestAccelerometerPermission(); // Request Device Motion (Accelerometer)

                    // <<< CHANGE: Request fullscreen on first interaction >>>
                    const fullscreenAlreadyRequested = sessionStorage.getItem(FULLSCREEN_REQUESTED_KEY);
                    if (!fullscreenAlreadyRequested) {
                        console.log("Requesting fullscreen on first interaction.");
                        requestFullscreen();
                        sessionStorage.setItem(FULLSCREEN_REQUESTED_KEY, 'true'); // Mark as requested for this session
                    }
                    interactionOccurred = true; // Mark interaction happened *after* checks
                }


                // Check for Reset Gesture: Second tap after long press
                if (resetGestureState.longPressDetected && (now - resetGestureState.longPressReleaseTime < RESET_SECOND_TAP_WINDOW_MS)) {
                    console.log("Reset Gesture Confirmed!");
                    showWarning("Resetting State...", 1500);
                    resetButton.click(); // Trigger programmatic reset
                    // Clear gesture state immediately
                    if (resetGestureState.resetTimeout) clearTimeout(resetGestureState.resetTimeout);
                    resetGestureState.longPressDetected = false; resetGestureState.pointerDownTime = 0; resetGestureState.longPressReleaseTime = 0;
                    return; // Don't process as normal touch
                }

                // Normal touch start
                currentInputState.touch.active = true;
                currentInputState.touch.dx = 0; currentInputState.touch.dy = 0;
                currentInputState.touch.lastX = clamp(e.clientX / window.innerWidth, 0, 1);
                currentInputState.touch.lastY = 1.0 - clamp(e.clientY / window.innerHeight, 0, 1);
                currentInputState.touch.x = currentInputState.touch.lastX;
                currentInputState.touch.y = currentInputState.touch.lastY;
                handlePointerMove(e); // Update pressure etc.

                // Try starting speech recognition on interaction if permission granted but not running
                 if (speechController && speechController.permissionGranted && !speechController.isListening && !speechController.isActive && !speechController.isStarting) {
                    speechController.startListening();
                }

                // --- Reset Gesture: Start Long Press Timer ---
                resetGestureState.pointerDownTime = now;
                resetGestureState.longPressDetected = false;
                resetGestureState.longPressReleaseTime = 0;
                if (resetGestureState.resetTimeout) clearTimeout(resetGestureState.resetTimeout); // Clear any lingering timeout
            };

            const handlePointerUp = (e) => {
                e.preventDefault();
                const now = performance.now();

                if (currentInputState.touch.active) {
                    // --- Reset Gesture: Check for Long Press ---
                    const pressDuration = now - resetGestureState.pointerDownTime;
                    if (pressDuration > LONG_PRESS_DURATION_MS && resetGestureState.pointerDownTime > 0) {
                        console.log("Long press detected. Waiting for second tap...");
                        showWarning(`Long Press: Tap again within ${RESET_SECOND_TAP_WINDOW_MS}ms to Reset.`, RESET_SECOND_TAP_WINDOW_MS + 100);
                        resetGestureState.longPressDetected = true;
                        resetGestureState.longPressReleaseTime = now;
                        // Set a timeout to clear the long press state if no second tap occurs
                        if (resetGestureState.resetTimeout) clearTimeout(resetGestureState.resetTimeout);
                        resetGestureState.resetTimeout = setTimeout(() => {
                            if (resetGestureState.longPressDetected) {
                                console.log("Reset gesture second tap window expired.");
                                resetGestureState.longPressDetected = false; resetGestureState.longPressReleaseTime = 0;
                                hideWarning(); // Hide the "Tap again" message
                            }
                        }, RESET_SECOND_TAP_WINDOW_MS);
                    } else {
                        // If it wasn't a long press, clear any potential reset state
                        resetGestureState.longPressDetected = false; resetGestureState.longPressReleaseTime = 0;
                        if (resetGestureState.resetTimeout) clearTimeout(resetGestureState.resetTimeout);
                        // Hide warning if it was showing the "tap again" message
                        const warnDiv = document.getElementById('warningInfo');
                        if(warnDiv && warnDiv.style.display !== 'none' && warnDiv.textContent.includes("Tap again")) hideWarning();
                    }

                    // Normal touch end
                    currentInputState.touch.active = false;
                    currentInputState.touch.pressure = 0;
                    currentInputState.touch.dx = 0; currentInputState.touch.dy = 0;
                    resetGestureState.pointerDownTime = 0; // Reset timer start time
                }
            };

            // Add pointer listeners to the canvas
            canvas.addEventListener('pointerdown', handlePointerDown, { passive: false });
            canvas.addEventListener('pointerup', handlePointerUp, { passive: false });
            canvas.addEventListener('pointerleave', handlePointerUp, { passive: false }); // Treat leaving canvas as pointer up
            canvas.addEventListener('pointermove', handlePointerMove, { passive: false });

            // Reset Button Listener (for programmatic trigger)
             resetButton.addEventListener('click', () => {
                 console.log("Resetting state via button click (programmatic)...");
                 speechController?.stopListening(); // Stop speech rec if active
                 try {
                     localStorage.removeItem(LOCAL_STORAGE_KEY);
                     sessionStorage.removeItem(FULLSCREEN_REQUESTED_KEY); // Also clear fullscreen request flag
                     console.log(`Cleared localStorage for key: ${LOCAL_STORAGE_KEY}`);
                     showWarning("State Cleared. Reloading...", 1500);
                     // Delay reload slightly to allow message to show
                     setTimeout(() => { window.location.reload(); }, 500);
                 } catch (e) {
                     console.error("Error clearing storage:", e);
                     showError("Failed to clear state.");
                 }
             });

            // --- Motion Sensor Permissions and Listeners ---
            let motionListenerAdded = false;
            const requestMotionPermission = () => {
                if(motionListenerAdded) return; // Don't add multiple listeners
                const addListener = () => {
                    window.addEventListener('deviceorientation', handleOrientation, true); // Use capture phase
                    motionListenerAdded = true;
                    console.log("DeviceOrientation listener added.");
                };
                // iOS 13+ requires permission request
                if (typeof DeviceOrientationEvent !== 'undefined' && typeof DeviceOrientationEvent.requestPermission === 'function') {
                    DeviceOrientationEvent.requestPermission()
                        .then(state => {
                            if (state === 'granted') { addListener(); }
                            else { console.warn("DeviceOrientation permission denied."); currentInputState.motion.available = false; }
                        }).catch(e => {
                            console.error("DeviceOrientation permission request error:", e); currentInputState.motion.available = false;
                        });
                } else {
                    // Non-iOS or older versions might not need explicit permission
                    addListener();
                }
            };
            const handleOrientation = (e) => {
                 // Check if data is actually available (can be null)
                 if (e.alpha !== null || e.beta !== null || e.gamma !== null) {
                     currentInputState.motion.alpha = e.alpha || 0; // Z axis rotation (compass)
                     currentInputState.motion.beta = e.beta || 0;  // X axis rotation (front/back tilt)
                     currentInputState.motion.gamma = e.gamma || 0; // Y axis rotation (left/right tilt)
                     if (!currentInputState.motion.available) {
                         console.log("DeviceOrientation data received.");
                         currentInputState.motion.available = true;
                     }
                 }
            };

            // --- Accelerometer Permissions and Listeners ---
            let accelListenerAdded = false;
            const requestAccelerometerPermission = () => {
                 if (accelListenerAdded) return;
                 const addListener = () => {
                     window.addEventListener('devicemotion', handleMotion, true); // Use capture phase
                     accelListenerAdded = true;
                     console.log("DeviceMotion (Accelerometer) listener added.");
                 };
                 // iOS 13+ requires permission request
                 if (typeof DeviceMotionEvent !== 'undefined' && typeof DeviceMotionEvent.requestPermission === 'function') {
                     DeviceMotionEvent.requestPermission()
                         .then(state => {
                             if (state === 'granted') { addListener(); }
                             else { console.warn("DeviceMotion permission denied."); currentInputState.accelerometer.available = false; }
                         }).catch(e => {
                             console.error("DeviceMotion permission request error:", e); currentInputState.accelerometer.available = false;
                         });
                 } else {
                     addListener();
                 }
            };
             const handleMotion = (event) => {
                 const acc = event.accelerationIncludingGravity; // Use gravity-inclusive data
                 if (acc && (acc.x != null || acc.y != null || acc.z != null)) {
                     currentInputState.accelerometer.x = acc.x || 0;
                     currentInputState.accelerometer.y = acc.y || 0;
                     currentInputState.accelerometer.z = acc.z || 0;
                     // Calculate magnitude of the acceleration vector
                     const magnitude = Math.sqrt((acc.x || 0)**2 + (acc.y || 0)**2 + (acc.z || 0)**2);
                     currentInputState.accelerometer.magnitude = magnitude;

                     if (!currentInputState.accelerometer.available) {
                         console.log("Accelerometer data received.");
                         currentInputState.accelerometer.available = true;
                         // Initialize history buffer when data first becomes available
                         currentInputState.accelerometer.history = new Array(ACCEL_FFT_SIZE).fill(magnitude);
                     }
                     // Update history buffer (for rhythm analysis)
                     const accelHistory = currentInputState.accelerometer.history;
                     accelHistory.shift(); // Remove oldest value
                     accelHistory.push(magnitude); // Add newest value
                 }
             };
        }

        // --- Speech Command Handling ---
        // <<< CHANGE: Added speech command handler >>>
        function handleSpeechCommand(command) {
            console.log("Executing Speech Command:", command);
            let success = false;
            switch (command) {
                case 'CREATE':
                    if (artifactManager && currentResonantState && !currentResonantState.isDisposed) {
                        artifactManager.createArtifact(currentResonantState)
                            .then(created => { if (created) console.log("Artifact creation triggered by voice."); else console.warn("Voice artifact creation failed."); })
                            .catch(e => console.error("Error creating artifact via voice:", e));
                    } else { console.warn("Cannot create artifact via voice now (system not ready?)."); }
                    break;
                case 'FORGET_OLDEST':
                    success = artifactManager?.forgetOldestArtifact() ?? false;
                    if (success) console.log("Forget oldest artifact triggered by voice."); else console.log("Forget command failed (no artifacts?).");
                    break;
                case 'RESET':
                    document.getElementById('resetButton').click(); // Trigger programmatic reset
                    break;
                default:
                    console.warn("Unknown speech command received by handler:", command);
            }
        }

        // --- Visual Feedback Trigger ---
        function triggerVisualFeedback(intensity = 0.5, duration = 0.1) {
            visualFeedback.intensity = Math.max(visualFeedback.intensity, clamp(intensity, 0, 1)); // Take max intensity if triggered rapidly
            visualFeedback.startTime = performance.now() / 1000.0; // Use current time
            visualFeedback.duration = duration;
            visualFeedback.active = true;
        }

        // --- Main Game Loop ---
        let lastTimestamp = 0;
        // <<< CHANGE: Added declaration for currentStateArray outside try/catch
        let currentStateArray = null;
        async function gameLoop(timestamp) {
             requestAnimationFrame(gameLoop); // Schedule next frame immediately
             const currentTime = timestamp / 1000.0; // Convert to seconds
             currentInputState.currentTime = currentTime; // Update global time state

             // <<< CHANGE: Wait until user interacts OR state loading has completed (or failed) >>>
             if (!interactionOccurred && !stateLoadAttempted) {
                 const warnDiv = document.getElementById('warningInfo');
                 if (warnDiv && warnDiv.textContent.includes("Interact to initialize")) { /* Keep showing initial msg */ }
                 return; // Don't run game logic yet
             }
             if (stateLoadAttempted && !stateLoadSucceeded && embeddingsReady) {
                  if(performance.now() % 2000 < 20) console.log("Waiting for state load completion..."); // Reduce log spam
                  return; // Wait for async state load (if embeddings haven't failed)
             }

             // Ensure core components are ready (more robust checks)
             if (!tf.ready() || !audioController?.isInitialized || !graphicsController?.material || !renderer || !currentResonantState || currentResonantState.isDisposed || !unifiedIntentVector || unifiedIntentVector.isDisposed) {
                 if(performance.now() % 5000 < 20) console.warn("Waiting for core components (TF/Audio/Graphics/Tensors)..."); // Reduce log spam
                 return;
             }
             // <<< CHANGE: Double check renderer context isn't lost before proceeding
             if (renderer.getContext().isContextLost()) {
                  if(performance.now() % 5000 < 20) console.error("WebGL Context Lost! Loop paused.");
                  return;
             }

             await tf.ready(); // Double check TF readiness

             // Calculate delta time, handling potential pauses/resumes
             const deltaTime = Math.max(0.001, Math.min(0.1, (timestamp - lastTimestamp) / 1000.0 || (1.0 / TARGET_FPS)));
             lastTimestamp = timestamp;

             // <<< CHANGE: Performance Monitoring & Dynamic Complexity >>>
             frameCount++; const fpsElapsed = (timestamp - lastFpsTime) / 1000.0;
             if (fpsElapsed >= 1.0) { // Update FPS and complexity level once per second
                 currentFPS = frameCount / fpsElapsed;
                 lastFpsTime = timestamp; frameCount = 0;
                 // Adjust complexity based on FPS deviation from target
                 const error = TARGET_FPS - currentFPS;
                 const baseAdjustment = 0.025; // How much to adjust per second at moderate error
                 // Use tanh to scale adjustment based on error size (more sensitive to small errors, capped effect for large errors)
                 const errorFactor = Math.tanh(error / (TARGET_FPS * 0.2)) * 1.5;
                 // Increased adjustment range slightly
                 const adjustment = clamp(errorFactor * baseAdjustment, -0.10, 0.10);
                 complexityLevel = clamp(complexityLevel + adjustment, 0.05, 1.0); // Apply adjustment within bounds [0.05, 1.0]
             }

             // --- Input Update & Analysis ---
             // Accelerometer Rhythm Analysis (at intervals)
             if (currentInputState.accelerometer.available && (currentTime - lastAccelTime >= ACCEL_ANALYSIS_INTERVAL_S)) {
                 lastAccelTime = currentTime;
                 const analysisFreq = 1.0 / ACCEL_ANALYSIS_INTERVAL_S; // Effective sampling rate for analysis
                 const accelRhythm = analyzeRhythm(currentInputState.accelerometer.history, analysisFreq, ACCEL_FFT_SIZE);
                 currentInputState.accelerometer.rhythmPeak = accelRhythm.peak;
                 currentInputState.accelerometer.rhythmTempo = accelRhythm.tempo;
             }
             // Microphone Input Update (every frame)
             currentInputState.mic = audioController?.getMicrophoneInput() ?? currentInputState.mic;

             // Sync Factor Calculation (Ambient vs User Rhythm)
             let potentialSync = 0;
             if (currentInputState.mic.available && currentInputState.accelerometer.available) {
                 const micPeakNorm = currentInputState.mic.rhythmPeak;
                 const accelPeakNorm = currentInputState.accelerometer.rhythmPeak;
                 const tempoDiff = Math.abs(currentInputState.mic.rhythmTempo - currentInputState.accelerometer.rhythmTempo);
                 // Similarity based on tempo difference (1 if same, 0 if >80BPM diff)
                 const tempoSimilarity = Math.max(0, 1.0 - tempoDiff / 80.0); // Wider tolerance for tempo difference
                 // Sync requires both to be rhythmic and tempos to align
                 potentialSync = micPeakNorm * accelPeakNorm * tempoSimilarity * 2.0; // Boosted scale factor
             }
             // Update sync factor smoothly (exponential moving average)
             currentInputState.syncFactor = clamp(currentInputState.syncFactor * SYNC_DECAY + potentialSync * (1.0 - SYNC_DECAY), 0.0, 1.0);

             // --- Artifact Creation Logic ---
             const nowMs = Date.now();
             if (embeddingsReady && artifactManager && (nowMs - lastArtifactCreationTime > ARTIFACT_CREATION_INTERVAL_MS)) {
                 lastArtifactCreationTime = nowMs; // Update time even if not created
                 // <<< CHANGE: Ensure stateDataForStdDev is retrieved before using it >>>
                 try {
                     const stateDataForStdDev = await currentResonantState.data(); // Get current state data
                     // Calculate standard deviation of state vector as a measure of state "activity" or "uniqueness"
                     const mean = stateDataForStdDev.reduce((a, b) => a + b, 0) / stateDataForStdDev.length;
                     const variance = stateDataForStdDev.reduce((a, b) => a + (b - mean) ** 2, 0) / stateDataForStdDev.length;
                     const stateStdDev = Math.sqrt(variance);
                     // Combine mic/accel rhythm peaks and state deviation for overall activity level
                     const activityLevel = (currentInputState.mic.rhythmPeak * 0.4 + currentInputState.accelerometer.rhythmPeak * 0.4 + stateStdDev * 2.8); // Increased std dev influence
                     // Boost thresholds slightly based on sync factor (more likely to capture when synced)
                     const syncBoost = 1.0 + (currentInputState.syncFactor - 0.5) * 0.5; // Stronger sync boost
                     const minThresh = ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MIN * syncBoost;
                     const maxThresh = ARTIFACT_CREATION_ACTIVITY_THRESHOLD_MAX * syncBoost;

                     // Create artifact if activity is within range and space available
                     if (activityLevel > minThresh && activityLevel < maxThresh && artifactManager.getArtifactCount() < MAX_ARTIFACTS) {
                         console.log(`Attempting artifact creation: Activity=${activityLevel.toFixed(3)}, Sync=${currentInputState.syncFactor.toFixed(2)} (Thresh: ${minThresh.toFixed(2)}-${maxThresh.toFixed(2)})`);
                         artifactManager.createArtifact(currentResonantState)
                             .then(created => {
                                 if (created && currentInputState.syncFactor > SYNC_THRESHOLD) {
                                     console.log("Artifact created during high ambient sync!");
                                     triggerVisualFeedback(0.9, 0.35); // Stronger feedback for synced creation
                                 } else if (created) {
                                     console.log("Artifact created (low/no sync).");
                                     triggerVisualFeedback(0.6, 0.2);
                                 }
                             })
                             .catch(e => console.error("Error during async artifact creation:", e));
                     } else if (artifactManager.getArtifactCount() >= MAX_ARTIFACTS) {
                          if(performance.now() % 10000 < 20) console.log(`Artifact creation skipped: Max artifacts reached (${MAX_ARTIFACTS}).`); // Reduce log spam
                     } else {
                          if(performance.now() % 10000 < 20) console.log(`Artifact creation skipped: Activity=${activityLevel.toFixed(3)} (Thresh: ${minThresh.toFixed(2)}-${maxThresh.toFixed(2)})`); // Reduce log spam
                     }
                 } catch (dataError) {
                     console.error("Error getting state data for artifact creation analysis:", dataError);
                 }
             }

             // Find relevant artifacts for current state
              if (artifactManager && currentResonantState && !currentResonantState.isDisposed) {
                  activeArtifactInfo = await artifactManager.findRelevantArtifacts(
                      currentResonantState,
                      ARTIFACT_SIMILARITY_THRESHOLD,
                      MAX_ACTIVE_ARTIFACTS_LOGIC // Use logic limit here
                  ).catch(e => {
                      console.error("Error finding relevant artifacts:", e);
                      return { ids: [], stateArrays: [], similarities: [] }; // Return empty on error
                  });
              } else {
                  activeArtifactInfo = { ids: [], stateArrays: [], similarities: [] }; // Reset if manager/state invalid
              }


             // --- Core NN Prediction Step ---
             let nextStateTensor = null; // Renamed from local scope variable in v0.5
             let nextIntentTensor = null; // Renamed from local scope variable in v0.5
             memoryInfo = tf.memory(); // Get memory info *before* tidy, as a baseline
             try {
                 // Wrap NN processing in tf.tidy to manage memory
                 const result = tf.tidy(() => {
                     // 1. Process raw inputs into unified intent vector
                     // <<< CHANGE: Pass currentTime to process method
                     const intentTensor = inputProcessorModel.process(currentInputState, currentTime);
                     // 2. Predict next state based on intent, current state, and artifacts
                     const stateTensor = coreLogicModel.predict(
                         intentTensor,
                         currentResonantState,
                         activeArtifactInfo.stateArrays, // Pass arrays from logic step
                         activeArtifactInfo.similarities
                     );
                     // 3. Get memory info within the tidy scope (more accurate for allocation)
                     const currentMemInfo = tf.memory();
                     // Return tensors to keep and memory info
                     return { newState: stateTensor, newIntent: intentTensor, memory: currentMemInfo };
                 });

                 // Dispose old tensors *outside* tidy, before reassigning
                 tf.dispose(currentResonantState);
                 tf.dispose(unifiedIntentVector);
                 // Keep the newly created tensors
                 currentResonantState = tf.keep(result.newState);
                 unifiedIntentVector = tf.keep(result.newIntent);
                 memoryInfo = result.memory; // Update global memory info with the accurate post-tidy value

             } catch (e) {
                 console.error("!!! Game Loop TF Tidy Error:", e);
                 memoryInfo = tf.memory(); // Get memory info even on error

                 // Attempt recovery if tensors became invalid
                 if (!currentResonantState || currentResonantState.isDisposed) {
                     console.error("CRITICAL: currentResonantState invalid post-tidy! Resetting.");
                     tf.dispose(currentResonantState); // Ensure disposal if somehow exists but disposed
                     currentResonantState = tf.keep(tf.fill([1, STATE_VECTOR_SIZE], 0.5)); // Reset to default
                 }
                 if (!unifiedIntentVector || unifiedIntentVector.isDisposed) {
                     console.error("CRITICAL: unifiedIntentVector invalid post-tidy! Resetting.");
                     tf.dispose(unifiedIntentVector);
                     unifiedIntentVector = tf.keep(tf.zeros([1, INPUT_VECTOR_SIZE])); // Reset to default
                 }
             }

             // --- Get State Data for Audio/Visuals ---
              // <<< CHANGE: Ensure currentStateArray is assigned correctly after NN step/error >>>
              if (currentResonantState && !currentResonantState.isDisposed) {
                  try {
                      currentStateArray = await currentResonantState.data(); // Async data retrieval
                  } catch (dataError) {
                      console.error("Error getting state tensor data after tidy:", dataError);
                      currentStateArray = currentStateArray || new Array(STATE_VECTOR_SIZE).fill(0.5); // Use previous or fallback
                  }
              } else {
                  currentStateArray = currentStateArray || new Array(STATE_VECTOR_SIZE).fill(0.5); // Use previous or fallback if tensor is invalid
                  console.warn("Using fallback/previous state array post-tidy.");
              }


             // --- Visual Feedback Update ---
             let currentFeedbackIntensity = 0;
             if (visualFeedback.active) {
                 const elapsed = currentTime - visualFeedback.startTime;
                 if (elapsed < visualFeedback.duration) {
                     const progress = elapsed / visualFeedback.duration;
                     currentFeedbackIntensity = visualFeedback.intensity * (1.0 - progress) * (1.0 - progress); // Fade out quadratically
                 } else {
                     visualFeedback.active = false; visualFeedback.intensity = 0; // Deactivate
                 }
             }

             // --- Update Graphics and Audio ---
             // Pass artifact info relevant for *shader* limit
             const shaderArtifactInfo = {
                 ids: activeArtifactInfo.ids.slice(0, MAX_ARTIFACTS_SHADER),
                 stateArrays: activeArtifactInfo.stateArrays.slice(0, MAX_ARTIFACTS_SHADER),
                 similarities: activeArtifactInfo.similarities.slice(0, MAX_ARTIFACTS_SHADER)
             };
             // <<< CHANGE: Use the awaited currentStateArray
             if (currentStateArray) {
                graphicsController?.update(currentStateArray, currentTime, shaderArtifactInfo, complexityLevel, currentInputState.syncFactor, currentFeedbackIntensity);
             }

             if (currentResonantState && !currentResonantState.isDisposed) {
                 audioController?.update(currentResonantState, complexityLevel);
             }

             // --- Debug Info Update ---
             if (USE_DEBUG && document.getElementById('debugInfo')) {
                 const d = document.getElementById('debugInfo');
                 const { numBytes, numTensors } = memoryInfo || { numBytes: 0, numTensors: 0 }; // Use fallback if memoryInfo is null
                 const artCnt = artifactManager?.getArtifactCount() ?? 0;
                 const logicArtIds = activeArtifactInfo.ids.join(',') || 'n'; // IDs influencing logic
                 const shArtIds = shaderArtifactInfo.ids.join(',') || 'n'; // IDs influencing shader
                 const mtn = currentInputState.motion.available ? `${currentInputState.motion.beta.toFixed(0)},${currentInputState.motion.gamma.toFixed(0)}`:'N';
                 const mic = currentInputState.mic.available ? `${currentInputState.mic.level.toFixed(2)}[${currentInputState.mic.rhythmPeak.toFixed(1)},${currentInputState.mic.rhythmTempo.toFixed(0)}]`:'N';
                 const acc = currentInputState.accelerometer.available ? `${currentInputState.accelerometer.magnitude.toFixed(1)}[${currentInputState.accelerometer.rhythmPeak.toFixed(1)},${currentInputState.accelerometer.rhythmTempo.toFixed(0)}]`:'N';
                 const be = tf.getBackend()||'N';
                 // <<< CHANGE: Get speech status string dynamically
                 const speechStat = speechController ? speechController.updateStatus() : 'N/A';
                 d.textContent = `V${VERSION}|FPS:${currentFPS.toFixed(1)}|Cmplx:${complexityLevel.toFixed(2)}|Sync:${currentInputState.syncFactor.toFixed(2)}|Ctx:${audioContext?.state??'N'}|Tch:${currentInputState.touch.active?'A':'I'}|Mtn:${mtn}|Acc:${acc}|Mic:${mic}|Speech:${speechStat}|Art:${artCnt}(L:${logicArtIds}/${MAX_ACTIVE_ARTIFACTS_LOGIC}|S:${shArtIds}/${MAX_ARTIFACTS_SHADER})|Emb:${embeddingsReady?'OK':'No'}|TF[${be}]:${numTensors}t/${(numBytes/1e6).toFixed(1)}MB`;
             }
        }

        // --- Persistence ---
        function saveStateToLocalStorage() {
             // <<< CHANGE: Added check for embeddingsReady before saving artifacts
             if (!interactionOccurred || !currentResonantState || currentResonantState.isDisposed || !artifactManager || !embeddingsReady) {
                  if (performance.now() % 15000 < 20) console.log("Save state skipped (conditions not met: interaction/state/artifacts/embeddings)."); // Less spammy log
                 return;
             }
             console.log("Saving state to localStorage...");
             try {
                 // Use .data() which is async and returns a promise
                 currentResonantState.data().then(stateArray => {
                     // Prepare artifacts for JSON serialization
                     const serializableArts = artifactManager.artifacts.map(art => ({
                         id: art.id,
                         stateVector: Array.from(art.stateVector), // Ensure array format
                         featureTags: art.featureTags,
                         embedding: Array.from(art.embedding), // Ensure array format
                         timestamp: art.timestamp
                     }));

                     const stateToSave = {
                         resonantState: Array.from(stateArray), // Ensure array format
                         artifacts: serializableArts,
                         timestamp: Date.now(),
                         version: VERSION // Store version for compatibility checks
                     };
                     const stateJSON = JSON.stringify(stateToSave);
                     localStorage.setItem(LOCAL_STORAGE_KEY, stateJSON);
                     console.log(`State (${(stateJSON.length / 1024).toFixed(1)} KB) saved.`);

                 }).catch(e => {
                     console.error("Error getting tensor data for saving state:", e);
                     showWarning("Failed to get data for save.", 3000);
                 });

             } catch (e) {
                 console.error("Save state error:", e);
                 // Handle specific errors like quota exceeded
                 if (e.name === 'QuotaExceededError') {
                     showError("Save Failed: Storage full!");
                     // Attempt to prune oldest artifact if storage is full
                     if(artifactManager?.getArtifactCount() > 0) {
                         console.warn("Attempting prune oldest artifact due to QuotaExceededError...");
                         artifactManager.forgetOldestArtifact();
                     }
                 } else {
                     showWarning("Could not save state.", 3000);
                 }
             }
         }

        // <<< CHANGE: Reworked load state logic for robustness >>>
        function loadStateFromLocalStorage() {
             stateLoadAttempted = true; // Mark that we are trying to load
             stateLoadSucceeded = false; // Assume failure until success
             let stateJSON;
             try {
                 stateJSON = localStorage.getItem(LOCAL_STORAGE_KEY);
                 if (!stateJSON) {
                     console.log(`No saved state found for key '${LOCAL_STORAGE_KEY}'.`);
                     stateLoadSucceeded = true; // Mark as 'succeeded' in the sense that there's nothing to load
                     return false; // No state found, return false
                 }

                 console.log(`Loading state for key '${LOCAL_STORAGE_KEY}'...`);
                 const savedState = JSON.parse(stateJSON); // Parse might fail here

                 // --- Validation Checks ---
                 if (!savedState || typeof savedState !== 'object') throw new Error("Invalid saved state format (not an object).");
                 if (savedState.version !== VERSION) { console.warn(`Saved state version mismatch (Need v${VERSION}, got v${savedState.version}). Clearing.`); throw new Error("Version mismatch."); }
                 if (!Array.isArray(savedState.resonantState) || savedState.resonantState.length !== STATE_VECTOR_SIZE) throw new Error("Invalid resonantState format or size.");
                 if (!Array.isArray(savedState.artifacts)) throw new Error("Invalid artifacts format.");

                 // --- Restore State ---
                 tf.tidy(() => {
                      // Validate numeric values and clamp before creating tensor
                     const validatedStateArray = savedState.resonantState.map(v => typeof v === 'number' && isFinite(v) ? clamp(v, 0, 1) : 0.5);
                     const loadedTensor = tf.tensor1d(validatedStateArray).expandDims(0);
                     tf.dispose(currentResonantState); // Dispose existing tensor first
                     currentResonantState = loadedTensor; // Assign new tensor
                 });
                 tf.keep(currentResonantState); // Keep the restored tensor
                 console.log("Restored resonant state tensor.");

                 // Prepare artifacts for loading (validation happens in setArtifacts)
                 const artifactsToLoad = savedState.artifacts;

                 // Defer setting artifacts until manager and embeddings are ready
                 let loadCheckRetries = 0;
                 const MAX_LOAD_RETRIES = 50; // Wait max ~10 seconds
                 const checkAndSetArtifacts = () => {
                     if (artifactManager && embeddingsReady) {
                         artifactManager.setArtifacts(artifactsToLoad); // This now contains validation
                         showWarning("Loaded previous state.", 3000);
                         console.log(`Artifacts loaded from save point: ${new Date(savedState.timestamp).toLocaleString()}`);
                         stateLoadSucceeded = true; // Mark load as complete
                     } else if (loadCheckRetries < MAX_LOAD_RETRIES) {
                         loadCheckRetries++;
                         // Retry if components not ready yet
                         setTimeout(checkAndSetArtifacts, 200);
                     } else {
                         console.error("Timeout waiting for artifact manager/embeddings during state load. Artifacts not loaded.");
                         showError("Load Failed: Components timeout. Artifacts lost.");
                         stateLoadSucceeded = true; // Mark as 'complete' even though artifacts failed, to unblock loop
                     }
                 };
                 setTimeout(checkAndSetArtifacts, 100); // Start check shortly after load attempt

                 interactionOccurred = true; // Mark as interacted since state was loaded

                 return true; // Return true to indicate loading process initiated

             } catch (e) {
                 console.error("Load state error:", e);
                 showError("Failed to load state. Starting fresh.");
                 if (e instanceof SyntaxError) { console.error("-> Likely corrupted JSON in localStorage:", stateJSON?.substring(0, 100)); }
                 try { localStorage.removeItem(LOCAL_STORAGE_KEY); } // Attempt to remove corrupted item
                 catch (removeError) { console.error("Error removing corrupted state:", removeError); }
                 stateLoadSucceeded = true; // Mark as 'complete' (failed load) to unblock loop
                 return false; // Signal load failure
             }
         }


        // --- Initialization ---
        async function initialize() {
            console.log(`Initializing Infundibulum Echoes v${VERSION}`);
            const warningDiv = document.getElementById('warningInfo');
            if (USE_DEBUG) document.getElementById('debugInfo').style.display = 'block';

            // 1. Graphics Setup (must happen first for context)
            try {
                 graphicsController = new GraphicsController(document.getElementById('renderCanvas'));
            } catch(e) {
                showError("Fatal: Graphics Controller Init Failed. Check console.");
                console.error("Graphics Controller critical initialization failure:", e);
                return; // Stop initialization if graphics fail
            }

            // 2. Audio Setup
            audioController = new AudioController(); // Instantiate, but full init needs interaction

            // 3. TensorFlow.js Setup
            try {
                await tf.ready();
                // <<< CHANGE: Always prefer webgl, no mode check
                const targetBackend = 'webgl';
                let currentBackend = tf.getBackend();
                if (currentBackend !== targetBackend) {
                     console.log(`Attempting to set TF backend to ${targetBackend}...`);
                     await tf.setBackend(targetBackend).catch(async (be) => {
                         console.warn(`Failed setting preferred backend ${targetBackend}, current: ${tf.getBackend()}. Error:`, be);
                         // Don't force another backend if webgl fails, let TF use its default
                     });
                 }
                currentBackend = tf.getBackend(); // Get final backend
                console.log(`TF Ready. Backend: ${currentBackend}`);
                if (currentBackend === 'webgl') {
                    tf.env().set('WEBGL_CONV_IM2COL', false);
                    tf.env().set('WEBGL_PACK', false);
                    // <<< CHANGE: Try enabling high performance webgl settings explicitly >>>
                    tf.env().set('WEBGL_USE_SHAPES_UNIFORMS', true);
                    tf.env().set('WEBGL_FORCE_F16_TEXTURES', true);
                } else {
                     console.warn(`Running on non-WebGL TF Backend: ${currentBackend}. Performance may vary.`);
                 }
                 console.log("TF backend configured.");
                 console.log("TF Memory (Initial):", tf.memory()); // Log initial memory
            } catch (err) {
                showError("Fatal: TF.js Backend Init Failed. Check console.");
                console.error("TensorFlow.js setup critical failure:", err);
                return; // Stop initialization if TF fails
            }

            // 4. Instantiate Core Logic Components
            inputProcessorModel = new PlaceholderInputProcessor(null, INPUT_VECTOR_SIZE);
            coreLogicModel = new PlaceholderCoreLogic(STATE_VECTOR_SIZE);
            featureExtractor = new FeatureExtractor(STATE_VECTOR_SIZE);
            embeddingProvider = new EmbeddingProvider(EMBEDDING_MODEL_NAME);
            artifactManager = new ArtifactManager(MAX_ARTIFACTS, STATE_VECTOR_SIZE, EMBEDDING_DIM, featureExtractor, embeddingProvider);
            // <<< CHANGE: Initialize Speech Controller
            speechController = new SpeechRecognitionController();
            speechController.setCommandCallback(handleSpeechCommand);
            console.log("Logic/RAG/Speech components instantiated.");

            // 5. Load State or Initialize Defaults
            // <<< CHANGE: Improved handling of load result and state flags
            const loadSuccess = loadStateFromLocalStorage(); // Attempt to load saved state
            if (!loadSuccess && !stateLoadSucceeded) { // If loading wasn't attempted or explicitly failed
                console.log("Initialized with default state (no valid save found or load failed).");
                warningDiv.textContent = `Ready (v${VERSION}). Interact or Speak. Long Press + Tap to Reset.`;
                 // Ensure default tensors are kept if not loaded
                 // Note: tf.keep is done when the tensors are initially created
                stateLoadSucceeded = true; // Mark load process as 'complete' even if nothing was loaded
            } else if (loadSuccess) {
                console.log("State load initiated, waiting for completion...");
                warningDiv.textContent = `Loading State (v${VERSION})...`; // Show loading message
            }
            // If loadStateFromLocalStorage returns false BUT stateLoadSucceeded is true, it means no state file was found - which is fine.

            // 6. Background Initialization (Embeddings) - Start immediately
             embeddingProvider.init().catch(e => console.error("Background Embedding Model init error:", e));

            // 7. Setup Input Listeners
            setupInputListeners();

            // 8. Setup Resize Listener
            window.addEventListener('resize', () => { graphicsController?.resize(); }, { passive: true });

            // 9. Setup Visibility Change / Page Hide Listeners
            document.addEventListener('visibilitychange', () => {
                if (document.hidden) {
                    saveStateToLocalStorage(); // Save state when tab becomes hidden
                    speechController?.stopListening(); // Stop listening when hidden
                } else {
                    // Try resuming audio context if needed
                     audioController?.tryInitializeAudio();
                    // Try starting speech again when visible if permission was granted
                    if(speechController?.permissionGranted && !speechController.isListening && !speechController.isActive) {
                         speechController.startListening();
                     }
                }
            });
            // Also save state when page is unloaded/hidden completely
            window.addEventListener('pagehide', saveStateToLocalStorage);


            // Final check for renderer validity before starting loop
            if (!renderer) {
                showError("Initialization Error: Renderer became invalid.");
                return;
            }

            console.log("Initialization complete. Starting game loop (will wait for interaction or state load)...");
            lastTimestamp = performance.now(); // Initialize timestamps for loop
            lastFpsTime = lastTimestamp;
            lastAccelTime = lastTimestamp / 1000.0; // Initialize accel analysis time
            requestAnimationFrame(gameLoop); // Start the main loop
        }

        // --- Utilities ---
        function showError(msg) {
            const w=document.getElementById('warningInfo');
            if(w){
                w.textContent=`FATAL: ${msg}`; w.style.color='red'; w.style.display='block';
                // Clear any existing warning timeout
                if(audioController?.warningTimeout) clearTimeout(audioController.warningTimeout);
                if (audioController) audioController.audioWarningDisplayed = true; // Mark warning as displayed
            }
            console.error(`FATAL: ${msg}`);
        }
        // Use audio controller's warning methods for consistency
        function showWarning(msg, dur=5000) { audioController?.showWarning(msg, dur); }
        function hideWarning() { audioController?.hideWarning(); }
        function showLoading(show, msg="") {
             const l=document.getElementById('loadingInfo'); const p=document.getElementById('loadingProgress');
             if(l){
                 l.style.display=show?'flex':'none';
                 l.style.flexDirection='column'; l.style.alignItems='center'; l.style.justifyContent='center';
                 if(show && p){
                     l.childNodes[0].nodeValue = msg + '\n'; // Set main message text node
                     p.textContent=''; // Clear progress details initially
                 }
             }
        }
        function clamp(v,min,max){return Math.max(min,Math.min(v,max));}
        function fract(n){return n-Math.floor(n);}

        // --- Entry Point ---
        // Ensure core libraries are loaded before initializing
        if (typeof tf !== 'undefined' && typeof THREE !== 'undefined') {
            // Use setTimeout to allow the browser to finish rendering the initial page elements
            setTimeout(initialize, 50);
        } else {
            showError("Fatal: Core libraries (TF/Three) failed load.");
            console.error("TF.js or Three.js was not found. Check network connection and script tags.");
            const d = document.getElementById('loadingInfo');
            if (d) { // Show error in loading div if possible
                d.innerHTML = "ERROR: Core library failed.<br>Refresh/check console.";
                d.style.display = 'flex'; d.style.color = 'red';
            }
        }

    </script>
</body>
</html>
